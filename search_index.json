[["index.html", "Taller de Introducción a Ciencia de Datos y Machine Learning Capítulo 1 BIENVENIDA 1.1 Objetivo 1.2 ¿Quienes somos?", " Taller de Introducción a Ciencia de Datos y Machine Learning Karina Lizette Gamboa Puente Oscar Arturo Bringas López Capítulo 1 BIENVENIDA 1.1 Objetivo El taller tiene como finalidad que el alumno conozca la esencia de la Ciencia de Datos. Se conocerá la diferencia e interacciones que se tiene con big data, business inteligence, estadística, machine learning, deep learning. Adicionalmente, se hablará de las principales herramientas usadas y las librerías que soportan a los lenguajes de programación. Dos casos de aplicación en la industria de seguros serán analizados a lo largo del taller para ejemplificar los conceptos y ventajas de los algoritmos usados. El lenguaje de programación usado será R, junto con las librerías y funciones más novedosas que hoy en día han sido desarrolladas. 1.2 ¿Quienes somos? ACT. ARTURO BRINGAS LinkedIn Actuario, egresado de la Facultad de Ciencias y Maestría en Ciencia de Datos, ITAM. Coordinador de estadística aplicada y Ciencia de Datos en el Departamento de Investigación Aplicada y Opinión de la UNAM, diseñando metodologías estadísticas para estudios de percepción social. Consultor para empresas y organizaciones como ONU, UNAM, INEGI, GNP, CBX, El Universal, IZZI y Amat. Actualmente se desempeña en diferentes proyectos contribuyendo a empresas en temas de machine learning, estadística, programación, visualización interactiva de datos y análisis geoespacial. ACT. LIZETTE GAMBOA LinkedIn Actuaria, egresada de la Facultad de Ciencias, UNAM, Maestría en Ciencia de Datos, ITAM. Experiencia en áreas de analítica predictiva, e inteligencia del negocio, Lead y Senior Data Scientist en consultoría en diferentes proyectos con empresas de tecnología, retail y del sector asegurador y financiero. Experta en entendimiento de negocio para la correcta implementación de algoritmos de inteligencia y explotación de datos. Actividad actual: Analytics Architect en Merama y Senior Data Science en CLOSTER Experiencia en GNP, Activer Banco y Casa de Bolsa, PlayCity Casinos, RakenDataGroup Consulting. entre otros. "],["intro.html", "Capítulo 2 INTRODUCCIÓN 2.1 ¿Qué es Ciencia de Datos? 2.2 Objetivo de la Ciencia se Datos 2.3 ¿Qué se requiere para hacer Ciencia de Datos? 2.4 Tipos de problemas que se pueden resolver con Ciencia de Datos 2.5 Tipos de aprendizaje", " Capítulo 2 INTRODUCCIÓN 2.1 ¿Qué es Ciencia de Datos? 2.1.1 Definiendo conceptos: Estadistica Disciplina que recolecta, organiza, analiza e interpreta datos. Lo hace a través de una población muestral generando estadística descriptiva y estadística inferencial. Estadística descriptiva: Describe de manera cuantificada información de los datos: distribuciones de los datos, análisis exploratorio, correlaciones, outliers, etc. Estadística inferencial: A partir de los datos de una población muestral, deducir / concluir / analizar hipótesis para una población. Principales tipos de problemas: Todo lo que tenga datos - tener muchos datos, puede llegar a ser un problema -. La estadística supone casi siempre que el sistema es estático y generaliza la solución bajo las mismas condiciones. Si la cantidad de datos es de “gran escala,” es muy probable que se tengan que hacer muestras para trabajar con los datos. Machine Learning: El ‘machine learning’ –aprendizaje automático– es una rama de la inteligencia artificial que permite que las máquinas aprendan de los patrones existentes en los datos. Se usan métodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión. (Está enfocado en la programación de máquinas para aprender de los patrones existentes en datos principalmente estructurados y anticiparse al futuro) Business Intelligence: BI aprovecha el software y los servicios para transformar los datos en conocimientos prácticos que informan las decisiones empresariales estratégicas y tácticas de una organización. Las herramientas de BI acceden y analizan conjuntos de datos y presentan hallazgos analíticos en informes, resúmenes, tableros, gráficos, cuadros, -indicadores- o KPI’s y mapas para proporcionar a los usuarios inteligencia detallada sobre el estado del negocio. (BI esta enfocado en analizar la historia pasada) Deep Learning: El aprendizaje profundo es un subcampo del aprendizaje automático que se ocupa de los algoritmos inspirados en la estructura y función del cerebro llamados redes neuronales artificiales. En Deep Learning, un modelo de computadora aprende a realizar tareas de clasificación directamente a partir de imágenes, texto o sonido. Los modelos de aprendizaje profundo pueden lograr una precisión de vanguardia, a veces superando el rendimiento a nivel humano. Los modelos se entrenan mediante el uso de un gran conjunto de datos etiquetados y arquitecturas de redes neuronales que contienen muchas capas. (Está enfocado en la programación de máquinas para el reconocimiento de imagenes y audio (datos no estructurados)) Big data se refiere a los grandes y diversos conjuntos de información que crecen a un ritmo cada vez mayor. Abarca el volumen de información, la velocidad a la que se crea y recopila, y la variedad o alcance de los puntos de datos que se cubren. Los macrodatos a menudo provienen de la minería de datos y llegan en múltiples formatos. Es comun que se confunda los conceptos de Big Data y Big Compute, como habiamos mencionado Big Data se refiere a el procesamiento de conjuntos de datos que son más voluminosos y complejos que los tradicionales y Big Compute a herramientas y enfoques que utilizan una gran cantidad de recursos de CPU y memoria de forma coordinada para resolver problemas que usan algoritmos muy complejos. Curiosidad: Servidores en líquido para ser enfriados Curiosidad 2: Centro de datos en el océano Entonces, ¿qué NO es ciencia de datos? No es una tecnología No es una herramienta No es desarrollo de software No es Business Intelligence* No es Big Data* No es Inteligencia Artificial* No es (solo) machine learning No es (solo) deep learning No es (solo) visualización No es (solo) hacer modelos 2.2 Objetivo de la Ciencia se Datos Ciencia de datos: Los científicos de datos analizan qué preguntas necesitan respuesta y dónde encontrar los datos relacionados. Tienen conocimiento de negocio y habilidades analíticas, así como la capacidad de extraer, limpiar y presentar datos. Las empresas utilizan científicos de datos para obtener, administrar y analizar grandes cantidades de datos no estructurados. Luego, los resultados se sintetizan y comunican a las partes interesadas clave para impulsar la toma de decisiones estratégicas en la organización. Fuente: Blog post de Drew Conway 2.3 ¿Qué se requiere para hacer Ciencia de Datos? Background científico Conocimientos generales de probabilidad, estadística, álgebra lineal, cálculo, geometría análitica, programación, conocimientos computacionales… etc Datos Relevancia y suficiencia Es indispensable saber si los datos con los que se trabajará son relevantes y suficientes, debemos evaluar qué preguntas podemos responder con los datos con los que contamos. Suficiencia: Los datos con los que trabajamos tienen que ser representativos de la población en general, necesitamos que las características representadas en la información sean suficientes para aproximar a la población objetivo. Relevancia: De igual manera los datos tienen que tener relevancia para la tarea que queremos resolver, por ejemplo, es probable que información sobre gusto en alimentos sea irrelevante para predecir número de hijos. Etiquetas Se necesita la intervención humana para etiquetar, clasificar e introducir los datos en el algoritmo. Software Existen distintos lenguajes de programación para realizar ciencia de datos: 2.4 Tipos de problemas que se pueden resolver con Ciencia de Datos Dependiendo de la industria en la que se quiera aplicar Machine Learning, podemos pensar en distintos enfoques, en la siguiente imagen se muestran algunos ejemplos: 2.5 Tipos de aprendizaje La diferencia entre el análisis supervisado y el no supervisado es la etiqueta, es decir, en el análisis supervisado tenemos una etiqueta “correcta” y el objetivo de los algoritmos es predecir esta etiqueta. 2.5.1 Aprendizaje supervisado Conocemos la respuesta correcta de antemano. Esta respuesta correcta fue “etiquetada” por un humano (la mayoría de las veces, en algunas circunstancias puede ser generada por otro algoritmo). Debido a que conocemos la respuesta correcta, existen muchas métricas de desempeño del modelo para verificar que nuestro algoritmo está haciendo las cosas “bien.” 2.5.1.1 Tipos de aprendizaje supervisado (Regresión vs clasificación) Existen dos tipos principales de aprendizaje supervisado, esto depende del tipo de la variable respuesta: Los algoritmos de clasificación se usan cuando el resultado deseado es una etiqueta discreta, es decir, clasifican un elemento dentro de diversas clases. En un problema de regresión, la variable target o variable a predecir es un valor numérico. 2.5.2 Aprendizaje no supervisado Aquí no tenemos la respuesta correcta de antemano ¿cómo podemos saber que el algoritmo está bien o mal? Estadísticamente podemos verificar que el algoritmo está bien Siempre tenemos que verificar con el cliente si los resultados que estamos obteniendo tienen sentido de negocio. Por ejemplo, número de grupos y características "],["ciclo.html", "Capítulo 3 CICLO DE VIDA 3.1 Ciclo de un proyecto de Ciencia de Datos 3.2 Data Science scoping", " Capítulo 3 CICLO DE VIDA 3.1 Ciclo de un proyecto de Ciencia de Datos Identificación del problema Debemos conocer si el problema es significativo, si el problema se puede resolver con ciencia de datos, y si habrá un compromiso real del lado de cliente/usuario/partner para implementar la solución con todas sus implicaciones: recursos físicos y humanos. Scoping El objetivo es definir el alcance del proyecto y por lo tanto definir claramente los objetivos. Conocer las acciones que se llevarán a cabo para cada objetivo. Estas definirán las soluciones analíticas a hacer. Queremos saber si los datos con los que contamos son relevantes y suficientes. Hacer visible los posibles conflictos éticos que se pueden tener en esta fase. Debemos definir el cómo evaluaremos que el análisis de esos datos será balanceada entre eficiencia, efectividad y equidad. Adquisición de datos Adquisición, almacenamiento, entendimiento y preparación de los datos para después poder hacer analítica sober ellos. Asegurar que en la transferencia estamos cumpliendo con el manejo adecuado de datos sensibles y privados. EDA El objetivo en esta fase es conocer los datos con los que contamos y contexto de negocio explicado a través de los mismos. Identificamos datos faltantes, sugerimos cómo imputarlos. Altamente apoyado de visualización y procesos de adquisición y limpieza de datos. Formulación analítica Esta fase incluye empezar a formular nuestro problema como uno de ciencia de datos, el conocimiento adquirido en la fase de exploración nos permite conocer a mayor detalle del problema y por lo tanto de la solución adecuada. Modelado Proceso iterativo para desarrollar diferentes “experimentos.” Mismo algoritmo/método diferentes hiperparámetros (grid search). Diferentes algortimos. Selección de un muy pequeño conjunto de modelos tomando en cuenta un balance entre interpretabilidad, complejidad, desempeño, fairness. Correcta interpretación de los resultados de desempeño de cada modelo. Validación Es muy importante poner a prueba el/los modelo/modelos seleccionados en la fase anterior. Esta prueba es en campo con datos reales, le llamamos prueba piloto. Debemos medir el impacto causal que nuestro modelo tuvo en un ambiente real. Acciones a realizar Finalmente esta etapa corresponde a compartir con los tomadores de decisiones/stakeholders/creadores de política pública los resultados obtenidos y la recomendación de acciones a llevar a cabo -menú de opciones-. Las implicaciones éticas de esta fase consisten en hacer conciente el impacto social de nuestro trabajo. 3.2 Data Science scoping El scoping es uno de los pasos más importante en los proyectos de ciencia de datos, es ideal realizarlo con ayuda del cliente, tiene como objetivo definir el alcance del proyecto, definir los objetivos, conocer las acciones que se llevaran acabo, conocer si los datos son relevantes y suficientes, proponer soluciones analíticas, entre otros puntos que se tocaran a continuación. 3.2.1 Data Maturity Framework Antes de iniciar con el scoping, queremos conocer si los interesados están listos para realizar un proyecto de ciencia de datos. Para ello, una opción es usar el Data Maturity Framework desarrollado en la Universidad de Chicago. El Data Maturity Framework nos sirve para ver dónde se encuentra la organización en el marco de madurez de datos y cómo mejorar su organización, tecnología y preparación de datos. Tiene tres áreas de contenido: Definición del problema Disponibilidad de datos y tecnología Preparación organizacional Esta dividido en tres partes: Un cuestionario y una encuesta para evaluar la preparación de la organización. Matriz de preparación de datos y tecnología Matriz de preparación organizacional 3.2.2 Scoping Para realizar el scoping podemos apoyarnos del siguiente documento. Ya que sabemos que la organización esta preparada para realizar un proyecto de ciencia de datos, podemos inicar el scoping, el proceso a seguir es el siguiente: 3.2.2.1 Definir el(los) objetivo(s) Considerado el paso más importante del proceso, los stakeholders iniciaran con un planteamiento del problema de manera muy general, nuestra responsabilidad será ir aterrizando ideas y definir el problema de manera más concreta, esta parte del scoping puede ocurrir en distintas iteraciones. Necesitamos hacer que el objetivo sea concreto, medible y optimizable. Cuando se van refinando objetivos, es común que se vaya priorizando por lo que tendremos tradeoffs que irán ligados a las acciones y al contexto del negocio. 3.2.2.2 ¿Qué acciones o intervenciones existen que serán mejoradas a través de este proyecto? Debemos definir acciones concretas, si esto no ocurre es muy probable que la solución no sea implementada por lo que el proyecto no tendrá uso y no estaráamos haciendo ciencia de datos. La implementación del proyecto debería ayudar a tener mejor información para llevar acabo estas acciones, es decir, el proyecto mejorará la toma de decisiones basadas en la evidencia de los datos. Hacer una lista con las acciones ayuda a que el proyecto sea accionable, es posible que estas acciones no existan aún en la organización, por lo que el proyecto puede ayudar a generar nuevas acciones. Es muy común que la acción definida por el stakeholder sea de muy alto nivel, en ese caso podemos tomar 2 caminos en el scoping: Proponer en el scoping que el proyecto informe a esa acción general. Generar a partir de esa acción general acciones más pequeñas. 3.2.2.3 ¿Qué datos tenemos y cuáles necesitamos? Primero observemos que no se habia hablado de los datos hasta este punto, lo anterior porque debemos primero pensar en el problema, entenderlo y luego ver con qué datos contamos para resolverlo. Si hacemos esto primero seguramente acabaremos desarrollando productos de datos “muertos” y no accionables. En este paso se le dara uso al Data Maturity Framework, queremos conocer cómo se guardan los datos, con qué frecuencia, en qué formato, en qué estructura, qué granularidad tiene, desde cuándo tenemos historia de estos datos, si existe un sesgo en su recolección, con qué frencuencia recolectan nueva información, sobreescribe la ya existente? Uno de los objetivos consiste en identificar si la granularidad, frecuencia y horizonte de tiempo en los datos corresponde a la granularidad, frecuencia y horizonte de tiempo de las acciones. Otro punto importante es saber si los datos con los que se cuenta son relevantes y suficientes para desarrollar el proyecto, se pueden considerar fuentes de datos externa. 3.2.2.3.1 Data Lakes Un data lake es un repositorio de almacenamiento centralizado que contiene datos de varias fuentes en un formato granular y sin procesar. Puede guardar datos estructurados, semiestructurados o no estructurados, lo que significa que los datos pueden conservarse en un formato más flexible para usarlos en un futuro. Al guardar datos, un data lake los asocia con identificadores y etiquetas de metadatos para poder extraerlos rápidamente. 3.2.2.3.1.1 Niveles de madurez en Data Lake Data puddle: Repositorio de datos Single-purpose o datos de un solo proyecto (single-project). Este repositorio utiliza tecnologías de “big data.” Los datos que viven en este Data puddle son utilizados normalmente por un solo equipo o en 1 solo proyecto, por lo que el contenido es conocido y entendido por los miembros del equipo. Data pond: Una colección de al menos 2 data puddle aunque en tecnologías clásicas de almacenamiento de datos como un datawarehouse o un data mart. Data lake: También contiene al menos 2 data puddle, sin embargo, se diferencia del data pond en dos cosas: Permite hacer self-service lo que implica que los usuarios de negocio pueden encontrar y usar los datos que viven en el sin tener que depender de la ayuda del departamento de TI. También tiene datos que los usuarios de negocio pueden querer aunque no haya un proyecto que lo requiera en ese momento. Data ocean: Expande los servicios que permite el data lake a todos los datos de la empresa habilitando la cultura data driven de la compañía. 3.2.2.4 ¿Cuál es el análisis que necesitamos hacer? En esta seccion del scoping queremos definir qué tipo de análisis necesitamos hacer con los datos con los que contamos para cumplir con los objetivos definidos y generar las acciones identificadas. El análisis puede incluir métodos y herramientas de diferentes disciplinas: ciencias computacionales, ciencia de datos, machine learning, estadística, ciencias sociales. Existen distintos tipops de análisis, los 4 más comunes son: Descripción: Centrado en entender eventos y comportamientos del pasado. Aunque puede confundirse con business intelligence, debido a que ya definimos objetivos y acciones vamos a desarrollar un producto de datos. Para este tipo de análisis podemos ocupar métodos de aprendizaje no supervisado: clustering. Detección: Más concentrado en los eventos que están sucediendo. Detección de anomalías. Predicción: Concentrado en el futuro, prediciendo futuros eventos o comportamientos. Cambio en comportamiento: Concentrado en entender las causas de cambios en comportamientos de personas eventos, organizaciones, vecindarios, etc. En esta fase tenemos que responder las siguientes preguntas: ¿Qué tipo de análisis neceistaremos? Puede ser más de uno. ¿Cómo vamos a validar el análisis? ¿Qué validaciones se pueden hacer con los datos existentes? ¿Cómo podemos diseñar una prueba en campo para validar el análisis antes de que pongamos el producto en producción. Identificar qué acciones se cubren con cada análisis, debemos tener todas las acciones cubiertas. 3.2.2.5 Ejemplos Los siguientes ejemplos forman parte del trabajo de DSSG, en cada uno de estos planteamientos intentaremos responder las siguientes preguntas: ¿Cuál es el objetivo? ¿Cómo se mide el objetivo? ¿Qué se optimiza? ¿Se puede optimizar? ¿Cuáles son los tradeoffs? ¿Que implicaciones éticas identificas? Envenenamiento por plomo: Hace unos años, comenzamos a trabajar con el Departamento de Salud Pública de Chicago para prevenir el envenenamiento por plomo. El objetivo inicial era aumentar la eficacia de sus inspecciones de peligro de plomo. Una forma de lograr ese objetivo sería concentrarse en los hogares que tienen peligros de plomo. Aunque fue útil, este enfoque no lograría su objetivo real, que era evitar que los niños se intoxicaran con plomo. Encontrar un hogar con peligros de plomo y repararlo solo es beneficioso si existe una alta probabilidad de que un niño presente (actualmente o en el futuro) se exponga al plomo. La siguiente iteración del objetivo fue maximizar la cantidad de inspecciones que detectan peligros de plomo en hogares donde hay un niño en riesgo (antes de que el niño se exponga al plomo). Finalmente, llegamos al objetivo final: identificar qué niños corren un alto riesgo de intoxicación por plomo en el futuro y luego dirigir las intervenciones a los hogares de esos niños.. High School Graduation: Uno de los mayores desafíos que enfrentan las escuelas hoy en día es ayudar a sus estudiantes a graduarse (a tiempo). Las tasas de graduación en los EE. UU. Son ~65%. Todos están interesados en identificar a los estudiantes que corren el riesgo de no graduarse a tiempo. Al hablar inicialmente con la mayoría de los distritos escolares, comienzan con un objetivo muy limitado de predecir qué niños es poco probable que se gradúen a tiempo. El primer paso es volver al objetivo de aumentar las tasas de graduación y preguntar si hay un subconjunto específico de estudiantes en riesgo que quieran identificar. ¿Qué pasaría si pudiéramos identificar a los estudiantes que tienen solo un 5% de probabilidades de estar en riesgo frente a los estudiantes que tienen un 95% de probabilidades de no graduarse a tiempo sin apoyo adicional? Si el objetivo es simplemente aumentar las tasas de graduación, es (probablemente) más fácil intervenir e influir en el primer grupo, mientras que el segundo grupo puede ser más desafiante debido a los recursos que necesita. ¿El objetivo es maximizar la probabilidad promedio/media/mediana de graduarse para una clase/escuela o es el objetivo enfocarse en los niños con mayor riesgo y maximizar la probabilidad de graduación del 10% inferior de los estudiantes? ¿O el objetivo es crear más equidad y disminuir la diferencia en la probabilidad de graduación a tiempo entre el cuartil superior y el cuartil inferior? Todos estos son objetivos razonables, pero las escuelas deben comprender, evaluar y decidir qué objetivos les interesan. Esta conversación a menudo los hace pensar más en definir analíticamente cuáles son sus objetivos organizacionales, así como las compensaciones.. Inspecciones: Hemos trabajado en varios proyectos que involucraron inspecciones, como con la EPA (Agencia de Protección Ambiental) y el Departamento de Conservación Ambiental del Estado de Nueva York para ayudarlos a priorizar qué instalaciones inspeccionar para detectar infracciones de eliminación de desechos, con la ciudad de Cincinnati para ayudar a identificar las propiedades en riesgo de violaciones del código para prevenir el deterioro -el proceso a través del cual una ciudad que funcionaba anteriormente, o parte de ella, cae en deterioro y decrepitud-, y con el Grupo del Banco Mundial para ayudarlos a priorizar qué denuncias de fraude y colusión investigar. En la mayoría de los problemas de inspección/investigación, hay muchas más entidades (viviendas, edificios, instalaciones, negocios, contratos) para inspeccionar que los recursos disponibles necesarios para realizar esas inspecciones. El objetivo con el que comienzan la mayoría de estas organizaciones es dirigir sus inspecciones a las entidades que tienen más probabilidades de violar las regulaciones existentes. Ese es un buen comienzo, pero la mayoría de estas organizaciones nunca pueden inspeccionar todas las instalaciones/hogares que pueden no cumplir con las normas, por lo que el objetivo que realmente buscan es la disuasión: reducir la cantidad total de instalaciones que estarán en violación. Un proceso de inspección ideal resultaría entonces en la reducción del número real de violaciones (encontradas o no), lo cual puede no ser lo mismo que un proceso de inspección que tiene como objetivo ser eficiente y aumentar la tasa de aciertos (% de inspección que resulta en violaciones). Programación de la recolección de residuos: Recientemente comenzamos a trabajar con Sanergy, una empresa social con sede en Kenia. Implementan inodoros portátiles en asentamientos urbanos informales y uno de sus mayores costos es contratar personas para vaciar los inodoros. Hoy en día, todos los inodoros se vacían todos los días, aunque existe una variación en cuánto se usan y cuánto se llenan. Para que puedan crecer y mantener bajos los costos, necesitan un enfoque más adaptable que pueda optimizar el cronograma de vaciado de los inodoros. El objetivo en este caso es asegurarse de no vaciar demasiado el inodoro cuando no está lleno, pero tampoco dejar que permanezca lleno porque entonces no se puede usar. Esto se traduce en una formulación que presiona para vaciar el inodoro lo más cerca posible de estar lleno al 100% sin llegar al 100%. "],["explora.html", "Capítulo 4 ANÁLISIS EXPLORATORIO 4.1 Análisis Exploratorio de Datos (EDA) 4.2 EDA: Análisis Exploratorio de Datos 4.3 GEDA: Análisis Exploratorio de Datos Gráficos 4.4 Uso, decisión e implementación de técnicas gráficas. 4.5 Análisis y Visualización con R", " Capítulo 4 ANÁLISIS EXPLORATORIO 4.1 Análisis Exploratorio de Datos (EDA) ´“El análisis exploratorio de datos se refiere al proceso crítico de realizar investigaciones iniciales sobre los datos para descubrir patrones, detectar anomalías, probar hipótesis y verificar suposiciones con la ayuda de estadísticas resumidas y representaciones gráficas.” Towards 4.2 EDA: Análisis Exploratorio de Datos Un analisis explorario de datos tiene principalmente 5 objetivos: Maximizar el conocimiento de un conjunto de datos Descubrir la estructura subyacente de los datos Extraer variables importantes Detectar valores atípicos y anomalías Probar los supuestos subyacentes EDA no es idéntico a los gráficos estadísticos aunque los dos términos se utilizan casi indistintamente. Los gráficos estadísticos son una colección de técnicas, todas basadas en gráficos y todas centradas en un aspecto de caracterización de datos. EDA abarca un lugar más grande. EDA es una filosofía sobre cómo diseccionar un conjunto de datos; lo que buscamos; cómo nos vemos; y cómo interpretamos. Los científicos de datos pueden utilizar el análisis exploratorio para garantizar que los resultados que producen sean válidos y aplicables a los resultados y objetivos comerciales deseados. EDA se utiliza principalmente para ver qué datos pueden revelar más allá del modelado formal o la tarea de prueba de hipótesis y proporciona una mejor comprensión de las variables del conjunto de datos y las relaciones entre ellas. También puede ayudar a determinar si las técnicas estadísticas que está considerando para el análisis de datos son apropiadas. Dependiendo del tipo de variable queremos obtener la siguiente información: Variables numéricas: Tipo de dato: float, integer Número de observaciones Mean Desviación estándar Cuartiles: 25%, 50%, 75% Valor máximo Valor mínimo Número de observaciones únicos Top 5 observaciones repetidas Número de observaciones con valores faltantes ¿Hay redondeos? Variables categóricas Número de categorías Valor de las categorías Moda Valores faltantes Número de observaciones con valores faltantes Proporción de observaciones por categoría Top 1, top 2, top 3 (moda 1, moda 2, moda 3) Faltas de ortografía ? Fechas Fecha inicio Fecha fin Huecos en las fechas: sólo tenemos datos entre semana, etc. Formatos de fecha (YYYY-MM-DD) Tipo de dato: date, time, timestamp Número de faltantes (NA) Número de observaciones Texto Longitud promedio de cada observación Identificar el lenguaje, si es posible Longitud mínima de cada observación Longitud máxima de cada observación Cuartiles de longitud: 25%, 50%, 75% Coordenadas geoespaciales Primero se pone la latitud y luego la longitud Primer decimal: 111 kms Segundo decimal: 11.1 kms Tercer decimal: 1.1 kms Cuarto decimal: 11 mts Quinto decimal: 1.1 mt Sexto decimal: 0.11 mts Valores que están cercanos al 100 representan la longitud El símbolo en cada coordenada representa si estamos al norte (positivo) o sur (negativo) -en la latitud-, al este (positivo) o al - oeste (negativo) -en la longitud-. 4.3 GEDA: Análisis Exploratorio de Datos Gráficos Como complemento al EDA podemos realizar un GEDA, que es un análisis exploratorio de los datos apoyándonos de visualizaciones, la visualización de datos no trata de hacer gráficas “bonitas” o “divertidas,” ni de simplificar lo complejo. Más bien, trata de aprovechar nuestra gran capacidad de procesamiento visual para exhibir de manera clara aspectos importantes de los datos. 4.3.1 Lo que no se debe hacer… Fuentes: WTF Visualizations Flowingdata 4.3.2 Principios de visualización El objetivo de una visualización es sintentizar información relevante al análisis presentada de manera sencilla y sin ambigüedad. Lo usamos de apoyo para explicar a una audiencia más amplia que puede no ser tan técnica. Una gráfica debe reportar el resultado de un análisis detallado, nunca lo reemplaza. No hacer gráficas porque se vean “cool” Antes de hacer una gráfica, debe pensarse en lo que se quiere expresar o representar Existen “reglas” o mejores gráficas para representar cierto tipo de información de acuerdo a los tipos de datos que se tienen o al objetivo se quiere lograr con la visualización. From Data to Viz No utilizar pie charts 4.3.3 Principios generales del diseño analítico: Muestra comparaciones, contrastes, diferencias. Muestra causalidad, mecanismo, explicación. Muestra datos multivariados, es decir, más de una o dos variables. Integra palabras, números, imágenes y diagramas. Las presentaciones analíticas, a fin de cuentas, se sostienen o caen dependiendo de la calidad, relevancia e integridad de su contenido. Esta categoría incluye técnicas específicas que dependen de la forma de nuestros datos y el tipo de pregunta que queremos investigar: 4.3.4 Técnicas de visualización: Tipos de gráficas: cuantiles, histogramas, caja y brazos, gráficas de dispersión, puntos/barras/ líneas, series de tiempo. Técnicas para mejorar gráficas: Transformación de datos, transparencia, vibración, suavizamiento y bandas de confianza. 4.3.5 Indicadores de calidad gráfica: Aplicables a cualquier gráfica en particular, son guías concretas y relativamente objetivas para evaluar la calidad de una gráfica. Integridad Gráfica: El factor de engaño, es decir, la distorsión gráfica de las cantidades representadas, debe ser mínimo. Chartjunk: Minimizar el uso de decoración gráfica que interfiera con la interpretación de los datos: 3D, rejillas, rellenos con patrones. Tinta de datos: Maximizar la proporción de tinta de datos vs. tinta total de la gráfica. La regla es: si hay tinta que no representa variación en los datos, o la eliminación de esa tinta no representa pérdidas de significado, esa tinta debe ser eliminada. El ejemplo más claro es el de las rejillas en gráficas y tablas: Densidad de datos: Las mejores gráficas tienen mayor densidad de datos, que es la razón entre el tamaño del conjunto de datos y el área de la gráfica. 4.4 Uso, decisión e implementación de técnicas gráficas. 4.4.1 Gráficos univariados: Histograma: El histograma es la forma más popular de mostrar la forma de un conjunto de datos. Se divide la escala de la variable en intervalos, y se realiza un conteo de los casos que caen en cada uno de los intervalos. Los histogramas pueden mostrar distintos aspectos de los datos dependiendo del tamaño y posición de los intervalos. Diagramas de caja y brazos: Es un método estandarizado para representar gráficamente una serie de datos numéricos a través de sus cuartiles. El diagrama de caja muestra a simple vista la mediana y los cuartiles de los datos, pudiendo también representar los valores atípicos de estos. Gráficas de barras: Una gráfica de este tipo nos muestra la frecuencia con la que se han observado los datos de una variable discreta, con una barra para cada categoría de esta variable. Gráficos Circulares (Pie Charts): Un gráfico circular o gráfica circular, también llamado “gráfico de pastel,” es un recurso estadístico que se utiliza para representar porcentajes y proporciones. 4.4.2 Gráficos multivariados Gráfico de dispersión: Los gráficos de dispersión se usan para trazar puntos de datos en un eje vertical y uno horizontal, mediante lo que se trata de mostrar cuánto afecta una variable a otra. Si no existe una variable dependiente, cualquier variable se puede representar en cada eje y el diagrama de dispersión mostrará el grado de correlación (no causalidad) entre las dos variables. Gráficas de líneas: Uno de los tipos de gráfica más utilizados es la de líneas, especialmente cuando se quieren comparar visualmente varias variables a lo largo del tiempo o algún otro parámetro. 4.5 Análisis y Visualización con R Una de las grandes ventajas de usar R es la versatilidad de análisis que pueden realizarse. Se trata de un lenguaje de programación estadístico que permite implementar técnicas simples y complejas para el entendimiento y análisis de la información. 4.5.1 Análisis estadístico library(tidyverse) data &lt;- read_csv(&quot;data/Churn.csv&quot;) glimpse(data) ## Rows: 7,043 ## Columns: 21 ## $ customerID &lt;chr&gt; &quot;7590-VHVEG&quot;, &quot;5575-GNVDE&quot;, &quot;3668-QPYBK&quot;, &quot;7795-CFOCW… ## $ gender &lt;chr&gt; &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;,… ## $ SeniorCitizen &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ Partner &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ Dependents &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;… ## $ tenure &lt;dbl&gt; 1, 34, 2, 45, 2, 8, 22, 10, 28, 62, 13, 16, 58, 49, 2… ## $ PhoneService &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, … ## $ MultipleLines &lt;chr&gt; &quot;No phone service&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No phone service&quot;, &quot;… ## $ InternetService &lt;chr&gt; &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;Fiber optic&quot;, &quot;Fiber opt… ## $ OnlineSecurity &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;… ## $ OnlineBackup &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;N… ## $ DeviceProtection &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Y… ## $ TechSupport &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ StreamingTV &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Ye… ## $ StreamingMovies &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ Contract &lt;chr&gt; &quot;Month-to-month&quot;, &quot;One year&quot;, &quot;Month-to-month&quot;, &quot;One … ## $ PaperlessBilling &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, … ## $ PaymentMethod &lt;chr&gt; &quot;Electronic check&quot;, &quot;Mailed check&quot;, &quot;Mailed check&quot;, &quot;… ## $ MonthlyCharges &lt;dbl&gt; 29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89.10, 29.7… ## $ TotalCharges &lt;dbl&gt; 29.85, 1889.50, 108.15, 1840.75, 151.65, 820.50, 1949… ## $ Churn &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Y… A continuación se presenta el análisis estadístico de proporción de clientes que han cancelado su servicio, así como la desviación típica e intervalos de confianza. churn &lt;- data %&gt;% select(Contract, Churn) %&gt;% group_by(Contract, Churn) %&gt;% summarise( n = n()) %&gt;% mutate( prop = n/sum(n), desv = sqrt(prop*(1 - prop)/sum(n)), li = prop - qnorm(0.975) * desv, ls = prop + qnorm(0.975) * desv ) %&gt;% arrange(Contract, desc(Churn)) %&gt;% ungroup() churn ## # A tibble: 6 × 7 ## Contract Churn n prop desv li ls ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Month-to-month Yes 1655 0.427 0.00795 0.412 0.443 ## 2 Month-to-month No 2220 0.573 0.00795 0.557 0.588 ## 3 One year Yes 166 0.113 0.00824 0.0965 0.129 ## 4 One year No 1307 0.887 0.00824 0.871 0.903 ## 5 Two year Yes 48 0.0283 0.00403 0.0204 0.0362 ## 6 Two year No 1647 0.972 0.00403 0.964 0.980 4.5.2 Análisis Gráfico de datos churn %>% ggplot() + aes(x = Churn, y = prop, color = Churn) + geom_point(size = 0.5) + geom_errorbar(aes(ymin = li, ymax = ls), width= 0.05) + coord_flip() + facet_wrap(~ Contract, ncol = 1) + scale_y_continuous(labels = scales::percent, limits = c(0, 1)) + ggtitle('Proporción de cancelación de clientes') + xlab('Cancelación') + ylab('Proporción') "],["modela.html", "Capítulo 5 MACHINE LEARNING (Supervisado) 5.1 ML y Algoritmos 5.2 Sesgo vs varianza 5.3 Estimación de errores 5.4 Partición de datos 5.5 Métricas de desempeño y estimación de errores 5.6 Algotitmos de machine learning", " Capítulo 5 MACHINE LEARNING (Supervisado) 5.1 ML y Algoritmos Como se habia mencionado, el Machine Learning es una disciplina del campo de la Inteligencia Artificial que, a través de algoritmos, dota a los ordenadores de la capacidad de identificar patrones en datos para hacer predicciones. Este aprendizaje permite a los computadores realizar tareas específicas de forma autónoma. El término se utilizó por primera vez en 1959. Sin embargo, ha ganado relevancia en los últimos años debido al aumento de la capacidad de computación y al BOOM de los datos. Un algoritmo para computadoras puede ser pensado como una receta. Describe exactamente qué pasos se realizan uno tras otro. Los ordenadores no entienden las recetas de cocina, sino los lenguajes de programación: En ellos, el algoritmo se descompone en pasos formales (comandos) que el ordenador puede entender. Algunos problemas pueden formularse fácilmente como un algoritmo, por ejemplo, contando del 1 al 100 o comprobando si un número es un número primo. Para otros problemas, esto es muy difícil, por ejemplo, reconocer la escritura o el texto de las teclas. Aquí los procedimientos de aprendizaje de la máquina ayudan. Durante mucho tiempo se han desarrollado algoritmos que permiten analizar los datos existentes y aplicar los conocimientos derivados de ello a los nuevos datos. La cuestión no es solo saber para qué sirve el Machine Learning, sino que saber cómo funciona y cómo poder implementarlo en la industria para aprovecharse de sus beneficios. Hay ciertos pasos que usualmente se siguen para crear un modelo de Machine Learning. Estos son típicamente realizados por científicos de los datos que trabajan en estrecha colaboración con los profesionales de los negocios para los que se está desarrollando el modelo. Seleccionar y preparar un conjunto de datos de entrenamiento Los datos de entrenamiento son un conjunto de datos representativos de los datos que el modelo de Machine Learning ingerirá para resolver el problema que está diseñado para resolver. Los datos de entrenamiento deben prepararse adecuadamente: aleatorizados y comprobados en busca de desequilibrios o sesgos que puedan afectar al entrenamiento. También deben dividirse en dos subconjuntos: el subconjunto de entrenamiento, que se utilizará para entrenar el algoritmo, y el subconjunto de validación, que se utilizará para probarlo y perfeccionarlo. Elegir un algoritmo para ejecutarlo en el conjunto de datos de entrenamiento Este es uno de los pasos más importantes, ya que se debe elegir qué algoritmo utilizar, siendo este un conjunto de pasos de procesamiento estadístico. El tipo de algoritmo depende del tipo (supervisado o no supervisado), la cantidad de datos del conjunto de datos de entrenamiento y del tipo de problema que se debe resolver. Entrenamiento del algoritmo para crear el modelo El entrenamiento del algoritmo es un proceso iterativo: implica ejecutar las variables a través del algoritmo, comparar el resultado con los resultados que debería haber producido, ajustar los pesos y los sesgos dentro del algoritmo que podrían dar un resultado más exacto, y ejecutar las variables de nuevo hasta que el algoritmo devuelva el resultado correcto la mayoría de las veces. El algoritmo resultante, entrenado y preciso, es el modelo de Machine Learning. Usar y mejorar el modelo El paso final es utilizar el modelo con nuevos datos y, en el mejor de los casos, para que mejore en precisión y eficacia con el tiempo. De dónde procedan los nuevos datos dependerá del problema que se resuelva. Por ejemplo, un modelo de Machine Learning diseñado para identificar el spam ingerirá mensajes de correo electrónico, mientras que un modelo de Machine Learning que maneja una aspiradora robot ingerirá datos que resulten de la interacción en el mundo real con muebles movidos o nuevos objetos en la habitación. 5.2 Sesgo vs varianza En el mundo de Machine Learning cuando desarrollamos un modelo nos esforzamos para hacer que sea lo más preciso, ajustando los parámetros, pero la realidad es que no se puede construir un modelo 100% preciso ya que nunca pueden estar libres de errores. Comprender cómo las diferentes fuentes de error generan sesgo y varianza nos ayudará a mejorar el proceso de ajuste de datos, lo que resulta en modelos más precisos, adicionalmente también evitará el error de sobreajuste y falta de ajuste. 5.2.1 Balance entre sesgo y varianza o Trade-off El objetivo de cualquier algoritmo supervisado de Machine Learning es lograr un bias bajo, una baja varianza y a su vez el algoritmo debe lograr un buen rendimiento de predicción. El bias frente a la varianza se refiere a la precisión frente a la consistencia de los modelos entrenados por su algoritmo. Podemos diagnosticarlos de la siguiente manera: Los algoritmos de baja varianza (alto bias) tienden a ser menos complejos, con una estructura subyacente simple o rígida. Los algoritmos de bajo bias (alta varianza) tienden a ser más complejos, con una estructura subyacente flexible. No hay escapatoria a la relación entre el bias y la varianza en Machine Learning, aumentar el bias disminuirá la varianza, aumentar la varianza disminuirá el bias. 5.2.2 Overfitting El modelo es muy particular. Error debido a la varianza Durante el entrenamiento tiene un desempeño muy bueno, pero al pasar nuevos datos su desempeño es malo. 5.2.3 Underfitting El modelo es demasiado general. Error debido al sesgo. Durante el entrenamiento no tiene un buen desempeño. 5.3 Estimación de errores 5.3.1 Errores reducibles Error por sesgo: Es la diferencia entre la predicción esperada de nuestro modelo y los valores verdaderos. Aunque al final nuestro objetivo es siempre construir modelos que puedan predecir datos muy cercanos a los valores verdaderos, no siempre es tan fácil porque algunos algoritmos son simplemente demasiado rígidos para aprender señales complejas del conjunto de datos. Imagina ajustar una regresión lineal a un conjunto de datos que tiene un patrón no lineal, no importa cuántas observaciones más recopiles, una regresión lineal no podrá modelar las curvas en esos datos. Esto se conoce como underfitting. Error por varianza: Se refiere a la cantidad que la estimación de la función objetivo cambiará si se utiliza diferentes datos de entrenamiento. La función objetivo se estima a partir de los datos de entrenamiento mediante un algoritmo de Machine Learning, por lo que deberíamos esperar que el algoritmo tenga alguna variación. Idealmente no debería cambiar demasiado de un conjunto de datos de entrenamiento a otro. Los algoritmos de Machine Learning que tienen una gran varianza están fuertemente influenciados por los detalles de los datos de entrenamiento, esto significa que los detalles de la capacitación influyen en el número y los tipos de parámetros utilizados para caracterizar la función de mapeo. 5.3.2 Error irreducible El error irreducible no se puede reducir, independientemente de qué algoritmo se usa. También se le conoce como ruido y, por lo general, proviene por factores como variables desconocidas que influyen en el mapeo de las variables de entrada a la variable de salida, un conjunto de características incompleto o un problema mal enmarcado. Acá es importante comprender que no importa cuán bueno hagamos nuestro modelo, nuestros datos tendrán cierta cantidad de ruido o un error irreductible que no se puede eliminar. 5.3.3 Error total Comprender el sesgo y la varianza es fundamental para comprender el comportamiento de los modelos de predicción, pero en general lo que realmente importa es el error general, no la descomposición específica. El punto ideal para cualquier modelo es el nivel de complejidad en el que el aumento en el sesgo es equivalente a la reducción en la varianza. Para construir un buen modelo, necesitamos encontrar un buen equilibrio entre el bias y la varianza de manera que minimice el error total. Un equilibrio óptimo de bias y varianza nunca sobreequiparía o no sería adecuado para el modelo. Por lo tanto comprender el sesgo y la varianza es fundamental para comprender el comportamiento de los modelos de predicción. 5.4 Partición de datos Cuando hay una gran cantidad de datos disponibles, una estrategia inteligente es asignar subconjuntos específicos de datos para diferentes tareas, en lugar de asignar la mayor cantidad posible solo a la estimación de los parámetros del modelo. Si el conjunto inicial de datos no es lo suficientemente grande, habrá cierta superposición de cómo y cuándo se asignan nuestros datos, y es importante contar con una metodología sólida para la partición de datos. 5.4.1 Métodos comunes para particionar datos El enfoque principal para la validación del modelo es dividir el conjunto de datos existente en dos conjuntos distintos: Entrenamiento: Este conjunto suele contener la mayoría de los datos, los cuales sirven para la construcción de modelos donde se pueden ajustar diferentes modelos, se investigan estrategias de ingeniería de características, etc. La mayor parte del proceso de modelado se utiliza este conjunto. Prueba: La otra parte de las observaciones se coloca en este conjunto. Estos datos se mantienen en reserva hasta que se elijan uno o dos modelos como los de mejor rendimiento. El conjunto de prueba se utiliza como árbitro final para determinar la eficiencia del modelo, por lo que es fundamental mirar el conjunto de prueba una sola vez. 5.4.2 ¿Qué proporción debería ser usada? No hay un porcentaje de división óptimo para el conjunto de entrenamiento y prueba. Muy pocos datos en el conjunto de entrenamiento obstaculizan la capacidad del modelo para encontrar estimaciones de parámetros adecuadas y muy pocos datos en el conjunto de prueba reducen la calidad de las estimaciones de rendimiento. Se debe elegir un porcentaje que cumpla con los objetivos de nuestro proyecto con consideraciones que incluyen: Costo computacional en el entrenamiento del modelo. Costo computacional en la evaluación del modelo. Representatividad del conjunto de formación. Representatividad del conjunto de pruebas. Los porcentajes de división más comunes comunes son: Entrenamiento: \\(80\\%\\), Prueba: \\(20\\%\\) Entrenamiento: \\(67\\%\\), Prueba: \\(33\\%\\) Entrenamiento: \\(50\\%\\), Prueba: \\(50\\%\\) 5.4.3 Conjunto de validación El conjunto de validación se definió originalmente cuando los investigadores se dieron cuenta de que medir el rendimiento del conjunto de entrenamiento conducía a resultados que eran demasiado optimistas. Esto llevó a modelos que se sobreajustaban, lo que significa que se desempeñaron muy bien en el conjunto de entrenamiento pero mal en el conjunto de prueba. Para combatir este problema, se retuvo un pequeño conjunto de datos de validación y se utilizó para medir el rendimiento del modelo mientras este está siendo entrenado. Una vez que la tasa de error del conjunto de validación comenzara a aumentar, la capacitación se detendría. En otras palabras, el conjunto de validación es un medio para tener una idea aproximada de qué tan bien se desempeñó el modelo antes del conjunto de prueba. Los conjuntos de validación se utilizan a menudo cuando el conjunto de datos original es muy grande. En este caso, una sola partición grande puede ser adecuada para caracterizar el rendimiento del modelo sin tener que realizar múltiples iteraciones de remuestreo. 5.5 Métricas de desempeño y estimación de errores Hasta ahora ya se ha visto cual es el proceso que debe llevar un modelo de Machine Learning y si bien, el alcance, la manera de preparar los datos, la elección de los algoritmos y el entrenamiento son partes clave, es igualmente importante medir el rendimiento del modelo entrenado. Al utilizar diferentes métricas para la evaluación del rendimiento, deberíamos estar en posición de detectar algún problema o de mejorar el poder de predicción general de nuestro modelo antes de que lo pongamos en producción. 5.5.1 Métricas de desempeño para clasificación Existen distintas métricas de desempeño para problemas de clasificación, debido a que contamos con la respuesta correcta podemos contar cuántos aciertos tuvimos y cuántos fallos tuvimos. Primero, por simplicidad ocuparemos un ejemplo de clasificación binaria, Fraude (1) o No fraude (0). En este tipo de algoritmos definimos cuál de las categorías será nuestra etiqueta positiva y cuál será la negativa. La positiva será la categoría que queremos predecir -en nuestro ejemplo, fraude- y la negativa lo opuesto -en el caso binario- en nuestro ejemplo, no fraude. Dadas estas definiciones tenemos 4 posibilidades: True positives: Nuestra predicción dijo que la transacción es fraude y la etiqueta real dice que es fradue. False positives: Nuestra predicción dijo que la transacción es fraude y la etiqueta real dice que no es fraude. True negatives: Nuestra predicción dijo que la transacción es no fraude y la etiqueta real dice que no es fraude. False negatives: Nuestra predicción dijo que la transacción es no fraude y la etiqueta real dice que es fraude. Matriz de confusión Esta métrica corresponde a una matriz en donde se plasma el conteo de los aciertos y los errores que haya hecho el modelo. En esta métrica utilizamos todos los aciertos y todos los errores que haya tenido el modelo en las predicciones, esto es: los verdaderos positivos (TP), los verdaderos negativos (TN), los falsos positivos (FP) y los falsos negativos (FN). Normalmente los renglones representan las etiquetas reales, ya sean positivas o negativas, y las columnas, las etiquetas predichas. Accuracy Número de aciertos totales entre todas las predicciones. \\[accuracy = \\frac{TP + TN}{ TP+FP+TN+FN}\\] La métrica más utilizada, en datasets imbalanceados esta métrica no nos sirve, al contrario, nos engaña. Precision: Eficiencia De los que identificamos como clase positiva, cuántos identificamos correctamente. ¿Qué tan eficientes somos en la predicción? \\[precision = \\frac{TP}{TP + FP}\\] ¿Cuándo utilizar precision? Esta es la métrica que ocuparás más, pues en un contexto de negocio, donde los recursos son finitos y tiene un costo asociado, ya sea monetario o de tiempo o de recursos, necesitarás que las predicciones de tu etiqueta positiva sean muy eficientes. Al utilizar esta métrica estaremos optimizando el modelo para minimizar el número de falsos positivos. Recall o Sensibilidad: Cobertura Del universo posible de nuestra clase positiva, cuántos identificamos correctamente. \\[recall = \\frac{TP}{TP + FN }\\] Esta métrica la ocuparás cuando en el contexto de negocio de tu problema sea más conveniente minimizar los falsos negativos por el impacto que estos pueden tener en las personas en quienes se implementará la predicción. Al utilizar esta métrica estaremos optimizando el modelo para minimizar el número de falsos negativos. Especificidad Es el número de observaciones correctamente identificados como negativos fuera del total de negativos. \\[Specificity = \\frac{TN}{TN+FP}\\] F1-score Combina precision y recall para optimizar ambos. \\[F = 2 *\\frac{precision * recall}{precision + recall} \\] Se recomienda utilizar esta métrica de desempeño cuando quieres balancear tanto los falsos positivos como los falsos negativos. Aunque es una buena solución para tomar en cuenta ambos errores, pocas veces hay problemas reales que permiten ocuparla, esto es porque en más del 90% de los casos tenemos una restricción en recursos. Ahora con esto en mente podemos definir las siguientes métricas: AUC y ROC: Area Under the Curve y Receiver operator characteristic Una curva ROC es un gráfico que muestra el desempeño de un modelo de clasificación en todos los puntos de corte. AUC significa “Área bajo la curva ROC.” Es decir, AUC mide el área debajo de la curva ROC. 5.5.2 Implementación con R Vamos a analizar los resultados de varios modelos, ellos pueden encontrarlos en la carpeta de archivos del curso: DATOS library(magrittr) library(tidymodels) predicted_data &lt;- readRDS(&quot;data/predicted_data.rds&quot;) predicted_data %&gt;% head(8) %&gt;% knitr::kable() .pred_No .pred_Yes truth customerID gender SeniorCitizen Partner Dependents tenure PhoneService MultipleLines InternetService OnlineSecurity OnlineBackup DeviceProtection TechSupport StreamingTV StreamingMovies Contract PaperlessBilling PaymentMethod MonthlyCharges TotalCharges Churn 0.9611667 0.0388333 No 5575-GNVDE Male 0 No No 34 Yes No DSL Yes No Yes No No No One year No Mailed check 56.95 1889.50 No 0.2345000 0.7655000 Yes 9305-CDSKC Female 0 No No 8 Yes Yes Fiber optic No No Yes No Yes Yes Month-to-month Yes Electronic check 99.65 820.50 Yes 0.6030000 0.3970000 No 6713-OKOMC Female 0 No No 10 No No phone service DSL Yes No No No No No Month-to-month No Mailed check 29.75 301.90 No 0.9960000 0.0040000 No 7469-LKBCI Male 0 No No 16 Yes No No No internet service No internet service No internet service No internet service No internet service No internet service Two year No Credit card (automatic) 18.95 326.80 No 0.9804167 0.0195833 No 9959-WOFKT Male 0 No Yes 71 Yes Yes Fiber optic Yes No Yes No Yes Yes Two year No Bank transfer (automatic) 106.70 7382.25 No 0.3996667 0.6003333 No 4183-MYFRB Female 0 No No 21 Yes No Fiber optic No Yes Yes No No Yes Month-to-month Yes Electronic check 90.05 1862.90 No 0.9965833 0.0034167 No 1680-VDCWW Male 0 Yes No 12 Yes No No No internet service No internet service No internet service No internet service No internet service No internet service One year No Bank transfer (automatic) 19.80 202.25 No 0.7549167 0.2450833 No 6322-HRPFA Male 0 Yes Yes 49 Yes No DSL Yes Yes No Yes No No Month-to-month No Credit card (automatic) 59.60 2970.30 No pr_curve_rforest_clas &lt;- predicted_data %&gt;% pr_curve(truth = truth, estimate = .pred_Yes, event_level = &#39;second&#39;) roc_curve_rforest_clas &lt;- predicted_data %&gt;% roc_curve(truth = truth, estimate = .pred_Yes, event_level = &#39;second&#39;) roc_curve_rforest_clas %&gt;% autoplot() + ggtitle(&quot;Curva ROC&quot;) + xlab(&quot;Tasa Falsos Positivos&quot;) + ylab(&quot;Cobertura&quot;) pr_curve_rforest_clas %&gt;% autoplot() + ggtitle(&quot;Curva PR&quot;) + xlab(&quot;Cobertura&quot;) + ylab(&quot;Precisión&quot;) cm &lt;- predicted_data %&gt;% mutate(est = factor(if_else(.pred_Yes &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;))) %&gt;% yardstick::conf_mat(truth = truth, estimate = est) autoplot(cm, type = &quot;heatmap&quot;) + ggtitle(&quot;Matriz de Confusión&quot;) + xlab(&quot;Verdaderos&quot;) + ylab(&quot;Predicciones&quot;) predicted_data %&lt;&gt;% mutate(est = factor(if_else(.pred_Yes &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;))) bind_rows( predicted_data %&gt;% yardstick::precision(truth, est, event_level = &quot;second&quot;), predicted_data %&gt;% yardstick::recall(truth, est, event_level = &quot;second&quot;), predicted_data %&gt;% yardstick::accuracy(truth, est, event_level = &quot;second&quot;), predicted_data %&gt;% roc_auc(truth = truth, estimatator = .pred_Yes, event_level = &quot;second&quot;), predicted_data %&gt;% pr_auc(truth = truth, estimatator = .pred_Yes, event_level = &quot;second&quot;) ) ## # A tibble: 5 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 precision binary 0.617 ## 2 recall binary 0.472 ## 3 accuracy binary 0.780 ## 4 roc_auc binary 0.787 ## 5 pr_auc binary 0.593 Comparemos los resultados de otros modelos: results_cla_tree &lt;- readRDS(file = &#39;data/results_cla_tree.RDS&#39;) results_cla_rforest &lt;- readRDS(file = &#39;data/results_cla_rforest.RDS&#39;) results_cla_logistico &lt;- readRDS( file = &#39;data/results_cla_logistico.RDS&#39;) roc_curve_cla_tree &lt;- roc_curve( results_cla_tree, truth = truth, estimate = .pred_Yes, event_level = &#39;second&#39;) %&gt;% mutate(ID = &#39;Árbol de decisión&#39;) roc_curve_cla_random &lt;- roc_curve( results_cla_rforest, truth = truth, estimate = .pred_Yes, event_level = &#39;second&#39;) %&gt;% mutate(ID = &#39;Random Forest&#39;) roc_curve_cla_logistico &lt;- roc_curve( results_cla_logistico, truth = Churn, estimate = .pred_Yes, event_level = &#39;second&#39;) %&gt;% mutate(ID = &#39;Logit&#39;) pr_curve_cla_tree &lt;- pr_curve( results_cla_tree, truth = truth, estimate = .pred_Yes, event_level = &#39;second&#39;) %&gt;% mutate(ID = &#39;Árbol de decisión&#39;) pr_curve_cla_random &lt;- pr_curve( results_cla_rforest, truth = truth, estimate = .pred_Yes, event_level = &#39;second&#39;) %&gt;% mutate(ID = &#39;Random Forest&#39;) pr_curve_cla_logistico &lt;- pr_curve( results_cla_logistico, truth = Churn, estimate = .pred_Yes, event_level = &#39;second&#39;) %&gt;% mutate(ID = &#39;Logit&#39;) # Pegamos id a los resultados de cada modelo results_pr_curve &lt;- rbind( pr_curve_cla_tree, pr_curve_cla_logistico, pr_curve_cla_random ) results_roc_curve &lt;- rbind(roc_curve_cla_tree,roc_curve_cla_logistico,roc_curve_cla_random) # Curvas pecision recall y ROC pr_curve_plot &lt;- results_pr_curve %&gt;% ggplot(aes(x = recall, y = precision, color = ID)) + geom_path(size = 1) + coord_equal() + ggtitle(&quot;Precision vs Recall&quot;) + theme_minimal() roc_curve_plot &lt;- results_roc_curve %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity, color = ID)) + geom_path(size = 1) + geom_abline() + coord_equal() + ggtitle(&quot;ROC Curve&quot;) + theme_minimal() pr_curve_plot roc_curve_plot 5.6 Algotitmos de machine learning Hasta este momento, se ha aprendido cuáles son las métricas más utilizadas para la evaluación de modelos de machine learning, así como la importancia del problema, contexto a resolver y en qué casos una métrica algunas métricas son más relevantes que otras. Apendimos también a interpretar el trade-off entre distintas métricas para tomar decisiones adecuadas a nuestros recursos disponibles. Ahora, se conocerá la estructura e idea general de algoritmos que son frecuentemente usados en la industria. 5.6.1 Regresión logística En esta sección aprenderemos sobre regresión logística. Existen dos tipos de modelos de regresión logística: regresión simple y regresión múltiple. La regresión logística simple es cuando se utiliza una variable independiente para estimar la probabilidad de pertenecer a un grupo de una variable cualitativa binaria. Cuando se utiliza más de una variable independiente, el proceso se denomina regresión logística múltiple. 5.6.1.1 Función sigmoide Si una variable cualitativa con dos categorías se codifica como 1 y 0, matemáticamente es posible ajustar un modelo de regresión lineal por mínimos cuadrados. El problema de esta aproximación es que, al tratarse de una recta, para valores extremos del predictor, se obtienen valores de \\(Y\\) menores que 0 o mayores que 1, lo que entra en contradicción con el hecho de que las probabilidades siempre están dentro del rango [0,1]. Para evitar estos problemas, la regresión logística transforma el valor devuelto por la regresión lineal empleando una función cuyo resultado está siempre comprendido entre 0 y 1. Existen varias funciones que cumplen esta descripción, una de las más utilizadas es la función logística (también conocida como función sigmoide): \\[\\sigma(x)=\\frac{1}{1+e^{-x}}\\] Función sigmoide: Para valores de \\(x\\) muy grandes, el valor de \\(e^{-x}\\) es aproximadamente 0 por lo que el valor de la función sigmoide es 1. Para valores de \\(x\\) muy negativos, el valor \\(e^{-x}\\) tiende a infinito por lo que el valor de la función sigmoide es 0. Sustituyendo la \\(x\\) de la función sigmoide por la función lineal \\(\\beta_0+\\beta_1X\\) se obtiene que: \\[P(Y=k|X=x)=\\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\\] donde \\(P(Y=k|X=x)\\) puede interpretarse como: la probabilidad de que la variable cualitativa \\(Y\\) adquiera el valor \\(k\\), dado que el predictor \\(X\\) tiene el valor \\(x\\). Esta función, puede ajustarse de forma sencilla con métodos de regresión lineal si se emplea su versión logarítmica: \\[ln(\\frac{p(Y=k|X=x)}{1−p(Y=k|X=x)})=\\beta_0+\\beta_1X\\] 5.6.2 KNN: K-Nearest-Neighbor KNN es un algoritmo de aprendizaje supervisado que podemos usar para regresión o clasificación. La idea detrás del algoritmo es sencilla, este clasifica una nueva observación en la categoria que tenga mas elementos de las k observaciones más cercanas. Es decir, se calculará la distancia de esta nueva observación a cada observación existente, ordenaremos estas distancias de menor a mayor, tomamos las k primeras distancias, la nueva observación sera asignada al grupo que tenga mayor número de observaciones en estas k primeras distancias. Clasificación ¿Cómo debería ser clasificada la nueva observación? Ejemplo: Regresión: Considerando un modelo de 3 vecinos más cercanos, las siguientes imágenes muestran el proceso de ajuste y predicción de nuevas observaciones. Ejemplo de balance de sesgo y varianza 5.6.3 Árboles de decisión (Decision trees) Un árbol de decisiones es un algoritmo del aprendizaje supervisado que se puede utilizar tanto para problemas de clasificación como de regresión. Es un clasificador estructurado en árbol, donde los nodos internos representan las características de un conjunto de datos, las ramas representan las reglas de decisión y cada nodo hoja representa el resultado. La idea básica de los árboles es buscar puntos de cortes en las variables de entrada para hacer predicciones, ir dividiendo la muestra, y encontrar cortes sucesivos para refinar las predicciones. En un árbol de decisión, hay dos tipos nodos, el nodo de decisión o nodos internos (Decision Node) y el nodo hoja o nodo terminal (Leaf node). Los nodos de decisión se utilizan para tomar cualquier decisión y tienen múltiples ramas, mientras que los nodos hoja son el resultado de esas decisiones y no contienen más ramas. Regresión: Clasificación: 5.6.4 Bagging Primero tenemos que definir qué es la ** Agregación de Bootstrap o Bagging**. Este es un aalgoritmo de aprendizaje automático diseñado para mejorar la estabilidad y precisión de algoritmos de ML usados en clasificación estadística y regresión. Además reduce la varianza y ayuda a evitar el sobreajuste. Aunque es usualmente aplicado a métodos de árboles de decisión, puede ser usado con cualquier tipo de método. Bagging es un caso especial del promediado de modelos. Los métodos de bagging son métodos donde los algoritmos simples son usados en paralelo. El principal objetivo de los métodos en paralelo es el de aprovecharse de la independencia que hay entre los algoritmos simples, ya que el error se puede reducir bastante al promediar las salidas de los modelos simples. Es como si, queriendo resolver un problema entre varias personas independientes unas de otras, damos por bueno lo que eligiese la mayoría de las personas. Para obtener la agregación de las salidas de cada modelo simple e independiente, bagging puede usar la votación para los métodos de clasificiación y el promedio para los métodos de regresión. 5.6.5 Random Forest Un bosque aleatorio es un algoritmo de aprendizaje automático supervisado que se construye a partir de algoritmos de árbol de decisión. Este algoritmo se aplica en diversas industrias, como la banca y el comercio electrónico, para predecir el comportamiento y los resultados. En esta clase se dará una descripción general del algoritmo de bosque aleatorio, cómo funciona y las características del algoritmo. También se señalan las ventajas y desventajas de este algoritmo. ¿Qué es? Un bosque aleatorio es una técnica de aprendizaje automático que se utiliza para resolver problemas de regresión y clasificación. Utiliza el aprendizaje por conjuntos, que es una técnica que combina muchos clasificadores para proporcionar soluciones a problemas complejos. Este algoritmo consta de muchos árboles de decisión. El “bosque” generado se entrena mediante agregación de bootstrap (bagging), el cual es es un meta-algoritmo de conjunto que mejora la precisión de los algoritmos de aprendizaje automático. El algoritmo establece el resultado en función de las predicciones de los árboles de decisión. Predice tomando el promedio o la media de la salida de varios árboles. El aumento del número de árboles aumenta la precisión del resultado. Un bosque aleatorio erradica las limitaciones de un algoritmo de árbol de decisión. Reduce el sobreajuste de conjuntos de datos y aumenta la precisión. 5.6.5.1 Características de los bosques aleatorios Es más preciso que el algoritmo árbol de decisiones. Proporciona una forma eficaz de gestionar los datos faltantes. Puede producir una predicción razonable sin ajuste de hiperparámetros. Resuelve el problema del sobreajuste en los árboles de decisión. En cada árbol forestal aleatorio, se selecciona aleatoriamente un subconjunto de características en el punto de división del nodo. 5.6.5.2 Aplicar árboles de decisión en un bosque aleatorio La principal diferencia entre el algoritmo de árbol de decisión y el algoritmo de bosque aleatorio es que el establecimiento de nodos raíz y la desagregación de nodos se realiza de forma aleatoria en este último. El bosque aleatorio emplea el método de bagging para generar la predicción requerida. El método bagging implica el uso de diferentes muestras de datos (datos de entrenamiento) en lugar de una sola muestra. Los árboles de decisión producen diferentes resultados, dependiendo de los datos de entrenamiento alimentados al algoritmo de bosque aleatorio. Estos resultados se clasificarán y se seleccionará el más alto como resultado final. Nuestro primer ejemplo todavía se puede utilizar para explicar cómo funcionan los bosques aleatorios. Supongamos que solo tenemos cuatro árboles de decisión. En este caso, los datos de entrenamiento que comprenden las observaciones y funciones del teléfono se dividirán en cuatro nodos raíz. Los nodos raíz podrían representar cuatro características que podrían influir en la elección del cliente (precio, almacenamiento interno, cámara y RAM). El bosque aleatorio dividirá los nodos seleccionando características al azar. La predicción final se seleccionará en función del resultado de los cuatro árboles. El resultado elegido por la mayoría de los árboles de decisión será la elección final. Si tres árboles predicen la compra y un árbol predice que no comprará, entonces la predicción final será la compra. En este caso, se prevé que el cliente comprará el teléfono. El siguiente diagrama muestra un clasificador de bosque aleatorio simple. 5.6.5.3 Ventajas y desventajas de bosques aleatorios Ventajas Puede realizar tareas de regresión y clasificación. Un bosque aleatorio produce buenas predicciones que se pueden entender fácilmente. Puede manejar grandes conjuntos de datos de manera eficiente. Proporciona un mayor nivel de precisión en la predicción de resultados sobre el algoritmo del árbol de decisión. Desventajas Cuando se usa un bosque aleatorio, se requieren bastantes recursos para el cálculo. Consume más tiempo en comparación con un algoritmo de árbol de decisiones. No producen buenos resultados cuando los datos son muy escasos. En este caso, el subconjunto de características y la muestra de arranque producirán un espacio invariante. Esto conducirá a divisiones improductivas, que afectarán el resultado. "],["muestreo.html", "Capítulo 6 DISEÑO DE EXPERIMENTOS 6.1 Diseño de experimentos para Machine Learning 6.2 Muestreo probabilístico", " Capítulo 6 DISEÑO DE EXPERIMENTOS 6.1 Diseño de experimentos para Machine Learning 6.1.1 ¿Por qué hacer un experimento? Diferentes segmentos de clientes pueden tener diferentes preferencias, gustos, características, etc. En cuanto a los productos ofertados por una empresa, así como la manera de ser tratados y o recompensados. Para poder conocer los métodos que mejor funcionan en cada segmento de la población es necesario observar su respuesta ante diferentes posibes tratamientos o estímulos. El tamaño de la población en cada grupo, así como la forma de medir los resultados debe realizarse con una metodología estadística que permita garantizar la confianza de los resultados. Un diseño de experimentos es precisamente la aplicación del método científico para generar conocimiento acerca de un sistema o proceso. En el diseño experimental se plantea un conjunto de pruebas documentadas de manera en que los datos recabados puedan ser analizados mediante la estadística para obtener patrones o respuestas predecibles válidas y objetivas que nos formen una conclusión efectiva. 6.1.2 A/B Testing Uno de los métodos estadísticos más usados es: A/B Testing. Este método consiste en segmentar a la población en distintos grupos y aplicar un estímulo a los integrantes de un grupo y comparar el efecto que el estímulo genera en otro grupo al cual se le trató de manera distinta. La manera de estratificar se realiza rigurosamente mediante técnicas estadísticas. Es una metodología de prueba de hipótesis utilizada para responder preguntas del estilo: ¿Cuál es el mejor incentivo para retener a los clientes? ¿Cuál es la mejor promoción para aumentar mis ventas en un segmento de clientes particular? Con el fin de disminuir posibles sesgos, esta técnica requiere de manera estricta que la asignación de grupos sea realizada de manera estocástica mediante técnicas de muestreo estratificado. 6.1.3 ¿Qué es y para qué sirve muestreo? El muestreo es el proceso de seleccionar un conjunto de individuos de una población con el fin de estudiarlos y poder caracterizar el total de la población. Nos ayuda a obtener información fiable de la población a partir de una muestra de la que extrae inferencias estadísticas con un margen de error medido en términos de probabilidades. El aumento del uso de las técnicas de muestreo en la investigación es claro porque es un procedimiento que cuesta mucho menos dinero y consume menos tiempo. Una muestra bien seleccionada de unos cuantos miles de individuos puede representar con gran precisión una población de millones. 6.1.4 Ventajas del muestreo en el mundo corporativo Ventajas Reducción de costos: Los costos de un estudio serán menores si los datos de interés se pueden obtener a partir de una muestra de la población. Por ejemplo, cuando se realizan estudios de prevalencia de un evento de interés, es más económico medir una muestra representativa de 1500 sujetos de una población, que el total de individuos que la componen. Eficiencia: Al trabajar con un número reducido de sujetos de estudio, representativos de la población; el tiempo necesario para conducir el estudio y obtener resultados y conclusiones será notoriamente menor. Importancia No es posible ponerse en contacto con todos los clientes, por lo que solo se incluye una muestra de esa población al realizar una investigación estadística. El supuesto es que la muestra es representativa de toda la población. Las empresas utilizan el muestreo para determinar nuevos nichos de mercado, productos, niveles de satisfacción y métodos de retención sin causar grandes pérdidas cuando el método implementado no es adecuado. 6.1.5 ¿Qué se requiere para formular un problema de muestreo? Es requisito fundamental de una buena muestra que las características de interés que existen en la población se reflejen en la muestra de la manera más cercana posible. Esas características tienen que ver principalmente con el tamaño de la muestra y con la manera de obtenerla. Para esto se necesitan definir los siguientes conceptos: Población objetivo: Colección completa de todas las unidades que se quieren estudiar. Muestra: Subconjunto de la población. Unidad de muestreo: Objeto a ser seleccionado en la muestra que permitirá el acceso a la unidad de observación. Unidad de observación: Objeto sobre el que finalmente se realiza la medición. Variable de interés: Característica propia de los individuos sobre la que se realiza la inferencia para resolver los objetivos de la investigación. En la teoría de muestreo la variable de interés no se supone como una variable aleatoria sino como una cantidad fija o una característica propia de las unidades que componen la población. 6.2 Muestreo probabilístico El muestreo probabilístico se define como aquel en que todos los individuos de la población tienen una probabilidad de entrar a formar parte de la muestra. Los diseños en que interviene el azar producen muestras representativas la mayoría de las veces, aunque no garantizan la representatividad de la población que sometemos a estudio. Los sistemas de muestreo probabilísticos de clasifican de la siguiente manera: Muestreo aleatorio simple: Se caracteriza porque cada elemento de la población tiene la misma probabilidad de ser escogido para formar parte de la muestra. Una vez censado el marco de la población, se asigna un número a cada individuo o elemento y se elige aleatoriamente Para seleccionar una muestra de este tipo se requiere tener en forma de lista todos los elementos que integran la población investigada y utilizar tablas de números aleatorios. Ejemplo: A un grupo de 100 personas se les numera de uno a cien y se depositan en una urna 100 canicas a su vez numeradas de uno a cien. Para obtener una muestra aleatoria simple de 20 elementos, tendríamos que sacar 20 canicas numeradas de la urna que nos seleccionarán en forma completamente aleatoria a los 20 elementos escogidos para que opinen sobre un nuevo producto. Muestreo sistemático: Es susceptible de ser más preciso que el muestreo aleatorio simple. Se elige un primer elemento del universo y luego se van escogiendo otros elementos igualmente espaciados a partir del primero. Consiste en dividir la población en \\(n\\) estratos, compuestos de \\(k\\) unidades. Ejemplo: a partir de una lista de 100 establecimientos de comestibles, deseamos seleccionar una muestra probabilística de 25 tiendas. La forma de hacerlo sería: Dividir 100 entre 25 para obtener 4 tiendas que es el salto sistemático. Extraer un número al azar entre 1 y 4. Supóngase que es el número 3 el cual corresponde al primer elemento seleccionado. Se incluyen en la muestra de establecimientos numerados: \\(3, 7, 11, 15\\) Muestreo estratificado: En este tipo de muestreo la población de estudio se divide en subgrupos o estratos, escogiendo posteriormente una muestra al azar de cada estrato. Esta división suele realizarse según una característica que pueda influir sobre los resultados del estudio. Si la estratificación se realiza respecto una característica se denomina muestreo estratificado simple, y si se realiza respecto a dos o más características sedenomina muestreo estratificado compuesto. Ejemplo: Si existen 5 millones de hipertensos en una población y hay un \\(35\\%\\) de pacientes que fuman, podemos estratificar de manera que en nuestra muestra queden representados al igual que en el total de la población, la misma proporción de hipertensos fumadores (\\(35\\%\\)) y de no fumadores (\\(65\\%\\)). Muestreo por conglomerados: El muestreo por conglomerados nos ayuda cuando es imposible o poco práctico crear un marco de muestreo de una población objetivo debido a que está muy dispersa geográficamente y el costo de la recopilación de datos es relativamente alto. Los elementos de la población son seleccionados al azar en forma natural por agrupaciones (clusters). Los elementos del muestreo se seleccionan de la población de manera individual, uno a la vez. La heterogeneidad del grupo es fundamental para un buen diseño del muestreo por conglomerados. Por otra parte, los elementos dentro de cada grupo debe ser tan heterogéneos como la población objetivo Ejemplo: Una ONG quiere crear una muestra de niñas en 5 ciudades vecinas para obtener información sobre su nivel de educación. "],["agrupa.html", "Capítulo 7 MACHINE LEARNING (No Supervisado) 7.1 Clustering 7.2 K - means 7.3 Partitioning Around Medoids (PAM) 7.4 DBSCAN", " Capítulo 7 MACHINE LEARNING (No Supervisado) 7.1 Clustering Como ya habiamos mencionado, en el aprendizaje supervisado la idea principal es aprender bajo supervisión, donde la señal de supervisión se nombra como valor objetivo o etiqueta. En el aprendizaje no supervisado, carecemos de este tipo de etiqueta. Por lo tanto, necesitamos encontrar nuestro camino sin ninguna supervisión ni guía. Esto simplemente significa que necesitamos descubrir qué es qué por nosotros mismos. Aplicaciones de agrupación en clusters Segmentación de clientes Una de las aplicaciones más comunes de la agrupación en clusters es la segmentación de clientes, Esta estrategia abarca todas las sectores, incluidas las telecomunicaciones, el comercio electrónico, los deportes, la publicidad, las ventas, etc. Agrupación de documentos Esta es otra aplicación común de la agrupación. Supongamos que tiene varios documentos y necesita agrupar documentos similares. La agrupación en clústeres nos ayuda a agrupar estos documentos de manera que documentos similares estén en los mismos grupos. Segmentación de imagen También podemos utilizar la agrupación en clusters para realizar la segmentación de imágenes. Aquí, intentamos agrupar píxeles similares en la imagen. Motores de recomendación También se puede utilizar en motores de recomendación. Supongamos que desea recomendar canciones a sus amigos. Puede ver las canciones que le gustaron a esa persona y luego usar la agrupación para encontrar canciones similares. Reducción de dimensiones mediante agrupamiento de variables similares Podemos usar los grupos de K-means para agrupar variables similares y crear un indice de estas variables para entrenar un modelo supervisado sin necesidad de usar todas las variables de un solo grupo. 7.2 K - means La agrupación en grupos con K-means es uno de los algoritmos de aprendizaje de máquina no supervisados más simples y populares. K-medias es un método de agrupamiento, que tiene como objetivo la partición de un conjunto de n observaciones en k grupos en el que cada observación pertenece al grupo cuyo valor medio es más cercano. Un cluster se refiere a una colección de puntos de datos agregadosa a un grupo debido a ciertas similitudes. 7.2.1 Ajuste de modelo: ¿Cómo funciona el algortimo? Paso 1: Seleccionar el número de clusters K El primer paso en k-means es elegir el número de conglomerados, K. Como estamos en un problema de análisis no supervisado, no hay K correcto, existen métodos para seleccionar algún K pero no hay respuesta correcta. Paso 2: Seleccionar K puntos aleatorios de los datos como centroides. A continuación, seleccionamos aleatoriamente el centroide para cada grupo. Supongamos que queremos tener 2 grupos, por lo que K es igual a 2, seleccionamos aleatoriamente los centroides: Paso 3: Asignamos todos los puntos al centroide del clúster más cercano. Una vez que hemos inicializado los centroides, asignamos cada punto al centroide del clúster más cercano: Paso 4: Volvemos a calcular los centroides de los clusters recién formados. Ahora, una vez que hayamos asignado todos los puntos a cualquiera de los grupos, el siguiente paso es calcular los centroides de los grupos recién formados: Paso 5: Repetir los pasos 3 y 4. Resumen de algoritmo Criterios de paro: Existen tres criterios de paro para detener el algoritmo: Los centroides de los grupos recién formados no cambian: Podemos detener el algoritmo si los centroides no cambian. Incluso después de múltiples iteraciones, si obtenemos los mismos centroides para todos los clústeres, podemos decir que el algoritmo no está aprendiendo ningún patrón nuevo y es una señal para detener el entrenamiento. Los puntos permanecen en el mismo grupo: Otra señal clara de que debemos detener el proceso de entrenamiento si los puntos permanecen en el mismo clúster incluso después de entrenar el algoritmo para múltiples iteraciones. Se alcanza el número máximo de iteraciones: Finalmente, podemos detener el entrenamiento si se alcanza el número máximo de iteraciones. Supongamos que hemos establecido el número de iteraciones en 100. El proceso se repetirá durante 100 iteraciones antes de detenerse. 7.2.2 Implementación en R library(readr) library(dplyr) library(factoextra) library(ggplot2) library(patchwork) Churn &lt;- read_csv(&quot;data/Churn.csv&quot;) Churn ## # A tibble: 7,043 × 21 ## customerID gender SeniorCitizen Partner Dependents tenure PhoneService ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 7590-VHVEG Female 0 Yes No 1 No ## 2 5575-GNVDE Male 0 No No 34 Yes ## 3 3668-QPYBK Male 0 No No 2 Yes ## 4 7795-CFOCW Male 0 No No 45 No ## 5 9237-HQITU Female 0 No No 2 Yes ## 6 9305-CDSKC Female 0 No No 8 Yes ## 7 1452-KIOVK Male 0 No Yes 22 Yes ## 8 6713-OKOMC Female 0 No No 10 No ## 9 7892-POOKP Female 0 Yes No 28 Yes ## 10 6388-TABGU Male 0 No Yes 62 Yes ## # … with 7,033 more rows, and 14 more variables: MultipleLines &lt;chr&gt;, ## # InternetService &lt;chr&gt;, OnlineSecurity &lt;chr&gt;, OnlineBackup &lt;chr&gt;, ## # DeviceProtection &lt;chr&gt;, TechSupport &lt;chr&gt;, StreamingTV &lt;chr&gt;, ## # StreamingMovies &lt;chr&gt;, Contract &lt;chr&gt;, PaperlessBilling &lt;chr&gt;, ## # PaymentMethod &lt;chr&gt;, MonthlyCharges &lt;dbl&gt;, TotalCharges &lt;dbl&gt;, Churn &lt;chr&gt; Churn &lt;- Churn %&gt;% mutate(tenure_scale = scale(tenure, center = T, scale = T) , MonthlyCharges_scale = scale(MonthlyCharges, center = T, scale = T) ) df &lt;- Churn %&gt;% select(tenure_scale, MonthlyCharges_scale) %&gt;% na.omit() km6 &lt;- kmeans(df, centers = 6, nstart = 5) fviz_cluster(km6, data = df, repel = F) set.seed(123) wss_plot &lt;- fviz_nbclust(df, kmeans, method = &quot;wss&quot;) set.seed(123) sil_plot &lt;- fviz_nbclust(df, kmeans, method = &quot;silhouette&quot;) wss_plot + sil_plot Churn &lt;- Churn %&gt;% bind_cols(km6[1]) glimpse(Churn) ## Rows: 7,043 ## Columns: 24 ## $ customerID &lt;chr&gt; &quot;7590-VHVEG&quot;, &quot;5575-GNVDE&quot;, &quot;3668-QPYBK&quot;, &quot;7795-C… ## $ gender &lt;chr&gt; &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Fema… ## $ SeniorCitizen &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Partner &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, … ## $ Dependents &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, … ## $ tenure &lt;dbl&gt; 1, 34, 2, 45, 2, 8, 22, 10, 28, 62, 13, 16, 58, 4… ## $ PhoneService &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;N… ## $ MultipleLines &lt;chr&gt; &quot;No phone service&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No phone service… ## $ InternetService &lt;chr&gt; &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;Fiber optic&quot;, &quot;Fiber… ## $ OnlineSecurity &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ OnlineBackup &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;… ## $ DeviceProtection &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;… ## $ TechSupport &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, … ## $ StreamingTV &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;,… ## $ StreamingMovies &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, … ## $ Contract &lt;chr&gt; &quot;Month-to-month&quot;, &quot;One year&quot;, &quot;Month-to-month&quot;, &quot;… ## $ PaperlessBilling &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;N… ## $ PaymentMethod &lt;chr&gt; &quot;Electronic check&quot;, &quot;Mailed check&quot;, &quot;Mailed check… ## $ MonthlyCharges &lt;dbl&gt; 29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89.10, … ## $ TotalCharges &lt;dbl&gt; 29.85, 1889.50, 108.15, 1840.75, 151.65, 820.50, … ## $ Churn &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;… ## $ tenure_scale &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt; ## $ MonthlyCharges_scale &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt; ## $ cluster &lt;int&gt; 2, 6, 4, 3, 4, 5, 5, 2, 5, 6, 4, 2, 1, 1, 5, … 7.3 Partitioning Around Medoids (PAM) El algoritmo k-medoides es un enfoque de agrupamiento relacionado con el agrupamiento de k-medias para particionar un conjunto de datos en k grupos o clústeres. En k-medoides, cada grupo está representado por uno de los puntos de datos pertenecientes a un grupo. Estos puntos son nombrados medoides. El término medoide se refiere a un objeto dentro de un grupo para el cual la disimilitud promedio entre él y todos los demás miembros del clúster son mínimos. Corresponde a el punto más céntrico del grupo. Estos objetos (uno por grupo) pueden ser considerado como un ejemplo representativo de los miembros de ese grupo que puede ser útil en algunas situaciones. Este algotimo es una alternativa sólida de k-medias. Debido a que este algoritmo es menos sensible al ruido y los valores atípicos, en comparación con k-medias, pues usa medoides como centros de conglomerados en lugar de medias. El uso de medias implica que la agrupación de k-medias es muy sensible a los valores atípicos, lo cual puede afectar gravemente la asignación de observaciones a los conglomerados. El método de agrupamiento de k-medoides más común es el algoritmo PAM (Partitioning Around Medoids, Kaufman &amp; Rousseeuw, 1990). 7.3.1 Implementación en R library(cluster) k_mediods &lt;- pam(df, 6) pam_plot &lt;- fviz_cluster( k_mediods, nstart = 5, ggtheme = theme_minimal()) + ggtitle(&#39;K-Medoids Plot&#39;) + theme(legend.position = &quot;bottom&quot;) pam_plot 7.3.2 Algoritmo PAM El algoritmo PAM se basa en la búsqueda de k objetos representativos o medoides entre las observaciones del conjunto de datos. Después de encontrar un conjunto de k medoides, los grupos se construyen asignando cada observación al medoide más cercano. Posteriormente, cada medoide m y cada punto de datos no medoide seleccionado se intercambian y se calcula la función objetivo. La función objetivo corresponde a la suma de las disimilitudes de todos los objetos a su medoide más cercano. El paso de intercambio intenta mejorar la calidad de la agrupación mediante el intercambio entre objetos seleccionados (medoides) y objetos no seleccionados. Si la función objetivo puede reducirse intercambiando un objeto seleccionado con un objeto no seleccionado, entonces el se realiza el intercambio. Esto se continúa hasta que la función objetivo ya no puede ser disminuida. El objetivo es encontrar k objetos representativos que minimicen la suma de disimilitudes de las observaciones con su objeto representativo más cercano. Como se mencionó anteriormente, el algoritmo PAM funciona con una matriz de disimilitud y para calcular esta matriz, el algoritmo puede utilizar dos métricas: La distancia euclideana, que es la raíz de la suma de cuadrados de las diferencias; Y la distancia de Manhattan, que es la suma de distancias absolutas. Nota: En la práctica, se debería obtener resultados similares la mayor parte del tiempo, utilizando cualquiera de estas distancias mencionadas. Si lo datos contienen valores atípicos, distancia de Manhattan debería dar resultados más sólidos, mientras que la distancia euclidiana se vería influenciada por valores inusuales. 7.4 DBSCAN DBSCAN (agrupación espacial basada en densidad y aplicación con ruido), es un algoritmo de agrupamiento basado en densidad, que puede utilizarse para identificar agrupaciones de cualquier forma en un conjunto de datos que contenga ruido y valores atípicos. La idea básica detrás del enfoque de agrupamiento basado en densidad se deriva de un método de agrupamiento intuitivo. Por ejemplo, mirando la siguiente figura, uno puede identificar fácilmente cuatro grupos junto con varios puntos de ruido, debido a las diferencias en la densidad de puntos. Los clústeres son regiones densas en el espacio de datos, separadas por regiones de menor densidad de puntos. El algoritmo DBSCAN se basa en esta noción intuitiva de “clústeres” y “ruido.” La idea clave es que para cada punto de un grupo, la vecindad de un determinado radio debe contener al menos un número mínimo de puntos. Los métodos de particionamiento vistos anteriormente son adecuados para encontrar grupos de forma esférica o grupos convexos. En otras palabras, ellos funcionan bien solo para grupos compactos y bien separados. Además, también son severamente afectados por la presencia de ruido y valores atípicos en los datos. Desafortunadamente, los datos de la vida real pueden contener: Grupos de forma arbitraria como los como se muestra en la siguiente figura (grupos ovalados, lineales y en forma de “S”). Muchos valores atípicos y ruido. El gráfico anterior contiene 5 grupos y valores atípicos, que incluyen: 2 clústeres ovalados 2 clústeres lineales 1 clúster compacto Dados los datos “multishapes” del paquete factoextra, el algoritmo de k-medias tiene dificultades para identificar estos grupos con formas arbitrarias. Para ilustrar esta situación, el siguiente código calcula k-medias en el conjunto de datos mencionado. Sabemos que hay 5 grupos de en los datos, pero se puede ver que el método de k-medias identifica incorrectamente estos 5 grupos. 7.4.1 Algoritmo El objetivo es identificar regiones densas, que se pueden medir por la cantidad de objetos cerca de un punto dado. Se requieren dos parámetros importantes para DBSCAN: epsilon (“eps”): Define el radio de vecindad alrededor un punto x. puntos mínimos (“MinPts”): Es el número mínimo de vecinos dentro del radio “eps”. Cualquier punto x en el conjunto de datos, con un recuento de vecinos mayor o igual que MinPts, es marcado como un punto central. Decimos que x es un punto fronterizo, si el número de sus vecinos es menos que MinPts, pero pertenece a la vecindad de algún punto central z. Finalmente, si un punto no es ni un núcleo ni un punto fronterizo, entonces se denomina punto de ruido o parte aislada. La siguiente figura muestra los diferentes tipos de puntos (puntos centrales, fronterizos y atípicos) usando MinPts = 6. Aquí x es un punto central porque los vecinos \\(s_{\\epsilon}(x) = 6\\), y es un punto fronterizo ya que \\(s_{\\epsilon}(y) &lt; \\text{ MinPts}\\), pero pertenece a la vecindad del punto central x. Finalmente, z es un punto de ruido. Un clúster basado en densidad se define como un grupo de puntos conectados por densidad. El algoritmo de agrupamiento basado en densidad (DBSCAN) funciona de la siguiente manera: Para cada punto \\(x_i\\), calcular la distancia entre \\(x_i\\) y los otros puntos. Hallar todos los puntos vecinos dentro de la distancia eps del punto de partida (\\(x_i\\)). Cada punto, con un vecino cuenta mayor o igual a MinPts, se marca como punto central o visitado. Para cada punto central, si aún no está asignado a un clúster, crear un nuevo clúster. Encuentrar recursivamente todos sus puntos densamente conectados y asignarlos a el mismo grupo que el punto central. Iterar a través de los puntos no visitados restantes en el conjunto de datos. Los puntos que no pertenecen a ningún clúster se tratan como valores atípicos o ruido. 7.4.2 Implementación en R Utilizaremos el paquete fpc para calcular DBSCAN. También es posible utilizar el paquete dbscan, que proporciona una reimplementación más rápida del algoritmo en comparación con el paquete fpc. library(&quot;fpc&quot;) set.seed(123) db &lt;- fpc::dbscan(df, eps = 0.15, MinPts = 5) fviz_cluster( db, data = df, stand = FALSE, ellipse = FALSE, show.clust.cent = FALSE, geom = &quot;point&quot;, palette = &quot;jco&quot;, ggtheme = theme_minimal() ) Nota: La función fviz_cluster() usa diferentes símbolos de puntos para los puntos centrales (es decir, puntos semilla) y puntos fronterizos. Los puntos negros corresponden a valores atípicos. Puede verse que DBSCAN funciona mejor para estos conjuntos de datos y puede identificar el conjunto correcto de clústeres en comparación con los algoritmos de k-medias. Los resultados del algoritmo se pueden ver de la siguiente manera print(db) ## dbscan Pts=1100 MinPts=5 eps=0.15 ## 0 1 2 3 4 5 ## border 31 24 1 5 7 1 ## seed 0 386 404 99 92 50 ## total 31 410 405 104 99 51 En la tabla anterior, los nombres de las columnas son el número de grupo. El grupo \\(0\\) corresponde a valores atípicos (puntos negros en el gráfico DBSCAN). 7.4.3 Ventajas de DBSCAN A diferencia de K-medias, DBSCAN no requiere que el usuario especifique el número de clústeres que se generarán. DBSCAN puede encontrar cualquier forma de clústeres. No es necesario que el grupo sea circular. DBSCAN puede identificar valores atípicos. "],["otros.html", "Capítulo 8 OTRAS HERRAMIENTAS Y TECNOLOGÍAS 8.1 Git y Github", " Capítulo 8 OTRAS HERRAMIENTAS Y TECNOLOGÍAS 8.1 Git y Github 8.1.1 ¿Qué es Git? Es un sistema de control de versiones, es distribuido, es decir que múltiples personas pueden trabajar en equipo, es open source y también se adapta a todo tipo de proyectos desde pequeños hasta grandes, además, se pueden fusionar archivos, guarda una línea de tiempo a lo largo de todo el proyecto. 8.1.2 ¿Qué es Github? Es una plataforma de desarrollo colaborativo, o también llamada la red social de los desarrolladores donde se alojan los repositorios, el código se almacena de forma pública pero se puede hacer privado con una cuenta de pago. 8.1.3 ¿Cómo utilizarlos? Creación de cuenta y autenticación Para utilizar GitHub de la mejor forma, necesitarás configurar Git. Git es responsable de todo lo relacionado con GitHub que suceda de forma local en tu computadora. Para colaborar de forma efectiva en GitHub, necesitarás escribir en propuestas y solicitudes de cambio utilizando el Lenguaje de Marcado Enriquecido de GitHub. Aprender a usar Git El enfoque colaborativo de GitHub para el desarrollo depende de las confirmaciones de publicación desde tu repositorio local hacia GitHub para que las vean, recuperen y actualicen otras personas utilizando Git. Para obtener más información sobre Git, consulta la guía del “Manual de Git”. Para obtener más información sobre cómo se utiliza Git en GitHub, consulta la sección “flujo de GitHub.” Configurar Git Si planeas utilizar Git localmente en tu computadora, ya sea a través de la línea de comandos, de un IDE o de un editor de texto, necesitarás instalar y configurar Git. Para obtener más información, consulta “Configurar Git.” Si prefieres utilizar una interfaz virtual, puedes descargar y utilziar GitHub Desktop. GitHub Desktop viene en un paquete con Git, así que no hay necesidad de instalar Git por separado. Para obtener más información, consulta “Comenzar con GitHub Desktop.” Una vez que instalaste Git, puedes conectarte a los repositorios de GitHub desde tu computadora local, ya sea que se trate de tu propio repositorio o de la bifurcación del de otro usuario. When you connect to a repository on GitHub.com from Git, you’ll need to authenticate with GitHub using either HTTPS or SSH. Para obtener más información, consulta la sección “Acerca de los repositorios remotos.” Elegir cómo interactuar con GitHub Cada quién tiene su propio flujo de trabajo único para interactuar con GitHub; las interfaces y métodos que utilices dependen de tu preferencia y de lo que funcione mejor para cubrir tus necesidades. Para obtener más información sobre cómo autenticarte en GitHub con cada uno de estos métodos, consulta la sección “Sobre la autenticación en GitHub.” Si quisieras conocer a detalle la forma de empezar con esta herramienta, puedes consultar toda la información en su página oficial: Github "],["despedida.html", "Capítulo 9 DESPEDIDA", " Capítulo 9 DESPEDIDA "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
