[["index.html", "AMAT- Introducción a Ciencia de Datos y Machine Learning Capítulo 1 BIENVENIDA 1.1 Objetivo 1.2 ¿Quienes somos? 1.3 Ciencia de Datos en R 1.4 Estructura del curso actual 1.5 Duración y evaluación del curso 1.6 Recursos y dinámica de clase", " AMAT- Introducción a Ciencia de Datos y Machine Learning Karina Lizette Gamboa Puente Oscar Arturo Bringas López Capítulo 1 BIENVENIDA 1.1 Objetivo La primera parte del curso tiene como finalidad que el alumno tenga un entendimiento general de conceptos, técnicas, algoritmos y del proceso de desarrollo de proyectos de Ciencia de Datos. Entenderá la diferencia entre Big Data, Machine Learning, Business Intelligence y Ciencia de Datos. Todo lo anterior será cumplido mientras el alumno aprende las paqueterías y funciones más novedosas que se usan en R para Ciencia de Datos y las tecnologías que dan soporte a este software. Se asume que el alumno tiene conocimientos generales de estadística, bases matemáticas y de programación básica en R. 1.2 ¿Quienes somos? ACT. ARTURO BRINGAS LinkedIn: arturo-bringas Email: act.arturo.b@ciencias.unam.mx Actuario, egresado de la Facultad de Ciencias y Maestría en Ciencia de Datos, ITAM. Experiencia en modelos predictivos y de clasificación de machine learning aplicado a seguros, deportes y movilidad internacional. Es jefe de departamento en Investigación Aplicada y Opinión de la UNAM, donde realiza estudios estadísticos de impacto social. Es consultor para empresas y organizaciones como GNP, El Universal, UNAM, Sinnia, la Organización de las Naciones Unidas Contra la Droga y el Delito (UNODC), entre otros. Actualmente es profesor de machine learning y programación en R en AMAT y se desempeña como consultor independiente en diferentes proyectos contribuyendo a empresas en temas de machine learning, estadística, series de tiempo, visualización de datos y análisis geoespacial. ACT. KARINA LIZETTE GAMBOA LinkedIn: KaLizzyGam Email: lizzygamboa@ciencias.unam.mx Actuaria, egresada de la Facultad de Ciencias, UNAM, candidata a Maestra en Ciencia de Datos por el ITAM. Experiencia en áreas de analítica predictiva e inteligencia del negocio. Lead y Senior Data Scientist en consultoría en diferentes sectores como tecnología, asegurador, financiero y bancario. Experta en entendimiento de negocio para la correcta implementación de algoritmos de inteligencia y explotación de datos. Actualmente se desarrolla como Arquitecta de Soluciones Analíticas en Merama, startup mexicana clasificada como uno de los nuevos unicornios de Latinoamérica. Senior Data Science en CLOSTER y como profesora del diplomado de Metodología de la Investigación Social por la UNAM así como instructora de cursos de Ciencia de Datos en AMAT. Empresas anteriores: GNP, Activer Banco y Casa de Bolsa, PlayCity Casinos, RakenDataGroup Consulting, entre otros. 1.3 Ciencia de Datos en R 1.4 Estructura del curso actual 1.4.1 Alcances del curso Al finalizar el módulo, el participante sabrá plantear un proyecto de ciencia de datos, desde sus requerimientos hasta sus alcances. Sabrá crear flujos de trabajo limpios y ordenados para crear poderosos modelos de Machine Learning. Este curso brindará las bases para introducirse al módulo intermedio de Ciencia de datos que se imparte en AMAT: 1. Data Science &amp; Machine Learning (Aprendizaje Supervisado II) Requisitos: Computadora con al menos 4Gb Ram. Instalación de R con al menos versión 4.1.0 Instalación de Rstudio con al menos versión 1.4 Kit básico para Ciencia de Datos con R (Tidyverse) ó Dominio de las funciones de manipulación y visualización de datos con Tidyverse en R Temario: 1.- Introducción a Ciencia de Datos (8 HRS) ¿Qué es Ciencia de Datos? Objetivo de la ciencia de datos ¿Qué se requiere para hacer ciencia de datos? Tipos de problemas que se pueden resolver Tipos de algoritmos y aprendizaje Ciclo de vida de un proyecto de Ciencia de Datos Taller de Scoping 2. Machine Learning: conceptos básicos (4 HRS) ML y algoritmos Análisis supervisado vs no supervisado Sesgo y varianza Pre-procesamiento e ingeniería de datos Partición de datos: test &amp; train 3. Machine Learning: Modelos de aprendizaje supervisado (20 HRS) Regresión lineal Regresión logística Regresión lasso Regresión ridge ElasticNet KNN Árbol de decisión Bagging (básico) Random Forest Comparación de modelos 1.5 Duración y evaluación del curso El programa tiene una duración de 32 hrs. Las clases serán impartidas los días sábado, de 9:00 am a 1:00 pm Serán asignados ejercicios que el participante deberá resolver entre una semana y otra. Al final del curso se solicitará un proyecto final, el cual deberá ser entregado para ser acreedor a la constancia de participación. 1.6 Recursos y dinámica de clase En esta clase estaremos usando: R da click aquí si aún no lo descargas RStudio da click aquí también Miro úsame Zoom Clases Pulgar arriba: Voy bien, estoy entendiendo! Pulgar abajo: Eso no quedó muy claro Mano arriba: Quiero participar/preguntar ó Ya estoy listo para iniciar Grupo de WhatsApp El chismecito está aquí Google Drive Notas de clase Revisame si quieres aprender Documento del taller de Scoping. "],["intro.html", "Capítulo 2 INTRODUCCIÓN A CIENCIA DE DATOS 2.1 ¿Qué es Ciencia de Datos? 2.2 Objetivo de la Ciencia de Datos 2.3 ¿Qué se requiere para hacer Ciencia de Datos? 2.4 Aplicaciones de Ciencia de Datos 2.5 Tipos de aprendizaje", " Capítulo 2 INTRODUCCIÓN A CIENCIA DE DATOS 2.1 ¿Qué es Ciencia de Datos? 2.1.1 Definiendo conceptos: Estadistica Disciplina que recolecta, organiza, analiza e interpreta datos. Lo hace a través de una población muestral generando estadística descriptiva y estadística inferencial. La estadística descriptiva, como su nombre lo indica, se encarga de describir datos y obtener conclusiones. La estadística inferencial argumenta sus resultados a partir de las muestras de una población. En la estadística descriptiva se utilizan números, como medidas, para analizar datos y llegar a conclusiones de acuerdo a ellos. Con la estadística inferencial se intenta conseguir información al utilizar un procedimiento ordenado en el manejo de los datos de la muestra. Algunos dicen que La estadística inferencial se encarga de realizar el cálculo de la probabilidad de que algo ocurra en el futuro Business Intelligence: BI aprovecha el software y los servicios para transformar los datos en conocimientos prácticos que informan las decisiones empresariales estratégicas y tácticas de una organización. Las herramientas de BI acceden y analizan conjuntos de datos y presentan hallazgos analíticos en informes, resúmenes, tableros, gráficos, cuadros, -indicadores- o KPI’s y mapas para proporcionar a los usuarios inteligencia detallada sobre el estado del negocio. (BI esta enfocado en analizar la historia pasada) ¿Qué característicastiene un KPI? Específicos Continuos y periódicos Objetivos Cuantificables Medibles Realistas Concisos Coherentes Relevantes Machine Learning: El ‘machine learning’ –aprendizaje automático– es una rama de la inteligencia artificial que permite que las máquinas aprendan de los patrones existentes en los datos. Se usan métodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión. (Está enfocado en la programación de máquinas para aprender de los patrones existentes en datos principalmente estructurados y anticiparse al futuro) Deep Learning: El aprendizaje profundo es un subcampo del aprendizaje automático que se ocupa de los algoritmos inspirados en la estructura y función del cerebro llamados redes neuronales artificiales. En Deep Learning, un modelo de computadora aprende a realizar tareas de clasificación directamente a partir de imágenes, texto o sonido. Los modelos de aprendizaje profundo pueden lograr una precisión de vanguardia, a veces superando el rendimiento a nivel humano. Los modelos se entrenan mediante el uso de un gran conjunto de datos etiquetados y arquitecturas de redes neuronales que contienen muchas capas. (Está enfocado en la programación de máquinas para el reconocimiento de imagenes y audio (datos no estructurados)) Big data se refiere a los grandes y diversos conjuntos de información que crecen a un ritmo cada vez mayor. Abarca el volumen de información, la velocidad a la que se crea y recopila, y la variedad o alcance de los puntos de datos que se cubren. Los macrodatos a menudo provienen de la minería de datos y llegan en múltiples formatos. Es comun que se confunda los conceptos de Big Data y Big Compute, como habiamos mencionado Big Data se refiere a el procesamiento de conjuntos de datos que son más voluminosos y complejos que los tradicionales y Big Compute a herramientas y enfoques que utilizan una gran cantidad de recursos de CPU y memoria de forma coordinada para resolver problemas que usan algoritmos muy complejos. Curiosidad: Servidores en líquido para ser enfriados Curiosidad 2: Centro de datos en el océano Entonces, ¿qué NO es ciencia de datos? No es una tecnología No es una herramienta No es desarrollo de software No es Business Intelligence* No es Big Data* No es Inteligencia Artificial* No es (solo) machine learning No es (solo) deep learning No es (solo) visualización No es (solo) hacer modelos 2.2 Objetivo de la Ciencia de Datos Los científicos de datos analizan qué preguntas necesitan respuesta y dónde encontrar los datos relacionados. Tienen conocimiento de negocio y habilidades analíticas, así como la capacidad de extraer, limpiar y presentar datos. Las empresas utilizan científicos de datos para obtener, administrar y analizar grandes cantidades de datos no estructurados. Luego, los resultados se sintetizan y comunican a las partes interesadas clave para impulsar la toma de decisiones estratégicas en la organización. Fuente: Blog post de Drew Conway Más sobre Conway: Forbes 2016 2.3 ¿Qué se requiere para hacer Ciencia de Datos? Background científico Conocimientos generales de probabilidad, estadística, álgebra lineal, cálculo, geometría análitica, programación, conocimientos computacionales… etc Datos Relevancia y suficiencia Es indispensable saber si los datos con los que se trabajará son relevantes y suficientes, debemos evaluar qué preguntas podemos responder con los datos con los que contamos. Suficiencia: Los datos con los que trabajamos tienen que ser representativos de la población en general, necesitamos que las características representadas en la información sean suficientes para aproximar a la población objetivo. Relevancia: De igual manera los datos tienen que tener relevancia para la tarea que queremos resolver, por ejemplo, es probable que información sobre gusto en alimentos sea irrelevante para predecir número de hijos. Etiquetas Se necesita la intervención humana para etiquetar, clasificar e introducir los datos en el algoritmo. Software Existen distintos lenguajes de programación para realizar ciencia de datos: 2.4 Aplicaciones de Ciencia de Datos Dependiendo de la industria en la que se quiera aplicar Machine Learning, podemos pensar en distintos enfoques, en la siguiente imagen se muestran algunos ejemplos: Podemos pensar en una infinidad de aplicaciones comerciales basadas en el análisis de datos. Con la intención de estructurar las posibles aplicaciones, se ofrece a contiuación una categorización que, aunque no es suficiente para englobar todos los posibles casos de uso, sí es sorprendente la cantidad de aplicaciones que abarca. 1. Aplicaciones centradas en los clientes Incrementar beneficio al mejorar recomendaciones de productos Upselling Cross-selling Reducir tasas de cancelación y mejorar tasas de retención Personalizar experiencia de usuario Mejorar el marketing dirigido Análisis de sentimientos Personalización de productos o servicios 2. Optimización de problemas Optimización de precios Ubicación de nuevas sucursales Maximización de ganancias mediante producción de materias primas Construcción de portafolios de inversión 3. Predicción de demanda Número futuro de clientes Número esperado de viajes en avión / camión / bicis Número de contagios por un virus (demanda médica / medicamentos / etc) Predicción de uso de recursos (luz / agua / gas) 4. Análisis de detección de fraudes Detección de robo de identidad Detección de transacciones ilícitas Detección de servicios fraudulentos Detección de zonas geográficas con actividades ilícitas 2.5 Tipos de aprendizaje La diferencia entre el análisis supervisado y el no supervisado es la etiqueta, es decir, en el análisis supervisado tenemos una etiqueta “correcta” y el objetivo de los algoritmos es predecir esta etiqueta. 2.5.1 Aprendizaje supervisado Conocemos la respuesta correcta de antemano. Esta respuesta correcta fue “etiquetada” por un humano (la mayoría de las veces, en algunas circunstancias puede ser generada por otro algoritmo). Debido a que conocemos la respuesta correcta, existen muchas métricas de desempeño del modelo para verificar que nuestro algoritmo está haciendo las cosas “bien.” 2.5.1.1 Tipos de aprendizaje supervisado (Regresión vs clasificación) Existen dos tipos principales de aprendizaje supervisado, esto depende del tipo de la variable respuesta: Los algoritmos de clasificación se usan cuando el resultado deseado es una etiqueta discreta, es decir, clasifican un elemento dentro de diversas clases. En un problema de regresión, la variable target o variable a predecir es un valor numérico. 2.5.2 Aprendizaje no supervisado Aquí no tenemos la respuesta correcta de antemano ¿cómo podemos saber que el algoritmo está bien o mal? Estadísticamente podemos verificar que el algoritmo está bien Siempre tenemos que verificar con el cliente si los resultados que estamos obteniendo tienen sentido de negocio. Por ejemplo, número de grupos y características "],["ciclo.html", "Capítulo 3 CICLO DE VIDA 3.1 Ciclo de un proyecto de Ciencia de Datos 3.2 Data Science scoping", " Capítulo 3 CICLO DE VIDA 3.1 Ciclo de un proyecto de Ciencia de Datos Identificación del problema Debemos conocer si el problema es significativo, si el problema se puede resolver con ciencia de datos, y si habrá un compromiso real del lado de cliente/usuario/partner para implementar la solución con todas sus implicaciones: recursos físicos y humanos. Scoping El objetivo es definir el alcance del proyecto y por lo tanto definir claramente los objetivos. Conocer las acciones que se llevarán a cabo para cada objetivo. Estas definirán las soluciones analíticas a hacer. Queremos saber si los datos con los que contamos son relevantes y suficientes. Hacer visible los posibles conflictos éticos que se pueden tener en esta fase. Debemos definir el cómo evaluaremos que el análisis de esos datos será balanceada entre eficiencia, efectividad y equidad. Adquisición de datos Adquisición, almacenamiento, entendimiento y preparación de los datos para después poder hacer analítica sober ellos. Asegurar que en la transferencia estamos cumpliendo con el manejo adecuado de datos sensibles y privados. EDA El objetivo en esta fase es conocer los datos con los que contamos y contexto de negocio explicado a través de los mismos. Identificamos datos faltantes, sugerimos cómo imputarlos. Altamente apoyado de visualización y procesos de adquisición y limpieza de datos. Formulación analítica Esta fase incluye empezar a formular nuestro problema como uno de ciencia de datos, el conocimiento adquirido en la fase de exploración nos permite conocer a mayor detalle del problema y por lo tanto de la solución adecuada. Modelado Proceso iterativo para desarrollar diferentes “experimentos.” Mismo algoritmo/método diferentes hiperparámetros (grid search). Diferentes algortimos. Selección de un muy pequeño conjunto de modelos tomando en cuenta un balance entre interpretabilidad, complejidad, desempeño, fairness. Correcta interpretación de los resultados de desempeño de cada modelo. Validación Es muy importante poner a prueba el/los modelo/modelos seleccionados en la fase anterior. Esta prueba es en campo con datos reales, le llamamos prueba piloto. Debemos medir el impacto causal que nuestro modelo tuvo en un ambiente real. Acciones a realizar Finalmente esta etapa corresponde a compartir con los tomadores de decisiones/stakeholders/creadores de política pública los resultados obtenidos y la recomendación de acciones a llevar a cabo -menú de opciones-. Las implicaciones éticas de esta fase consisten en hacer conciente el impacto social de nuestro trabajo. 3.2 Data Science scoping El scoping es uno de los pasos más importante en los proyectos de ciencia de datos, es ideal realizarlo con ayuda del cliente, tiene como objetivo definir el alcance del proyecto, definir los objetivos, conocer las acciones que se llevaran acabo, conocer si los datos son relevantes y suficientes, proponer soluciones analíticas, entre otros puntos que se tocaran a continuación. 3.2.1 Data Maturity Framework Antes de iniciar con el scoping, queremos conocer si los interesados están listos para realizar un proyecto de ciencia de datos. Para ello, una opción es usar el Data Maturity Framework desarrollado en la Universidad de Chicago. El Data Maturity Framework nos sirve para ver dónde se encuentra la organización en el marco de madurez de datos y cómo mejorar su organización, tecnología y preparación de datos. Tiene tres áreas de contenido: Definición del problema Disponibilidad de datos y tecnología Preparación organizacional Esta dividido en tres partes: Un cuestionario y una encuesta para evaluar la preparación de la organización. Matriz de preparación de datos y tecnología Matriz de preparación organizacional 3.2.2 Scoping Para realizar el scoping podemos apoyarnos del siguiente documento. Ya que sabemos que la organización esta preparada para realizar un proyecto de ciencia de datos, podemos inicar el scoping, el proceso a seguir es el siguiente: 3.2.2.1 Definir el(los) objetivo(s) Considerado el paso más importante del proceso, los stakeholders iniciaran con un planteamiento del problema de manera muy general, nuestra responsabilidad será ir aterrizando ideas y definir el problema de manera más concreta, esta parte del scoping puede ocurrir en distintas iteraciones. Necesitamos hacer que el objetivo sea concreto, medible y optimizable. Cuando se van refinando objetivos, es común que se vaya priorizando por lo que tendremos tradeoffs que irán ligados a las acciones y al contexto del negocio. 3.2.2.2 ¿Qué acciones o intervenciones existen que serán mejoradas a través de este proyecto? Debemos definir acciones concretas, si esto no ocurre es muy probable que la solución no sea implementada por lo que el proyecto no tendrá uso y no estaráamos haciendo ciencia de datos. La implementación del proyecto debería ayudar a tener mejor información para llevar acabo estas acciones, es decir, el proyecto mejorará la toma de decisiones basadas en la evidencia de los datos. Hacer una lista con las acciones ayuda a que el proyecto sea accionable, es posible que estas acciones no existan aún en la organización, por lo que el proyecto puede ayudar a generar nuevas acciones. Es muy común que la acción definida por el stakeholder sea de muy alto nivel, en ese caso podemos tomar 2 caminos en el scoping: Proponer en el scoping que el proyecto informe a esa acción general. Generar a partir de esa acción general acciones más pequeñas. 3.2.2.3 ¿Qué datos tenemos y cuáles necesitamos? Primero observemos que no se habia hablado de los datos hasta este punto, lo anterior porque debemos primero pensar en el problema, entenderlo y luego ver con qué datos contamos para resolverlo. Si hacemos esto primero seguramente acabaremos desarrollando productos de datos “muertos” y no accionables. En este paso se le dara uso al Data Maturity Framework, queremos conocer cómo se guardan los datos, con qué frecuencia, en qué formato, en qué estructura, qué granularidad tiene, desde cuándo tenemos historia de estos datos, si existe un sesgo en su recolección, con qué frencuencia recolectan nueva información, sobreescribe la ya existente? Uno de los objetivos consiste en identificar si la granularidad, frecuencia y horizonte de tiempo en los datos corresponde a la granularidad, frecuencia y horizonte de tiempo de las acciones. Otro punto importante es saber si los datos con los que se cuenta son relevantes y suficientes para desarrollar el proyecto, se pueden considerar fuentes de datos externa. 3.2.2.3.1 Data Lakes Un data lake es un repositorio de almacenamiento centralizado que contiene datos de varias fuentes en un formato granular y sin procesar. Puede guardar datos estructurados, semiestructurados o no estructurados, lo que significa que los datos pueden conservarse en un formato más flexible para usarlos en un futuro. Al guardar datos, un data lake los asocia con identificadores y etiquetas de metadatos para poder extraerlos rápidamente. 3.2.2.3.1.1 Niveles de madurez en Data Lake Data puddle: Repositorio de datos Single-purpose o datos de un solo proyecto (single-project). Este repositorio utiliza tecnologías de “big data.” Los datos que viven en este Data puddle son utilizados normalmente por un solo equipo o en 1 solo proyecto, por lo que el contenido es conocido y entendido por los miembros del equipo. Data pond: Una colección de al menos 2 data puddle aunque en tecnologías clásicas de almacenamiento de datos como un datawarehouse o un data mart. Data lake: También contiene al menos 2 data puddle, sin embargo, se diferencia del data pond en dos cosas: Permite hacer self-service lo que implica que los usuarios de negocio pueden encontrar y usar los datos que viven en el sin tener que depender de la ayuda del departamento de TI. También tiene datos que los usuarios de negocio pueden querer aunque no haya un proyecto que lo requiera en ese momento. Data ocean: Expande los servicios que permite el data lake a todos los datos de la empresa habilitando la cultura data driven de la compañía. 3.2.2.4 ¿Cuál es el análisis que necesitamos hacer? En esta seccion del scoping queremos definir qué tipo de análisis necesitamos hacer con los datos con los que contamos para cumplir con los objetivos definidos y generar las acciones identificadas. El análisis puede incluir métodos y herramientas de diferentes disciplinas: ciencias computacionales, ciencia de datos, machine learning, estadística, ciencias sociales. Existen distintos tipops de análisis, los 4 más comunes son: Descripción: Centrado en entender eventos y comportamientos del pasado. Aunque puede confundirse con business intelligence, debido a que ya definimos objetivos y acciones vamos a desarrollar un producto de datos. Para este tipo de análisis podemos ocupar métodos de aprendizaje no supervisado: clustering. Detección: Más concentrado en los eventos que están sucediendo. Detección de anomalías. Predicción: Concentrado en el futuro, prediciendo futuros eventos o comportamientos. Cambio en comportamiento: Concentrado en entender las causas de cambios en comportamientos de personas eventos, organizaciones, vecindarios, etc. En esta fase tenemos que responder las siguientes preguntas: ¿Qué tipo de análisis neceistaremos? Puede ser más de uno. ¿Cómo vamos a validar el análisis? ¿Qué validaciones se pueden hacer con los datos existentes? ¿Cómo podemos diseñar una prueba en campo para validar el análisis antes de que pongamos el producto en producción. Identificar qué acciones se cubren con cada análisis, debemos tener todas las acciones cubiertas. 3.2.2.5 Ejemplos Los siguientes ejemplos forman parte del trabajo de DSSG, en cada uno de estos planteamientos intentaremos responder las siguientes preguntas: ¿Cuál es el objetivo? ¿Cómo se mide el objetivo? ¿Qué se optimiza? ¿Se puede optimizar? ¿Cuáles son los tradeoffs? ¿Que implicaciones éticas identificas? Envenenamiento por plomo: Hace unos años, comenzamos a trabajar con el Departamento de Salud Pública de Chicago para prevenir el envenenamiento por plomo. El objetivo inicial era aumentar la eficacia de sus inspecciones de peligro de plomo. Una forma de lograr ese objetivo sería concentrarse en los hogares que tienen peligros de plomo. Aunque fue útil, este enfoque no lograría su objetivo real, que era evitar que los niños se intoxicaran con plomo. Encontrar un hogar con peligros de plomo y repararlo solo es beneficioso si existe una alta probabilidad de que un niño presente (actualmente o en el futuro) se exponga al plomo. La siguiente iteración del objetivo fue maximizar la cantidad de inspecciones que detectan peligros de plomo en hogares donde hay un niño en riesgo (antes de que el niño se exponga al plomo). Finalmente, llegamos al objetivo final: identificar qué niños corren un alto riesgo de intoxicación por plomo en el futuro y luego dirigir las intervenciones a los hogares de esos niños.. High School Graduation: Uno de los mayores desafíos que enfrentan las escuelas hoy en día es ayudar a sus estudiantes a graduarse (a tiempo). Las tasas de graduación en los EE. UU. Son ~65%. Todos están interesados en identificar a los estudiantes que corren el riesgo de no graduarse a tiempo. Al hablar inicialmente con la mayoría de los distritos escolares, comienzan con un objetivo muy limitado de predecir qué niños es poco probable que se gradúen a tiempo. El primer paso es volver al objetivo de aumentar las tasas de graduación y preguntar si hay un subconjunto específico de estudiantes en riesgo que quieran identificar. ¿Qué pasaría si pudiéramos identificar a los estudiantes que tienen solo un 5% de probabilidades de estar en riesgo frente a los estudiantes que tienen un 95% de probabilidades de no graduarse a tiempo sin apoyo adicional? Si el objetivo es simplemente aumentar las tasas de graduación, es (probablemente) más fácil intervenir e influir en el primer grupo, mientras que el segundo grupo puede ser más desafiante debido a los recursos que necesita. ¿El objetivo es maximizar la probabilidad promedio/media/mediana de graduarse para una clase/escuela o es el objetivo enfocarse en los niños con mayor riesgo y maximizar la probabilidad de graduación del 10% inferior de los estudiantes? ¿O el objetivo es crear más equidad y disminuir la diferencia en la probabilidad de graduación a tiempo entre el cuartil superior y el cuartil inferior? Todos estos son objetivos razonables, pero las escuelas deben comprender, evaluar y decidir qué objetivos les interesan. Esta conversación a menudo los hace pensar más en definir analíticamente cuáles son sus objetivos organizacionales, así como las compensaciones.. Inspecciones: Hemos trabajado en varios proyectos que involucraron inspecciones, como con la EPA (Agencia de Protección Ambiental) y el Departamento de Conservación Ambiental del Estado de Nueva York para ayudarlos a priorizar qué instalaciones inspeccionar para detectar infracciones de eliminación de desechos, con la ciudad de Cincinnati para ayudar a identificar las propiedades en riesgo de violaciones del código para prevenir el deterioro -el proceso a través del cual una ciudad que funcionaba anteriormente, o parte de ella, cae en deterioro y decrepitud-, y con el Grupo del Banco Mundial para ayudarlos a priorizar qué denuncias de fraude y colusión investigar. En la mayoría de los problemas de inspección/investigación, hay muchas más entidades (viviendas, edificios, instalaciones, negocios, contratos) para inspeccionar que los recursos disponibles necesarios para realizar esas inspecciones. El objetivo con el que comienzan la mayoría de estas organizaciones es dirigir sus inspecciones a las entidades que tienen más probabilidades de violar las regulaciones existentes. Ese es un buen comienzo, pero la mayoría de estas organizaciones nunca pueden inspeccionar todas las instalaciones/hogares que pueden no cumplir con las normas, por lo que el objetivo que realmente buscan es la disuasión: reducir la cantidad total de instalaciones que estarán en violación. Un proceso de inspección ideal resultaría entonces en la reducción del número real de violaciones (encontradas o no), lo cual puede no ser lo mismo que un proceso de inspección que tiene como objetivo ser eficiente y aumentar la tasa de aciertos (% de inspección que resulta en violaciones). Programación de la recolección de residuos: Recientemente comenzamos a trabajar con Sanergy, una empresa social con sede en Kenia. Implementan inodoros portátiles en asentamientos urbanos informales y uno de sus mayores costos es contratar personas para vaciar los inodoros. Hoy en día, todos los inodoros se vacían todos los días, aunque existe una variación en cuánto se usan y cuánto se llenan. Para que puedan crecer y mantener bajos los costos, necesitan un enfoque más adaptable que pueda optimizar el cronograma de vaciado de los inodoros. El objetivo en este caso es asegurarse de no vaciar demasiado el inodoro cuando no está lleno, pero tampoco dejar que permanezca lleno porque entonces no se puede usar. Esto se traduce en una formulación que presiona para vaciar el inodoro lo más cerca posible de estar lleno al 100% sin llegar al 100%. "],["machine-learning-conceptos-básicos.html", "Capítulo 4 MACHINE LEARNING: CONCEPTOS BÁSICOS 4.1 Conocimiento previo de los datos: EDA y GEDA 4.2 ML y Algoritmos 4.3 Análisis Supervisado vs No supervisado 4.4 Sesgo vs varianza 4.5 Pre-procesamiento e ingeniería de datos 4.6 Recetas 4.7 Partición de datos", " Capítulo 4 MACHINE LEARNING: CONCEPTOS BÁSICOS 4.1 Conocimiento previo de los datos: EDA y GEDA “El análisis exploratorio de datos se refiere al proceso crítico de realizar investigaciones iniciales sobre los datos para descubrir patrones, detectar anomalías, probar hipótesis y verificar suposiciones con la ayuda de estadísticas resumidas y representaciones gráficas.” Towards 4.1.1 EDA: Análisis Exploratorio de Datos Un analisis explorario de datos tiene principalmente 5 objetivos: Maximizar el conocimiento de un conjunto de datos Descubrir la estructura subyacente de los datos Extraer variables importantes Detectar valores atípicos y anomalías Probar los supuestos subyacentes EDA no es idéntico a los gráficos estadísticos aunque los dos términos se utilizan casi indistintamente. Los gráficos estadísticos son una colección de técnicas, todas basadas en gráficos y todas centradas en un aspecto de caracterización de datos. EDA abarca un lugar más grande. EDA es una filosofía sobre cómo diseccionar un conjunto de datos; lo que buscamos; cómo nos vemos; y cómo interpretamos. Los científicos de datos pueden utilizar el análisis exploratorio para garantizar que los resultados que producen sean válidos y aplicables a los resultados y objetivos comerciales deseados. EDA se utiliza principalmente para ver qué datos pueden revelar más allá del modelado formal o la tarea de prueba de hipótesis y proporciona una mejor comprensión de las variables del conjunto de datos y las relaciones entre ellas. También puede ayudar a determinar si las técnicas estadísticas que está considerando para el análisis de datos son apropiadas. Dependiendo del tipo de variable queremos obtener la siguiente información: Variables numéricas: Tipo de dato: float, integer Número de observaciones Mean Desviación estándar Cuartiles: 25%, 50%, 75% Valor máximo Valor mínimo Número de observaciones únicos Top 5 observaciones repetidas Número de observaciones con valores faltantes ¿Hay redondeos? Variables categóricas Número de categorías Valor de las categorías Moda Valores faltantes Número de observaciones con valores faltantes Proporción de observaciones por categoría Top 1, top 2, top 3 (moda 1, moda 2, moda 3) Faltas de ortografía ? Fechas Fecha inicio Fecha fin Huecos en las fechas: sólo tenemos datos entre semana, etc. Formatos de fecha (YYYY-MM-DD) Tipo de dato: date, time, timestamp Número de faltantes (NA) Número de observaciones Texto Longitud promedio de cada observación Identificar el lenguaje, si es posible Longitud mínima de cada observación Longitud máxima de cada observación Cuartiles de longitud: 25%, 50%, 75% Coordenadas geoespaciales Primero se pone la latitud y luego la longitud Primer decimal: 111 kms Segundo decimal: 11.1 kms Tercer decimal: 1.1 kms Cuarto decimal: 11 mts Quinto decimal: 1.1 mt Sexto decimal: 0.11 mts Valores que están cercanos al 100 representan la longitud El símbolo en cada coordenada representa si estamos al norte (positivo) o sur (negativo) -en la latitud-, al este (positivo) o al - oeste (negativo) -en la longitud-. 4.1.2 GEDA: Análisis Exploratorio de Datos Gráficos Como complemento al EDA podemos realizar un GEDA, que es un análisis exploratorio de los datos apoyándonos de visualizaciones, la visualización de datos no trata de hacer gráficas “bonitas” o “divertidas,” ni de simplificar lo complejo. Más bien, trata de aprovechar nuestra gran capacidad de procesamiento visual para exhibir de manera clara aspectos importantes de los datos. 4.1.2.1 Lo que no se debe hacer… Fuentes: WTF Visualizations Flowingdata {r echo=FALSE,fig.align=‘center,’ out.width = “700pt”} knitr::include_graphics(“img/03-viz/bad_viz3.png”) 4.1.2.2 Principios de visualización El objetivo de una visualización es sintentizar información relevante al análisis presentada de manera sencilla y sin ambigüedad. Lo usamos de apoyo para explicar a una audiencia más amplia que puede no ser tan técnica. Una gráfica debe reportar el resultado de un análisis detallado, nunca lo reemplaza. No hacer gráficas porque se vean “cool” Antes de hacer una gráfica, debe pensarse en lo que se quiere expresar o representar Existen “reglas” o mejores gráficas para representar cierto tipo de información de acuerdo a los tipos de datos que se tienen o al objetivo se quiere lograr con la visualización. From Data to Viz No utilizar pie charts 4.1.2.3 Principios generales del diseño analítico: Muestra comparaciones, contrastes, diferencias. Muestra causalidad, mecanismo, explicación. Muestra datos multivariados, es decir, más de una o dos variables. Integra palabras, números, imágenes y diagramas. Las presentaciones analíticas, a fin de cuentas, se sostienen o caen dependiendo de la calidad, relevancia e integridad de su contenido. Esta categoría incluye técnicas específicas que dependen de la forma de nuestros datos y el tipo de pregunta que queremos investigar: 4.1.2.4 Técnicas de visualización: Tipos de gráficas: cuantiles, histogramas, caja y brazos, gráficas de dispersión, puntos/barras/ líneas, series de tiempo. Técnicas para mejorar gráficas: Transformación de datos, transparencia, vibración, suavizamiento y bandas de confianza. 4.1.2.5 Indicadores de calidad gráfica: Aplicables a cualquier gráfica en particular, son guías concretas y relativamente objetivas para evaluar la calidad de una gráfica. Integridad Gráfica: El factor de engaño, es decir, la distorsión gráfica de las cantidades representadas, debe ser mínimo. Chartjunk: Minimizar el uso de decoración gráfica que interfiera con la interpretación de los datos: 3D, rejillas, rellenos con patrones. Tinta de datos: Maximizar la proporción de tinta de datos vs. tinta total de la gráfica. La regla es: si hay tinta que no representa variación en los datos, o la eliminación de esa tinta no representa pérdidas de significado, esa tinta debe ser eliminada. El ejemplo más claro es el de las rejillas en gráficas y tablas: Densidad de datos: Las mejores gráficas tienen mayor densidad de datos, que es la razón entre el tamaño del conjunto de datos y el área de la gráfica. 4.1.3 Uso, decisión e implementación de técnicas gráficas. 4.1.3.1 Gráficos univariados: Histograma: El histograma es la forma más popular de mostrar la forma de un conjunto de datos. Se divide la escala de la variable en intervalos, y se realiza un conteo de los casos que caen en cada uno de los intervalos. Los histogramas pueden mostrar distintos aspectos de los datos dependiendo del tamaño y posición de los intervalos. Diagramas de caja y brazos: Es un método estandarizado para representar gráficamente una serie de datos numéricos a través de sus cuartiles. El diagrama de caja muestra a simple vista la mediana y los cuartiles de los datos, pudiendo también representar los valores atípicos de estos. Gráficas de barras: Una gráfica de este tipo nos muestra la frecuencia con la que se han observado los datos de una variable discreta, con una barra para cada categoría de esta variable. Gráficos Circulares (Pie Charts): Un gráfico circular o gráfica circular, también llamado “gráfico de pastel,” es un recurso estadístico que se utiliza para representar porcentajes y proporciones. 4.1.3.2 Gráficos multivariados Gráfico de dispersión: Los gráficos de dispersión se usan para trazar puntos de datos en un eje vertical y uno horizontal, mediante lo que se trata de mostrar cuánto afecta una variable a otra. Si no existe una variable dependiente, cualquier variable se puede representar en cada eje y el diagrama de dispersión mostrará el grado de correlación (no causalidad) entre las dos variables. Gráficas de líneas: Uno de los tipos de gráfica más utilizados es la de líneas, especialmente cuando se quieren comparar visualmente varias variables a lo largo del tiempo o algún otro parámetro. 4.1.4 Ggplot2 Comparando con los gráficos base de R, ggplot2: Tiene una gramática más compleja para gráficos simples Tiene una gramática menos compleja para gráficos complejos o muy customizados. Los datos siempre deben ser un data.frame. Usa un sistema diferente para añadir elementos al gráfico. Histograma con los gráficos base: data(iris) hist(iris$Sepal.Length) Histograma con ggplot2: library(ggplot2) ggplot(iris, aes(x = Sepal.Length)) + geom_histogram(color = &#39;white&#39;, bins=8) Ahora vamos a ver un gráfico con colores y varias series de datos. Con los gráficos base: plot(Sepal.Length ~ Sepal.Width, col = factor(Species), data = iris) Con ggplot2: ggplot(iris, aes(x=Sepal.Width , y= Sepal.Length, color=Species))+ geom_point() 4.1.4.1 Objetos aesteticos En ggplot2, aestético significa “algo que puedes ver.” Algunos ejemplos son: Posición (por ejemplo, los ejes x e y) Color (color “externo”) Fill (color de relleno) Shape (forma de puntos) Linetype (tipo de linea) Size (tamaño) Alpha (para la transparencia: los valores más altos tendrían formas opacas y los más bajos, casi transparentes). Hay que advertir que no todas las estéticas tienen la misma potencia en un gráfico. El ojo humano percibe fácilmente longitudes distintas. Pero tiene problemas para comparar áreas (que es lo que regula la estética size) o intensidades de color. Se recomienda usar las estéticas más potentes para representar las variables más importantes. Cada tipo de objeto geométrico (geom) solo acepta un subconjunto de todos los aestéticos. Puedes consultar la pagina de ayuda de geom() para ver que aestéticos acepta. El mapeo aestético se hace con la función aes(). 4.1.4.2 Objetos geométricos o capas Los objetos geométricos son las formas que puede tomar un gráfico. Algunos ejemplos son: Barras (geom_bar(), para las variables univariados discretos o nominales) Histogramas (geom_hist() para aquellas variables univariadas continuas) Puntos (geom_point() para scatter plots, gráficos de puntos, etc…) Lineas (geom_line() para series temporales, lineas de tendencia, etc…) Cajas (geom_boxplot() para gráficos de cajas) Un gráfico debe tener al menos un geom, pero no hay limite. Puedes añadir más geom usando el signo +. Si queremos conocer la lista de objetos geométricos podemos ejercutar el siguiente código: help.search(&quot;geom_&quot;, package = &quot;ggplot2&quot;) Una vez añadida una capa al gráfico a este pueden agregarse nuevas capaas ggplot(iris, aes(x = Petal.Length, y = Petal.Width, colour = Species)) + geom_point() ggplot(iris, aes(x = Petal.Length, y = Petal.Width, colour = Species)) + geom_point()+ geom_smooth() 4.1.4.3 Facetas Muchos de los gráficos que pueden generarse con los elementos anteriores pueden reproducirse usando los gráficos tradicionales de R, pero no los que usan facetas, que pueden permitirnos explorar las variables de diferente forma, por ejemplo: ggplot(iris, aes(x = Petal.Length, y = Petal.Width)) + geom_point() + geom_smooth() + facet_grid(~ Species) crea tres gráficos dispuestos horizontalmente que comparan la relación entre la anchura y la longitud del pétalo de las tres especies de iris. Una característica de estos gráficos, que es crítica para poder hacer comparaciones adecuadas, es que comparten ejes. 4.1.4.4 Más sobre estéticas Las estéticas se pueden etiquetar con la función labs. Además, se le puede añadir un título al gráfico usando la función ggtitle. Por ejemplo, en el gráfico anterior se pueden reetiquetar los ejes y la leyenda haciendo 4.1.5 EDA y GEDA con R Para los ejercicios en clase utilizaremos el set de datos: Diamonds: library(dplyr) library(ggplot2) library(reshape2) data(&quot;diamonds&quot;) Descripción Un conjunto de datos que contiene los precios y otros atributos de casi 54.000 diamantes. Las variables son las siguientes: price: precio en dólares estadounidenses ( $ 326 -  $ 18,823) carat: peso del diamante (0.2–5.01) cut: calidad del corte (Regular, Bueno, Muy Bueno, Premium, Ideal) color: color del diamante, de D (mejor) a J (peor) clarity: una medida de la claridad del diamante (I1 (peor), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (mejor)) x: longitud en mm (0-10,74) y: ancho en mm (0–58,9) width in mm (0–58.9) -z: profundidad en mm (0–31,8) depth porcentaje de profundidad total = z / media (x, y) = 2 * z / (x + y) (43–79) table: ancho de la parte superior del diamante en relación con el punto más ancho (43–95) Ejemplo práctico: diamonds %&gt;% ggplot(aes(x = cut_number(carat, 5), y = price, color = cut)) + geom_boxplot() + labs(title = &quot;Distribución de precio por categoría de corte&quot;) + labs (caption = &quot;Data source:Diamont set&quot;) + labs(x = &quot;Peso del diamante&quot;) + labs(y = &quot;Precio&quot;)+ guides(color=guide_legend(title=&quot;Calidad del corte&quot;)) + ylim(0, 20000) + scale_y_continuous(labels = scales::dollar_format(), breaks = seq(0, 20000,2500 ), limits = c(0, 20000)) 4.1.5.1 Un vistazo rápido a los datos library(DataExplorer) plot_intro(diamonds) plot_missing(diamonds) 4.1.5.2 Análisis univariado 4.1.5.3 Variables numéricas Los histogramas son gráficas de barras que se obtienen a partir de tablas de frecuencias, donde cada barra se escala según la frecuencia relativa entre el ancho del intervalo de clase correspondiente. Un histograma muestra la acumulación ó tendencia, la variabilidad o dispersión y la forma de la distribución. El Diagrama de Caja y bigoteses un tipo de gráfico que muestra un resumen de una gran cantidad de datos en cinco medidas descriptivas, además de intuir su morfología y simetría. Este tipo de gráficos nos permite identificar valores atípicos y comparar distribuciones. Además de conocer de una forma cómoda y rápida como el 50% de los valores centrales se distribuyen. Se puede detectar rápidamente los siguientes valores: Primer cuartil: el 25% de los valores son menores o igual a este valor (punto 2 en el gráfico anterior). Mediana o Segundo Cuartil: Divide en dos partes iguales la distribución. De forma que el 50% de los valores son menores o igual a este valor (punto 3 en el gráfico siguiente). Tercer cuartil: el 75% de los valores son menores o igual a este valor (punto 4 en el gráfico siguiente). Rango Intercuartílico (RIC): Diferencia entre el valor del tercer cuartil y el primer cuartil. Tip: El segmento que divide la caja en dos partes es la mediana (punto 3 del gráfico), que facilitará la comprensión de si la distribución es simétrica o asimétrica, si la mediana se sitúa en el centro de la caja entonces la distribución es simétrica y tanto la media, mediana y moda coinciden. Precio diamonds %&gt;% ggplot( aes( x = price)) + geom_histogram( aes(y = ..density..), color= &quot;Blue&quot;, fill= &quot;White&quot;, bins = 30 ) + stat_density(geom = &quot;line&quot;, colour = &quot;black&quot;, size = 1)+ scale_x_continuous(labels = scales::dollar_format()) + scale_y_continuous(labels = scales::comma_format()) + stat_density(geom = &quot;line&quot;, colour = &quot;black&quot;, size = 1) + ggtitle(&quot;Distribución de precio&quot;) diamonds %&gt;% ggplot( aes( x = price)) + geom_boxplot(binwidth = 1000, color= &quot;Blue&quot;, fill= &quot;White&quot;) + scale_x_continuous(labels = scales::dollar_format()) + scale_y_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución de precio&quot;) Peso del diamante diamonds %&gt;% ggplot( aes( x = carat)) + geom_histogram(binwidth = .03, color= &quot;purple&quot;, fill= &quot;pink&quot;, alpha= 0.3) + scale_y_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución de peso de los diamantes&quot;) + theme_bw() diamonds %&gt;% ggplot( aes( x = carat)) + geom_boxplot(binwidth = .3, color= &quot;purple&quot;, fill= &quot;pink&quot;, alpha= 0.3) + scale_x_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución de peso de los diamantes&quot;) + theme_bw() 4.1.5.4 Variables nominales/categóricas Cálidad de corte diamonds %&gt;% ggplot( aes( x = cut)) + geom_bar( color= &quot;darkblue&quot;, fill= &quot;cyan&quot;, alpha= 0.7) + scale_y_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución de cálidad de corte&quot;) + theme_dark() df_pie &lt;- diamonds %&gt;% group_by(cut) %&gt;% summarise(freq = n(), .groups=&#39;drop&#39;) df_pie %&gt;% ggplot( aes( x = &quot;&quot;, y=freq, fill = factor(cut))) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(theta = &quot;y&quot;, start=0) ggplot(data = diamonds)+ geom_bar( mapping = aes(x = cut, fill = cut), show.legend = F, width = 1)+ theme(aspect.ratio = 1)+ labs(x= NULL, y = NULL)+ coord_polar() Claridad diamonds %&gt;% ggplot( aes( y = clarity)) + geom_bar( color= &quot;darkblue&quot;, fill= &quot;black&quot;, alpha= 0.7) + geom_text(aes(label = ..count..), stat = &quot;count&quot;, vjust = 1, hjust = 1.5,colour = &quot;blue&quot;)+ scale_x_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución claridad&quot;) + theme_get() 4.1.5.5 Análisis multivariado Precio vs Calidad del corte diamonds %&gt;% ggplot(aes(y= price,x=cut,color=cut)) + geom_jitter(size=1.2, alpha= 0.5) diamonds %&gt;% ggplot(aes(y= price,x=cut,color=cut)) + geom_boxplot(size=1.2, alpha= 0.5) diamonds %&gt;% ggplot(aes(x= price ,fill=cut)) + geom_histogram(position = &#39;identity&#39;, alpha = 0.5) diamonds %&gt;% ggplot(aes(x= price ,fill=cut)) + geom_histogram(position = &#39;identity&#39;, alpha = 0.5)+ facet_wrap(~cut, ncol = 1) diamonds %&gt;% ggplot( aes(x = carat ,y=price)) + geom_point(aes(col = clarity) ) + geom_smooth() diamonds %&gt;% ggplot( aes(x = carat ,y=price)) + geom_point(aes(col = clarity) ) + facet_wrap(~clarity)+ geom_smooth() Otro tipo de gráficas interactivas library(stringr) fun_mean &lt;- function(x){ mean &lt;- data.frame( y = mean(x), label = mean(x, na.rm = T) ) return(mean) } means &lt;- diamonds %&gt;% group_by(clarity) %&gt;% summarise(price = round(mean(price), 1)) plot &lt;- diamonds %&gt;% ggplot(aes(x = clarity, y = price)) + geom_boxplot(aes(fill = clarity)) + stat_summary( fun = mean, geom = &quot;point&quot;, colour = &quot;darkred&quot;, shape = 18, size = 3 ) + geom_text( data = means, aes(label = str_c(&quot;$&quot;,price), y = price + 600) ) + ggtitle(&quot;Precio vs Claridad de diamantes&quot;) + xlab(&quot;Claridad&quot;) + ylab(&quot;Precio&quot;) plotly::ggplotly(plot) 4.1.5.6 Warning ! Nunca se debe olvidar que debemos de analizar los datos de manera objetiva, nuestro criterio sobre un problema o negocio no debe de tener sesgos sobre lo que “nos gustaria encontrar en los datos” o lo “que creemos que debe pasar”…. 4.2 ML y Algoritmos Como se habia mencionado, el Machine Learning es una disciplina del campo de la Inteligencia Artificial que, a través de algoritmos, dota a los ordenadores de la capacidad de identificar patrones en datos para hacer predicciones. Este aprendizaje permite a los computadores realizar tareas específicas de forma autónoma. El término se utilizó por primera vez en 1959. Sin embargo, ha ganado relevancia en los últimos años debido al aumento de la capacidad de computación y al BOOM de los datos. Un algoritmo para computadoras puede ser pensado como una receta. Describe exactamente qué pasos se realizan uno tras otro. Los ordenadores no entienden las recetas de cocina, sino los lenguajes de programación: En ellos, el algoritmo se descompone en pasos formales (comandos) que el ordenador puede entender. Algunos problemas pueden formularse fácilmente como un algoritmo, por ejemplo, contando del 1 al 100 o comprobando si un número es un número primo. Para otros problemas, esto es muy difícil, por ejemplo, reconocer la escritura o el texto de las teclas. Aquí los procedimientos de aprendizaje de la máquina ayudan. Durante mucho tiempo se han desarrollado algoritmos que permiten analizar los datos existentes y aplicar los conocimientos derivados de ello a los nuevos datos. La cuestión no es solo saber para qué sirve el Machine Learning, sino que saber cómo funciona y cómo poder implementarlo en la industria para aprovecharse de sus beneficios. Hay ciertos pasos que usualmente se siguen para crear un modelo de Machine Learning. Estos son típicamente realizados por científicos de los datos que trabajan en estrecha colaboración con los profesionales de los negocios para los que se está desarrollando el modelo. Seleccionar y preparar un conjunto de datos de entrenamiento Los datos de entrenamiento son un conjunto de datos representativos de los datos que el modelo de Machine Learning ingerirá para resolver el problema que está diseñado para resolver. Los datos de entrenamiento deben prepararse adecuadamente: aleatorizados y comprobados en busca de desequilibrios o sesgos que puedan afectar al entrenamiento. También deben dividirse en dos subconjuntos: el subconjunto de entrenamiento, que se utilizará para entrenar el algoritmo, y el subconjunto de validación, que se utilizará para probarlo y perfeccionarlo. Elegir un algoritmo para ejecutarlo en el conjunto de datos de entrenamiento Este es uno de los pasos más importantes, ya que se debe elegir qué algoritmo utilizar, siendo este un conjunto de pasos de procesamiento estadístico. El tipo de algoritmo depende del tipo (supervisado o no supervisado), la cantidad de datos del conjunto de datos de entrenamiento y del tipo de problema que se debe resolver. Entrenamiento del algoritmo para crear el modelo El entrenamiento del algoritmo es un proceso iterativo: implica ejecutar las variables a través del algoritmo, comparar el resultado con los resultados que debería haber producido, ajustar los pesos y los sesgos dentro del algoritmo que podrían dar un resultado más exacto, y ejecutar las variables de nuevo hasta que el algoritmo devuelva el resultado correcto la mayoría de las veces. El algoritmo resultante, entrenado y preciso, es el modelo de Machine Learning. Usar y mejorar el modelo El paso final es utilizar el modelo con nuevos datos y, en el mejor de los casos, para que mejore en precisión y eficacia con el tiempo. De dónde procedan los nuevos datos dependerá del problema que se resuelva. Por ejemplo, un modelo de Machine Learning diseñado para identificar el spam ingerirá mensajes de correo electrónico, mientras que un modelo de Machine Learning que maneja una aspiradora robot ingerirá datos que resulten de la interacción en el mundo real con muebles movidos o nuevos objetos en la habitación. 4.3 Análisis Supervisado vs No supervisado Los algoritmos de Machine Learning se dividen en tres categorías, siendo las dos primeras las más comunes: Aprendizaje supervisado: estos algoritmos cuentan con un aprendizaje previo basado en un sistema de etiquetas asociadas a unos datos que les permiten tomar decisiones o hacer predicciones. Algunos ejemplos son: - Un detector de spam que etiqueta un e-mail como spam o no. - Predecir precios de casas - Clasificación de imagenes - Predecir el clima - ¿Quiénes son los clientes descontentos? Aprendizaje no supervisado: en el aprendizaje supervisado, la idea principal es aprender bajo supervisión, donde la señal de supervisión se nombra como valor objetivo o etiqueta. En el aprendizaje no supervisado, carecemos de este tipo de etiqueta. Por lo tanto, necesitamos encontrar nuestro camino sin ninguna supervisión ni guía. Esto simplemente significa que necesitamos descubrir qué es qué por nosotros mismos. Algunos ejemplos son: - Encontrar segmentos de clientes. - Reducir la complejidad de un problema - Selección de variables - Encontrar grupos - Reducción de dimensionalidad Aprendizaje por refuerzo: su objetivo es que un algoritmo aprenda a partir de la propia experiencia. Esto es, que sea capaz de tomar la mejor decisión ante diferentes situaciones de acuerdo a un proceso de prueba y error en el que se recompensan las decisiones correctas. Algunos ejemplos son: - Reconocimiento facial - Diagnósticos médicos - Clasificar secuencias de ADN 4.3.1 Regresión vs clasificación Existen dos tipos principales de aprendizaje supervisado, esto depende del tipo de la variable respuesta: 4.3.1.1 Clasificación En el aprendizaje supervisado, los algoritmos de clasificación se usan cuando el resultado es una etiqueta discreta. Esto quiere decir que se utilizan cuando la respuesta se fundamenta en conjunto finito de resultados. 4.3.1.2 Regresión El análisis de regresión es un subcampo del aprendizaje automático supervisado cuyo objetivo es establecer un método para la relación entre un cierto número de características y una variable objetivo continua. 4.4 Sesgo vs varianza En el mundo de Machine Learning cuando desarrollamos un modelo nos esforzamos para hacer que sea lo más preciso, ajustando los parámetros, pero la realidad es que no se puede construir un modelo 100% preciso ya que nunca pueden estar libres de errores. Comprender cómo las diferentes fuentes de error generan sesgo y varianza nos ayudará a mejorar el proceso de ajuste de datos, lo que resulta en modelos más precisos, adicionalmente también evitará el error de sobreajuste y falta de ajuste. 4.4.0.1 Errores reducibles Error por sesgo: Es la diferencia entre la predicción esperada de nuestro modelo y los valores verdaderos. Aunque al final nuestro objetivo es siempre construir modelos que puedan predecir datos muy cercanos a los valores verdaderos, no siempre es tan fácil porque algunos algoritmos son simplemente demasiado rígidos para aprender señales complejas del conjunto de datos. Imagina ajustar una regresión lineal a un conjunto de datos que tiene un patrón no lineal, no importa cuántas observaciones más recopiles, una regresión lineal no podrá modelar las curvas en esos datos. Esto se conoce como underfitting. Error por varianza: Se refiere a la cantidad que la estimación de la función objetivo cambiará si se utiliza diferentes datos de entrenamiento. La función objetivo se estima a partir de los datos de entrenamiento mediante un algoritmo de Machine Learning, por lo que deberíamos esperar que el algoritmo tenga alguna variación. Idealmente no debería cambiar demasiado de un conjunto de datos de entrenamiento a otro. Los algoritmos de Machine Learning que tienen una gran varianza están fuertemente influenciados por los detalles de los datos de entrenamiento, esto significa que los detalles de la capacitación influyen en el número y los tipos de parámetros utilizados para caracterizar la función de mapeo. 4.4.0.2 Error irreducible El error irreducible no se puede reducir, independientemente de qué algoritmo se usa. También se le conoce como ruido y, por lo general, proviene por factores como variables desconocidas que influyen en el mapeo de las variables de entrada a la variable de salida, un conjunto de características incompleto o un problema mal enmarcado. Acá es importante comprender que no importa cuán bueno hagamos nuestro modelo, nuestros datos tendrán cierta cantidad de ruido o un error irreductible que no se puede eliminar. 4.4.0.3 Balance entre sesgo y varianza o Trade-off El objetivo de cualquier algoritmo supervisado de Machine Learning es lograr un bias bajo, una baja varianza y a su vez el algoritmo debe lograr un buen rendimiento de predicción. El bias frente a la varianza se refiere a la precisión frente a la consistencia de los modelos entrenados por su algoritmo. Podemos diagnosticarlos de la siguiente manera: Los algoritmos de baja varianza (alto bias) tienden a ser menos complejos, con una estructura subyacente simple o rígida. Los algoritmos de bajo bias (alta varianza) tienden a ser más complejos, con una estructura subyacente flexible. No hay escapatoria a la relación entre el bias y la varianza en Machine Learning, aumentar el bias disminuirá la varianza, aumentar la varianza disminuirá el bias. 4.4.0.4 Error total Comprender el sesgo y la varianza es fundamental para comprender el comportamiento de los modelos de predicción, pero en general lo que realmente importa es el error general, no la descomposición específica. El punto ideal para cualquier modelo es el nivel de complejidad en el que el aumento en el sesgo es equivalente a la reducción en la varianza. Para construir un buen modelo, necesitamos encontrar un buen equilibrio entre el bias y la varianza de manera que minimice el error total. Un equilibrio óptimo de bias y varianza nunca sobreequiparía o no sería adecuado para el modelo. Por lo tanto comprender el bias y la varianza es fundamental para comprender el comportamiento de los modelos de predicción. 4.4.0.5 Overfitting El modelo es muy particular. Error debido a la varianza Durante el entrenamiento tiene un desempeño muy bueno, pero al pasar nuevos datos su desempeño es malo. 4.4.0.6 Underfitting El modelo es demasiado general. Error debido al sesgo. Durante el entrenamiento no tiene un buen desempeño. 4.5 Pre-procesamiento e ingeniería de datos Hay varios pasos que se deben de seguir para crear un modelo útil: Recopilación de datos. Limpieza de datos. Creación de nuevas variables. Estimación de parámetros. Selección y ajuste del modelo. Evaluación del rendimiento. Al comienzo de un proyecto, generalmente hay un conjunto finito de datos disponibles para todas estas tareas. OJO: A medida que los datos se reutilizan para múltiples tareas, aumentan los riesgos de agregar sesgos o grandes efectos de errores metodológicos. 4.5.1 Pre-procesamiento de datos Como punto de partida para nuestro flujo de trabajo de aprendizaje automático, necesitaremos datos de entrada. En la mayoría de los casos, estos datos se cargarán y almacenarán en forma de data frames o tibbles en R. Incluirán una o varias variables predictoras y, en caso de aprendizaje supervisado, también incluirán un resultado conocido. Sin embargo, no todos los modelos pueden lidiar con diferentes problemas de datos y, a menudo, necesitamos transformar los datos para obtener el mejor rendimiento posible del modelo. Este proceso se denomina pre-procesamiento y puede incluir una amplia gama de pasos, como: Dicotomización de variables: Variables cualitativas que solo pueden tomar el valor \\(0\\) o \\(1\\) para indicar la ausencia o presencia de una condición específica. Estas variables se utilizan para clasificar los datos en categorías mutuamente excluyentes o para activar comandos de encendido / apagado Near Zero Value (nzv) o Varianza Cero: En algunas situaciones, el mecanismo de generación de datos puede crear predictores que solo tienen un valor único (es decir, un “predictor de varianza cercando a cero”). Para muchos modelos (excluidos los modelos basados en árboles), esto puede hacer que el modelo se bloquee o que el ajuste sea inestable. De manera similar, los predictores pueden tener solo una pequeña cantidad de valores únicos que ocurren con frecuencias muy bajas. Imputaciones: Si faltan algunos predictores, ¿deberían estimarse mediante imputación? * Des-correlacionar: Si hay predictores correlacionados, ¿debería mitigarse esta correlación? Esto podría significar filtrar predictores, usar análisis de componentes principales o una técnica basada en modelos (por ejemplo, regularización). * Normalizar: ¿Deben centrarse y escalar los predictores? Transformar: ¿Es útil transformar los predictores para que sean más simétricos? (por ejemplo, escala logarítmica). Dependiendo del caso de uso, algunos pasos de pre-procesamiento pueden ser indispensables para pasos posteriores, mientras que otros solo son opcionales. Sin embargo, dependiendo de los pasos de pre-procesamiento elegidos, el rendimiento del modelo puede cambiar significativamente en pasos posteriores. Por lo tanto, es muy común probar varias configuraciones. En la tabla, \\(\\checkmark\\) indica que el método es obligatorio para el modelo y \\(\\times\\) indica que no lo es. El símbolo \\(\\circ\\) significa que la técnica puede ayudar al modelo, pero no es obligatorio. Notación: Es posible que la des-correlación de predictores no ayude a mejorar el rendimiento. Sin embargo, menos predictores correlacionados pueden mejorar la estimación de las puntuaciones de importancia de la varianza. Esencialmente, la selección de predictores altamente correlacionados es casi aleatoria. La notación \\(+\\) significa que la respuesta depende de la implementación: Teoricamente, cualquier modelo basado en árboles no requiere imputación de datos, sin embargo, muchas implementaciones de conjuntos de árboles requieren imputación. Si bien los métodos de refuerzo basados en árboles generalmente no requieren la creación de variables ficticias, los modelos que usan xgboost sí lo hacen. 4.5.2 Ingeniería de datos La ingeniería de datos abarca actividades que reformatean los valores de los predictores para que se puedan utilizar de manera eficaz para nuestro modelo. Esto incluye transformaciones y codificaciones de los datos para representar mejor sus características importantes. Por ejemplo: 1.- Supongamos que un conjunto de datos tiene dos predictores que se pueden representar de manera más eficaz en nuestro modelo como una proporción, así, tendríamos un nuevo predictor a partir de la proporción de los dos predictores originales. x x_prop 691 0.1836789 639 0.1698565 969 0.2575758 955 0.2538543 508 0.1350346 2.- Al elegir cómo codificar nuestros datos en el modelado, podríamos elegir una opción que creemos que está más asociada con el resultado. El formato original de los datos, por ejemplo numérico (edad) versus categórico (grupo). Edad Grupo 7 Niños 78 Adultos mayores 17 Adolescentes 25 Adultos 90 Adultos mayores La ingeniería y el pre-procesamiento de datos también pueden implicar el reformateo requerido por el modelo. Algunos modelos utilizan métricas de distancia geométrica y, en consecuencia, los predictores numéricos deben centrarse y escalar para que estén todos en las mismas unidades. De lo contrario, los valores de distancia estarían sesgados por la escala de cada columna. 4.6 Recetas Una receta es una serie de pasos o instrucciones para el procesamiento de datos. A diferencia del método de fórmula dentro de una función de modelado, la receta define los pasos sin ejecutarlos inmediatamente; es sólo una especificación de lo que se debe hacer. La estructura de una receta sigue los siguientes pasos: Inicialización Transformación Preparación Aplicación La siguiente sección explica la estructura y flujo de transformaciones: receta &lt;- recipe(response ~ X1 + X2 + X3 + ... + Xn, data = dataset ) %&gt;% transformation_1(...) %&gt;% transformation_2(...) %&gt;% transformation_3(...) %&gt;% ... final_transformation(...) %&gt;% prep() bake(receta, new_data = new_dataset) A continuación se muestran distintos ejemplos de transformaciones realizadas comunmente en el pre-procesamiento de modelos predictivos. Como ejemplo, utilizaremos el subconjunto de predictores disponibles en los datos de vivienda: Ames Vecindario (29 vecindarios) Superficie habitable bruta sobre el nivel del suelo Año de constricción Tipo de edificio ANTERIORMENTE… Un modelo de regresión lineal ordinario se ajustaba a los datos con la función estándar lm() de la siguiente manera: lm(Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) + Year_Built + Bldg_Type, data = ames) Cuando se ejecuta esta función, los datos se convierten en a una matriz de diseño numérico (también llamada matriz de modelo) y luego se utiliza el método de mínimos cuadrados para estimar los parámetros. Lo que hace la fórmula anterior se puede descomponer en una serie de pasos: 1.- El precio de venta se define como el resultado, mientras que las variables de vecindario, superficie habitable bruta, año de construcción y tipo de edificio se definen como predictores. 2.- Se aplica una transformación logarítmica al predictor de superficie habitable bruta. 3.- Las columnas de vecindad y tipo de edificio se convierten de un formato no numérico a un formato numérico (dado que los mínimos cuadrados requieren predictores numéricos). La siguiente receta es equivalente a la fórmula anterior: simple_ames &lt;- recipe( Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type, data = ames) %&gt;% step_log(Gr_Liv_Area, base = 10) %&gt;% step_dummy(all_nominal_predictors()) simple_ames ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 4 ## ## Operations: ## ## Log transformation on Gr_Liv_Area ## Dummy variables from all_nominal_predictors() Ventajas de usar una receta: Los cálculos se pueden reciclar entre modelos ya que no están estrechamente acoplados a la función de modelado. Una receta permite un conjunto más amplio de opciones de procesamiento de datos que las que pueden ofrecer las fórmulas. La sintaxis puede ser muy compacta. Por ejemplo, all_nominal_predictors() se puede usar para capturar muchas variables para tipos específicos de procesamiento, mientras que una fórmula requeriría que cada una se enumere explícitamente. Todo el procesamiento de datos se puede capturar en un solo objeto en lugar de tener scripts que se repiten o incluso se distribuyen en diferentes archivos. 4.6.1 Pasos y estructura de recetas Como se mostró anteriormente, existen 4 pasos fundamentales para el procesamiento y transformación de conjuntos de datos. Estos pasos se describen de la siguiente manera: Receta: Inicializa una receta y define los roles de las variables Transformaciones: Mutaciones a los renglones y columnas hasta desear el resultado Preparación: Se realizan las estimaciones estadísticas con los datos La función prep() estima las cantidades requeridas y las estadísticas necesarias para cualquier paso declarado en la receta. prep &lt;- prep(simple_ames) prep ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 4 ## ## Training data contained 2930 data points and no missing data. ## ## Operations: ## ## Log transformation on Gr_Liv_Area [trained] ## Dummy variables from Neighborhood, Bldg_Type [trained] Aplicación Se llevan a cabo las transformaciones especificadas en la receta preparada a un conjunto de datos. Finalmente, la función bake() lleva a cabo la transformación de un conjunto de datos a través de las estimaciones indicadas en una receta y aplica las operaciones a un conjunto de datos para crear una matriz de diseño. La función bake(object, new_data = NULL) devolverá los datos con los que se entrenó la receta. Nota: La función juice() devolverá los resultados de una receta en la que se hayan aplicado todos los pasos a los datos. Similar a la función bake() con el comando new_data = NULL. simple_ames %&gt;% prep() %&gt;% bake(new_data = NULL) %&gt;% glimpse() ## Rows: 2,930 ## Columns: 35 ## $ Gr_Liv_Area &lt;dbl&gt; 3.219060, 2.95230… ## $ Year_Built &lt;int&gt; 1960, 1961, 1958,… ## $ Sale_Price &lt;int&gt; 215000, 105000, 1… ## $ Neighborhood_College_Creek &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Old_Town &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Edwards &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Somerset &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Northridge_Heights &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Gilbert &lt;dbl&gt; 0, 0, 0, 0, 1, 1,… ## $ Neighborhood_Sawyer &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Northwest_Ames &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Sawyer_West &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Mitchell &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Brookside &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Crawford &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Iowa_DOT_and_Rail_Road &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Timberland &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Northridge &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Stone_Brook &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_South_and_West_of_Iowa_State_University &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Clear_Creek &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Meadow_Village &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Briardale &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Bloomington_Heights &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Veenker &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Northpark_Villa &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Blueste &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Greens &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Green_Hills &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Landmark &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Hayden_Lake &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Bldg_Type_TwoFmCon &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Bldg_Type_Duplex &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Bldg_Type_Twnhs &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Bldg_Type_TwnhsE &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… En cuanto a las transformaciones posibles, existe una gran cantidad de funciones que soportan este proceso. En esta sección se muestran algunas de las transformación más comunes, entre ellas: Normalización Dicotomización Creación de nuevas columnas Datos faltantes Imputaciones Interacciones Etc. 4.6.1.1 Normalizar columnas numéricas Quizá la transformación numérica más usada en todos los modelos es la estandarización o normalización de variables numéricas. Este proceso se realiza para homologar la escala de las variables numéricas, de modo que no predomine una sobre otra debido a la diferencia de magnitudes o escalas. Este proceso se tiene de fondo el siguiente proceso estadístico: \\[Z=\\frac{X-\\hat{\\mu}_x}{\\hat{\\sigma}_x}\\] Donde: X = Es una variable o columna numérica \\(\\hat{\\mu}_x\\) = Es la estimación de la media de la variable X \\(\\hat{\\sigma}_x\\) = Es la estimación de la desviación estándar de la variable X La librería recipes nos permite realizar este proceso ágilmente mediante la función: step_normalize(). ames %&gt;% select(Sale_Price, Neighborhood, Gr_Liv_Area, Year_Built, Bldg_Type) %&gt;% head(5) ## # A tibble: 5 × 5 ## Sale_Price Neighborhood Gr_Liv_Area Year_Built Bldg_Type ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; ## 1 215000 North_Ames 1656 1960 OneFam ## 2 105000 North_Ames 896 1961 OneFam ## 3 172000 North_Ames 1329 1958 OneFam ## 4 244000 North_Ames 2110 1968 OneFam ## 5 189900 Gilbert 1629 1997 OneFam simple_ames &lt;- recipe(Sale_Price ~ ., data = ames) %&gt;% step_normalize(all_numeric_predictors()) simple_ames ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 73 ## ## Operations: ## ## Centering and scaling for all_numeric_predictors() simple_ames %&gt;% prep() %&gt;% bake(new_data = NULL) %&gt;% select(Sale_Price, Neighborhood, Gr_Liv_Area, Year_Built, Bldg_Type) %&gt;% head(5) ## # A tibble: 5 × 5 ## Sale_Price Neighborhood Gr_Liv_Area Year_Built Bldg_Type ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 215000 North_Ames 0.309 -0.375 OneFam ## 2 105000 North_Ames -1.19 -0.342 OneFam ## 3 172000 North_Ames -0.338 -0.442 OneFam ## 4 244000 North_Ames 1.21 -0.111 OneFam ## 5 189900 Gilbert 0.256 0.848 OneFam 4.6.1.2 Dicotomización de categorías Otra transformación necesaria en la mayoría de los modelos predictivos en la creación de las variables dummy. Se mencionó anteriormente que los modelos requieren de una matriz numérica de características explicativas que permita calcular patrones estadísticos para predecir la variable de respuesta. El proceso de dicotomización consiste en crear una variable dicotómica por cada categoría de una columna con valores nominales. ames %&gt;% select(Sale_Price, Bldg_Type) %&gt;% head(5) ## # A tibble: 5 × 2 ## Sale_Price Bldg_Type ## &lt;int&gt; &lt;fct&gt; ## 1 215000 OneFam ## 2 105000 OneFam ## 3 172000 OneFam ## 4 244000 OneFam ## 5 189900 OneFam ames %&gt;% select(Bldg_Type) %&gt;% distinct() %&gt;% pull() ## [1] OneFam TwnhsE Twnhs Duplex TwoFmCon ## Levels: OneFam TwoFmCon Duplex Twnhs TwnhsE simple_ames &lt;- recipe(Sale_Price ~ Bldg_Type, data = ames) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% prep() simple_ames ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 1 ## ## Training data contained 2930 data points and no missing data. ## ## Operations: ## ## Dummy variables from Bldg_Type [trained] simple_ames %&gt;% bake(new_data = NULL) %&gt;% head(5) ## # A tibble: 5 × 5 ## Sale_Price Bldg_Type_TwoFmC… Bldg_Type_Duplex Bldg_Type_Twnhs Bldg_Type_TwnhsE ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 215000 0 0 0 0 ## 2 105000 0 0 0 0 ## 3 172000 0 0 0 0 ## 4 244000 0 0 0 0 ## 5 189900 0 0 0 0 El proceso de dicotomización demanda que únicamente (n-1) categorías sean expresadas, mientras que la restante será considerada la categoría default o basal. Esta última categoría es la usada en el modelo cuando todas las demás se encuentran ausentes. 4.6.1.3 Codificación de datos cualitativos nuevos o faltantes Una de las tareas de ingeniería de datos más comunes es el tratamiento de datos faltantes, datos no antes vistos y datos con poca frecuencia. El problema principal con estos casos es que los modelos no saben cómo relacionar estos eventos con futuras predicciones. Es conveniente realizar las transformaciones necesarias de tratamiento de estos datos antes de pasar a la etapa de modelado. Por ejemplo: step_unknown() cambia los valores perdidos en un nivel de factor “desconocido.” step_other() analiza las frecuencias de los niveles de los factores en el conjunto de datos y convierte los valores que ocurren con poca frecuencia a un nivel general de “otro,” con un umbral que se puede especificar. step_novel() puede asignar un nuevo nivel si anticipamos que se puede encontrar un nuevo factor en datos futuros. Un buen ejemplo es el predictor de vecindad en nuestros datos. Aquí hay dos vecindarios que tienen menos de cinco propiedades. ggplot(data = ames, aes(y = Neighborhood)) + geom_bar() + labs(y = NULL) Para algunos modelos, puede resultar problemático tener variables dummy con una sola entrada distinta de cero en la columna. Como mínimo, es muy improbable que estas características sean importantes para un modelo. Si agregamos step_other (Neighborhood, threshold = 0.01) a nuestra receta, el último \\(1\\%\\) de los vecindarios se agrupará en un nuevo nivel llamado “otro,” esto atrapará a 8 vecindarios. simple_ames &lt;- recipe( Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type, data = ames) %&gt;% step_other(Neighborhood, threshold = 0.01) %&gt;% prep() ejemplo &lt;- juice(simple_ames) ggplot(ejemplo, aes(y = Neighborhood)) + geom_bar() + labs(y = NULL) 4.6.2 Imputaciones La función step_unknown crea una categoría nombrada unknown, la cual sirve como reemplazo de datos categóricos faltantes, sin embargo, para imputar datos numéricos se requiere de otra estrategia. Las imputaciones o sustituciones más comunes son realizadas a través de medidas de tendencia central tales como la media y mediana. A continuación se muestra un ejemplo: ames_na &lt;- ames ames_na[sample(nrow(ames), 5), c(&quot;Gr_Liv_Area&quot;, &quot;Lot_Area&quot;)] &lt;- NA ames_na %&gt;% filter(is.na(Gr_Liv_Area) | is.na(Lot_Area)) %&gt;% select(Sale_Price, Gr_Liv_Area, Lot_Area) ## # A tibble: 5 × 3 ## Sale_Price Gr_Liv_Area Lot_Area ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 224000 NA NA ## 2 121500 NA NA ## 3 159434 NA NA ## 4 135960 NA NA ## 5 214000 NA NA simple_ames &lt;- recipe(Sale_Price ~ Gr_Liv_Area + Lot_Area, data = ames_na) %&gt;% step_impute_mean(Gr_Liv_Area) %&gt;% step_impute_median(Lot_Area) %&gt;% prep() bake(simple_ames, new_data = ames_na) %&gt;% filter(is.na(Gr_Liv_Area) | is.na(Lot_Area)) ## # A tibble: 0 × 3 ## # … with 3 variables: Gr_Liv_Area &lt;int&gt;, Lot_Area &lt;int&gt;, Sale_Price &lt;int&gt; Forzamos algunos renglones a que sean omitidos aleatoriamente. Posteriormente, estos valores son imputados mediante su media y mediana. 4.6.3 Agregar o modificar columnas Quizá la transformación más usada sea la agregación o mutación de columnas existentes. Similar a la función mutate() de dplyr, la función step_mutate() se encarga de realizar esta tarea dentro de un pipeline o receta. ejemplo &lt;- recipe( Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Year_Remod_Add, data = ames) %&gt;% step_mutate( Sale_Price_Peso = Sale_Price * 19.87, Last_Inversion = Year_Remod_Add - Year_Built ) %&gt;% step_arrange(desc(Last_Inversion)) %&gt;% prep() ejemplo ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 5 ## ## Training data contained 2930 data points and no missing data. ## ## Operations: ## ## Variable mutation for ~Sale_Price * 19.87, ~Year_Remod_Add - Yea... [trained] ## Row arrangement using ~desc(Last_Inversion) [trained] ejemplo %&gt;% bake(new_data = NULL) %&gt;% select(Sale_Price, Sale_Price_Peso, Year_Remod_Add, Year_Built, Last_Inversion) ## # A tibble: 2,930 × 5 ## Sale_Price Sale_Price_Peso Year_Remod_Add Year_Built Last_Inversion ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 131000 2602970 2007 1880 127 ## 2 265979 5285003. 2003 1880 123 ## 3 295000 5861650 2002 1880 122 ## 4 94000 1867780 1996 1875 121 ## 5 138000 2742060 2006 1890 116 ## 6 122000 2424140 1987 1872 115 ## 7 240000 4768800 2002 1890 112 ## 8 119600 2376452 2006 1895 111 ## 9 124000 2463880 1991 1880 111 ## 10 100000 1987000 1995 1885 110 ## # … with 2,920 more rows En este ejemplo se realiza la creación de una nueva variable y la modificación de una ya existente. 4.6.4 Interacciones Los efectos de interacción involucran dos o más predictores. Tal efecto ocurre cuando un predictor tiene un efecto sobre el resultado que depende de uno o más predictores. Numéricamente, un término de interacción entre predictores se codifica como su producto. Las interacciones solo se definen en términos de su efecto sobre el resultado y pueden ser combinaciones de diferentes tipos de datos (por ejemplo, numéricos, categóricos, etc.). Después de explorar el conjunto de datos de Ames, podríamos encontrar que las pendientes de regresión para el área habitable bruta difieren para los diferentes tipos de edificios: ggplot(ames, aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_point(alpha = .2) + facet_wrap(~ Bldg_Type) + geom_smooth(method = lm, formula = y ~ x, se = FALSE, col = &quot;red&quot;) + scale_x_log10() + scale_y_log10() + labs(x = &quot;Gross Living Area&quot;, y = &quot;Sale Price (USD)&quot;) Con la receta actual, step_dummy() ya ha creado variables ficticias. ¿Cómo combinaríamos estos para una interacción? El paso adicional se vería como step_interact(~ términos de interacción) donde los términos en el lado derecho de la tilde son las interacciones. Estos pueden incluir selectores, por lo que sería apropiado usar: simple_ames &lt;- recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type, data = ames) %&gt;% step_other(Neighborhood, threshold = 0.05) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_interact( ~ Gr_Liv_Area:starts_with(&quot;Bldg_Type_&quot;) ) %&gt;% prep() simple_ames %&gt;% bake(new_data = NULL) %&gt;% glimpse() ## Rows: 2,930 ## Columns: 19 ## $ Gr_Liv_Area &lt;int&gt; 1656, 896, 1329, 2110, 1629, 1604, 13… ## $ Year_Built &lt;int&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2… ## $ Sale_Price &lt;int&gt; 215000, 105000, 172000, 244000, 18990… ## $ Neighborhood_College_Creek &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Old_Town &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Edwards &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Somerset &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Northridge_Heights &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Gilbert &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1… ## $ Neighborhood_Sawyer &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Neighborhood_other &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0… ## $ Bldg_Type_TwoFmCon &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_Duplex &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_Twnhs &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_TwnhsE &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0… ## $ Gr_Liv_Area_x_Bldg_Type_TwoFmCon &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Gr_Liv_Area_x_Bldg_Type_Duplex &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Gr_Liv_Area_x_Bldg_Type_Twnhs &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Gr_Liv_Area_x_Bldg_Type_TwnhsE &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1338, 1280, 1616, 0… Se pueden especificar interacciones adicionales en esta fórmula separándolas con el signo \\(*\\). 4.6.5 Transformaciones generales Reflejando las operaciones originales de dplyr, los siguientes pasos se pueden usar para realizar una variedad de operaciones básicas a los datos. step_select(): Selecciona un subconjunto de variables específicas en el conjunto de datos. step_mutate(): Crea una nueva variable o modifica una existente usando dplyr::mutate(). step_mutate_at(): Lee una especificación de un paso de receta que modificará las variables seleccionadas usando una función común a través de dplyr::mutate_at(). step_filter(): Crea una especificación de un paso de receta que eliminará filas usando dplyr::filter(). step_arrange(): Ordena el conjunto de datos de acuerdo con una o más variables. step_rm(): Crea una especificación de un paso de receta que eliminará las variables según su nombre, tipo o función. step_nzv(): Realiza una selección de variables eliminando todas aquellas cuya varianza se encuentre cercana a cero. step_naomit(): Elimina todos los renglones que tengan alguna variable con valores perdidos. step_normalize(): Centra y escala las variables numéricas especificadas, generando una transformación a una distribución normal estándar. step_range(): Transforma el rango de un conjunto de variables numéricas al especificado. step_interact(): Crea un nuevo conjunto de variables basadas en la interacción entre dos variables. step_ratio(): Crea una nueva variable a partir del cociente entre dos variables. all_predictors(): Selecciona a todos los predictores del conjunto de entrenamineto para aplicarles alguna de las funciones mencionadas. all_numeric_predictors(): Selecciona a todos los predictores numéricos del conjunto de entrenamineto para aplicarles alguna de las funciones mencionadas. all_nominal_predictors(): Selecciona a todos los predictores nominales del conjunto de entrenamineto para aplicarles alguna de las funciones mencionadas. La guía completa de las familia de funciones step puede consultarse en la documentación oficial 4.7 Partición de datos Cuando hay una gran cantidad de datos disponibles, una estrategia inteligente es asignar subconjuntos específicos de datos para diferentes tareas, en lugar de asignar la mayor cantidad posible solo a la estimación de los parámetros del modelo. Si el conjunto inicial de datos no es lo suficientemente grande, habrá cierta superposición de cómo y cuándo se asignan nuestros datos, y es importante contar con una metodología sólida para la partición de datos. 4.7.1 Métodos comunes para particionar datos El enfoque principal para la validación del modelo es dividir el conjunto de datos existente en dos conjuntos distintos: Entrenamiento: Este conjunto suele contener la mayoría de los datos, los cuales sirven para la construcción de modelos donde se pueden ajustar diferentes modelos, se investigan estrategias de ingeniería de características, etc. La mayor parte del proceso de modelado se utiliza este conjunto. Prueba: La otra parte de las observaciones se coloca en este conjunto. Estos datos se mantienen en reserva hasta que se elijan uno o dos modelos como los de mejor rendimiento. El conjunto de prueba se utiliza como árbitro final para determinar la eficiencia del modelo, por lo que es fundamental mirar el conjunto de prueba una sola vez. Supongamos que asignamos el \\(80\\%\\) de los datos al conjunto de entrenamiento y el \\(20\\%\\) restante a las pruebas. El método más común es utilizar un muestreo aleatorio simple. El paquete rsample tiene herramientas para realizar divisiones de datos como esta; la función initial_split() fue creada para este propósito. library(tidymodels) tidymodels_prefer() # Fijar un número aleatorio con para que los resultados puedan ser reproducibles set.seed(123) # Partición 80/20 de los datos ames_split &lt;- initial_split(ames, prop = 0.80) ames_split ## &lt;Analysis/Assess/Total&gt; ## &lt;2344/586/2930&gt; La información impresa denota la cantidad de datos en el conjunto de entrenamiento \\((n = 2,344)\\), la cantidad en el conjunto de prueba \\((n = 586)\\) y el tamaño del grupo original de muestras \\((n = 2,930)\\). El objeto ames_split es un objeto rsplit y solo contiene la información de partición; para obtener los conjuntos de datos resultantes, aplicamos dos funciones más: ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) dim(ames_train) ## [1] 2344 74 El muestreo aleatorio simple es apropiado en muchos casos, pero hay excepciones. Cuando hay un desbalance de clases en los problemas de clasificación, el uso de una muestra aleatoria simple puede asignar al azar estas muestras poco frecuentes de manera desproporcionada al conjunto de entrenamiento o prueba. Para evitar esto, se puede utilizar un muestreo estratificado. La división de entrenamiento/prueba se lleva a cabo por separado dentro de cada clase y luego estas submuestras se combinan en el conjunto general de entrenamiento y prueba. Para los problemas de regresión, los datos de los resultados se pueden agrupar artificialmente en cuartiles y luego realizar un muestreo estratificado cuatro veces por separado. Este es un método eficaz para mantener similares las distribuciones del resultado entre el conjunto de entrenamiento y prueba. Observamos que la distribución del precio de venta está sesgada a la derecha. Las casas más caras no estarían bien representadas en el conjunto de entrenamiento con una simple partición; esto aumentaría el riesgo de que nuestro modelo sea ineficaz para predecir el precio de dichas propiedades. Las líneas verticales punteadas indican los cuatro cuartiles para estos datos. Una muestra aleatoria estratificada llevaría a cabo la división 80/20 dentro de cada uno de estos subconjuntos de datos y luego combinaría los resultados. En rsample, esto se logra usando el argumento de estratos: set.seed(123) ames_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) Hay muy pocas desventajas en el uso de muestreo estratificado. Un caso es cuando los datos tienen un componente de tiempo, como los datos de series de tiempo. Aquí, es más común utilizar los datos más recientes como conjunto de prueba. El paquete rsample contiene una función llamada initial_time_split() que es muy similar a initial_split(). En lugar de usar un muestreo aleatorio, el argumento prop denota qué proporción de la primera parte de los datos debe usarse como conjunto de entrenamiento; la función asume que los datos se han clasificado previamente en un orden apropiado. 4.7.2 ¿Qué proporción debería ser usada? No hay un porcentaje de división óptimo para el conjunto de entrenamiento y prueba. Muy pocos datos en el conjunto de entrenamiento obstaculizan la capacidad del modelo para encontrar estimaciones de parámetros adecuadas y muy pocos datos en el conjunto de prueba reducen la calidad de las estimaciones de rendimiento. Se debe elegir un porcentaje que cumpla con los objetivos de nuestro proyecto con consideraciones que incluyen: Costo computacional en el entrenamiento del modelo. Costo computacional en la evaluación del modelo. Representatividad del conjunto de formación. Representatividad del conjunto de pruebas. Los porcentajes de división más comunes comunes son: Entrenamiento: \\(80\\%\\), Prueba: \\(20\\%\\) Entrenamiento: \\(67\\%\\), Prueba: \\(33\\%\\) Entrenamiento: \\(50\\%\\), Prueba: \\(50\\%\\) 4.7.3 Conjunto de validación El conjunto de validación se definió originalmente cuando los investigadores se dieron cuenta de que medir el rendimiento del conjunto de entrenamiento conducía a resultados que eran demasiado optimistas. Esto llevó a modelos que se sobreajustaban, lo que significa que se desempeñaron muy bien en el conjunto de entrenamiento pero mal en el conjunto de prueba. Para combatir este problema, se retuvo un pequeño conjunto de datos de validación y se utilizó para medir el rendimiento del modelo mientras este está siendo entrenado. Una vez que la tasa de error del conjunto de validación comenzara a aumentar, la capacitación se detendría. En otras palabras, el conjunto de validación es un medio para tener una idea aproximada de qué tan bien se desempeñó el modelo antes del conjunto de prueba. Los conjuntos de validación se utilizan a menudo cuando el conjunto de datos original es muy grande. En este caso, una sola partición grande puede ser adecuada para caracterizar el rendimiento del modelo sin tener que realizar múltiples iteraciones de remuestreo. Con rsample, un conjunto de validación es como cualquier otro objeto de remuestreo; este tipo es diferente solo en que tiene una sola iteración set.seed(12) val_set &lt;- validation_split(ames_train, prop = 3/4, strata = NULL) val_set #val_set contiene el conjunto de entrenamiento y validación. ## # Validation Set Split (0.75/0.25) ## # A tibble: 1 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [1756/586]&gt; validation Esta función regresa una columna para los objetos de división de datos y una columna llamada id que tiene una cadena de caracteres con el identificador de remuestreo. El argumento de estratos hace que el muestreo aleatorio se lleve a cabo dentro de la variable de estratificación. Esto puede ayudar a garantizar que el número de datos en los datos del análisis sea equivalente a las proporciones del conjunto de datos original. (Los estratos inferiores al 10% del total se agrupan). Otra opción de muestreo bastante común es la realizada mediante múltiples submuestras de los datos originales. Diversos métodos se revisarán a lo largo del curso. 4.7.4 Leave-one-out cross-validation La validación cruzada es una manera de predecir el ajuste de un modelo a un hipotético conjunto de datos de prueba cuando no disponemos del conjunto explícito de datos de prueba. El método LOOCV en un método iterativo que se inicia empleando como conjunto de entrenamiento todas las observaciones disponibles excepto una, que se excluye para emplearla como validación. Si se emplea una única observación para calcular el error, este varía mucho dependiendo de qué observación se haya seleccionado. Para evitarlo, el proceso se repite tantas veces como observaciones disponibles se tengan, excluyendo en cada iteración una observación distinta, ajustando el modelo con el resto y calculando el error con dicha observación. Finalmente, el error estimado por el es el promedio de todos lo \\(i\\) errores calculados. La principal desventaja de este método es su costo computacional. El proceso requiere que el modelo sea reajustado y validado tantas veces como observaciones disponibles se tengan lo que en algunos casos puede ser muy complicado. rsample contiene la función loo_cv(). set.seed(55) ames_loo &lt;- loo_cv(ames_train) ames_loo ## # Leave-one-out cross-validation ## # A tibble: 2,342 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [2341/1]&gt; Resample1 ## 2 &lt;split [2341/1]&gt; Resample2 ## 3 &lt;split [2341/1]&gt; Resample3 ## 4 &lt;split [2341/1]&gt; Resample4 ## 5 &lt;split [2341/1]&gt; Resample5 ## 6 &lt;split [2341/1]&gt; Resample6 ## 7 &lt;split [2341/1]&gt; Resample7 ## 8 &lt;split [2341/1]&gt; Resample8 ## 9 &lt;split [2341/1]&gt; Resample9 ## 10 &lt;split [2341/1]&gt; Resample10 ## # … with 2,332 more rows 4.7.4.1 Cálculo del error En la validación cruzada dejando uno fuera se realizan tantas iteraciones como muestras \\((N)\\) tenga el conjunto de datos. De forma que para cada una de las \\(N\\) iteraciones se realiza un cálculo de error. El resultado final se obtiene realizando la media de los \\(N\\) errores obtenidos, según la fórmula: \\[E = \\frac{1}{N}\\sum_{i = 1}^N E_i\\] 4.7.5 V Fold Cross Validation En la validación cruzada de V iteraciones (V Fold Cross Validation) los datos de muestra se dividen en V subconjuntos. Uno de los subconjuntos se utiliza como datos de prueba y el resto \\((V-1)\\) como datos de entrenamiento. El proceso de validación cruzada es repetido durante \\(v\\) iteraciones, con cada uno de los posibles subconjuntos de datos de prueba. Finalmente se obtiene el promedio de los rendimientos de cada iteración para obtener un único resultado. Lo más común es utilizar la validación cruzada de 10 iteraciones. Este método de validación cruzada se utiliza principalmente para: Estimar el error cuando nuestro conjunto de prueba es muy pequeño. Es decir, se tiene la misma confuguración de parámetos y solamente cambia el conjunto de prueba y validación. Encontrar lo mejores hiperparámetros que ajusten mejor el modelo. Es decir, en cada bloque se tiene una configuración de hiperparámetros distinto y se seleccionará aquellos hiperparámetros que hayan producido el error más pequeño. En la función vfold_cv() la entrada principal es el conjunto de entrenamiento, así como el número de bloques: set.seed(55) ames_folds &lt;- vfold_cv(ames_train, v = 10) ames_folds ## # 10-fold cross-validation ## # A tibble: 10 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [2107/235]&gt; Fold01 ## 2 &lt;split [2107/235]&gt; Fold02 ## 3 &lt;split [2108/234]&gt; Fold03 ## 4 &lt;split [2108/234]&gt; Fold04 ## 5 &lt;split [2108/234]&gt; Fold05 ## 6 &lt;split [2108/234]&gt; Fold06 ## 7 &lt;split [2108/234]&gt; Fold07 ## 8 &lt;split [2108/234]&gt; Fold08 ## 9 &lt;split [2108/234]&gt; Fold09 ## 10 &lt;split [2108/234]&gt; Fold10 La columna denominada splits contiene la información sobre cómo dividir los datos (similar al objeto utilizado para crear la partición inicial de entrenamiento / prueba). Si bien cada fila de divisiones tiene una copia incrustada de todo el conjunto de entrenamiento, R es lo suficientemente inteligente como para no hacer copias de los datos en la memoria. El método de impresión dentro del tibble muestra la frecuencia de cada uno: [2K / 230] indica que aproximadamente dos mil muestras están en el conjunto de análisis y 230 están en ese conjunto de evaluación en particular. Estos objetos rsample también contienen siempre una columna de caracteres llamada id que etiqueta la partición. Algunos métodos de remuestreo requieren varios campos de identificación. Para recuperar manualmente los datos particionados, las funciones de analysis() y assessment() devuelven los de datos de análisis y evaluación respectivamente. # Primer bloque ames_folds$splits[[1]] %&gt;% analysis() %&gt;% # O assessment() head(7) ## # A tibble: 7 × 74 ## MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 One_Story_1946_and_New… Resident… 70 8400 Pave No_A… Regular ## 2 Two_Story_PUD_1946_and… Resident… 21 1680 Pave No_A… Regular ## 3 Two_Story_PUD_1946_and… Resident… 21 1680 Pave No_A… Regular ## 4 Two_Story_PUD_1946_and… Resident… 21 1680 Pave No_A… Regular ## 5 One_Story_PUD_1946_and… Resident… 53 4043 Pave No_A… Regular ## 6 One_Story_PUD_1946_and… Resident… 24 2280 Pave No_A… Regular ## 7 One_Story_PUD_1946_and… Resident… 50 7175 Pave No_A… Regular ## # … with 67 more variables: Land_Contour &lt;fct&gt;, Utilities &lt;fct&gt;, ## # Lot_Config &lt;fct&gt;, Land_Slope &lt;fct&gt;, Neighborhood &lt;fct&gt;, Condition_1 &lt;fct&gt;, ## # Condition_2 &lt;fct&gt;, Bldg_Type &lt;fct&gt;, House_Style &lt;fct&gt;, Overall_Cond &lt;fct&gt;, ## # Year_Built &lt;int&gt;, Year_Remod_Add &lt;int&gt;, Roof_Style &lt;fct&gt;, Roof_Matl &lt;fct&gt;, ## # Exterior_1st &lt;fct&gt;, Exterior_2nd &lt;fct&gt;, Mas_Vnr_Type &lt;fct&gt;, ## # Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;fct&gt;, Foundation &lt;fct&gt;, Bsmt_Cond &lt;fct&gt;, ## # Bsmt_Exposure &lt;fct&gt;, BsmtFin_Type_1 &lt;fct&gt;, BsmtFin_SF_1 &lt;dbl&gt;, … 4.7.6 Medidas de ajuste Las medidas de ajuste obtenidas pueden ser utilizadas para estimar cualquier medida cuantitativa de ajuste apropiada para los datos y el modelo. En un modelo basado en clasificación binaria, para resumir el ajuste del modelo se pueden usar las medidas: Tasa de error de clasificación (Accuracy) Precisión Sensibilidad o coertura (Recall) Especificidad Cuando el valor a predecir se distribuye de forma continua se puede calcular el error utilizando medidas como: Error porcentual absoluto medio (MAPE) Error absoluto medio (MAE) Error cuadrático medio (MSE) Raíz del error cuadrático medio (RMSE) Raíz del error logarítmico cuadrático medio (RMLSE) \\(R^2\\) (Coeficiente de determinación) \\(R^2_a\\) (Coeficiente de determinación ajustado) 4.7.6.1 Cálculo del error En cada una de las \\(v\\) iteraciones de este tipo de validación se realiza un cálculo de error. El resultado final lo obtenemos a partir de realizar la media de los \\(V\\) valores de errores obtenidos, según la fórmula: \\[E = \\frac{1}{V}\\sum_{i = 1}^vE_i\\] 4.7.7 Validación cruzada para series de tiempo En este procedimiento, hay una serie de conjuntos de prueba, cada uno de los cuales consta de una única observación. El conjunto de entrenamiento correspondiente consta solo de observaciones que ocurrieron antes de la observación que forma el conjunto de prueba. Por lo tanto, no se pueden utilizar observaciones futuras para construir el pronóstico. El siguiente diagrama ilustra la serie de conjuntos de entrenamiento y prueba, donde las observaciones azules forman los conjuntos de entrenamiento y las observaciones rojas forman los conjuntos de prueba. La precisión del pronóstico se calcula promediando los conjuntos de prueba. Este procedimiento a veces se conoce como “evaluación en un origen de pronóstico continuo” porque el “origen” en el que se basa el pronóstico avanza en el tiempo. Con los pronósticos de series de tiempo, los pronósticos de un paso pueden no ser tan relevantes como los pronósticos de varios pasos. En este caso, el procedimiento de validación cruzada basado en un origen de pronóstico continuo se puede modificar para permitir el uso de errores de varios pasos. Suponga que estamos interesados en modelos que producen buenos pronósticos de 4 pasos por delante. Entonces el diagrama correspondiente se muestra a continuación. La validación cruzada de series de tiempo se implementa con la función tsCV() del paquete forecast. "],["aprendizaje-supervisado-1.html", "Capítulo 5 APRENDIZAJE SUPERVISADO 5.1 Datos y tipos de modelos: 5.2 Regresión Lineal 5.3 Regresión Logística 5.4 Regularización 5.5 K-Nearest-Neighbor 5.6 Árboles de decisión 5.7 Bagging 5.8 Random Forest 5.9 Comparación de modelos", " Capítulo 5 APRENDIZAJE SUPERVISADO 5.1 Datos y tipos de modelos: En este curso se realizarán ejemplos tanto de regresión como de clasificación. Cada uno de los modelos a estudiar se implementarán tanto para respuestas continuas como variables categóricas 5.1.1 Regresión: Preparación de datos En esta sección, prepararemos datos para ajustar modelos de regresión y de clasificación, usando la paquetería recipes. Primero ajustaremos la receta, después obtendremos la receta actualizada con las estimaciones y al final el conjunto de datos listo para el modelo. 5.1.1.1 Datos de regresión: Ames Housing Data Los datos que usaremos son los de Ames Housing Data, el conjunto de datos contiene información de la Ames Assessor’s Office utilizada para calcular valuaciones para propiedades residenciales individuales vendidas en Ames, IA, de 2006 a 2010. Podemos encontrar más información en el siguiente link Ames Housing Data. library(tidymodels) library(stringr) library(tidyverse) data(ames) names(ames) ## [1] &quot;MS_SubClass&quot; &quot;MS_Zoning&quot; &quot;Lot_Frontage&quot; ## [4] &quot;Lot_Area&quot; &quot;Street&quot; &quot;Alley&quot; ## [7] &quot;Lot_Shape&quot; &quot;Land_Contour&quot; &quot;Utilities&quot; ## [10] &quot;Lot_Config&quot; &quot;Land_Slope&quot; &quot;Neighborhood&quot; ## [13] &quot;Condition_1&quot; &quot;Condition_2&quot; &quot;Bldg_Type&quot; ## [16] &quot;House_Style&quot; &quot;Overall_Cond&quot; &quot;Year_Built&quot; ## [19] &quot;Year_Remod_Add&quot; &quot;Roof_Style&quot; &quot;Roof_Matl&quot; ## [22] &quot;Exterior_1st&quot; &quot;Exterior_2nd&quot; &quot;Mas_Vnr_Type&quot; ## [25] &quot;Mas_Vnr_Area&quot; &quot;Exter_Cond&quot; &quot;Foundation&quot; ## [28] &quot;Bsmt_Cond&quot; &quot;Bsmt_Exposure&quot; &quot;BsmtFin_Type_1&quot; ## [31] &quot;BsmtFin_SF_1&quot; &quot;BsmtFin_Type_2&quot; &quot;BsmtFin_SF_2&quot; ## [34] &quot;Bsmt_Unf_SF&quot; &quot;Total_Bsmt_SF&quot; &quot;Heating&quot; ## [37] &quot;Heating_QC&quot; &quot;Central_Air&quot; &quot;Electrical&quot; ## [40] &quot;First_Flr_SF&quot; &quot;Second_Flr_SF&quot; &quot;Gr_Liv_Area&quot; ## [43] &quot;Bsmt_Full_Bath&quot; &quot;Bsmt_Half_Bath&quot; &quot;Full_Bath&quot; ## [46] &quot;Half_Bath&quot; &quot;Bedroom_AbvGr&quot; &quot;Kitchen_AbvGr&quot; ## [49] &quot;TotRms_AbvGrd&quot; &quot;Functional&quot; &quot;Fireplaces&quot; ## [52] &quot;Garage_Type&quot; &quot;Garage_Finish&quot; &quot;Garage_Cars&quot; ## [55] &quot;Garage_Area&quot; &quot;Garage_Cond&quot; &quot;Paved_Drive&quot; ## [58] &quot;Wood_Deck_SF&quot; &quot;Open_Porch_SF&quot; &quot;Enclosed_Porch&quot; ## [61] &quot;Three_season_porch&quot; &quot;Screen_Porch&quot; &quot;Pool_Area&quot; ## [64] &quot;Pool_QC&quot; &quot;Fence&quot; &quot;Misc_Feature&quot; ## [67] &quot;Misc_Val&quot; &quot;Mo_Sold&quot; &quot;Year_Sold&quot; ## [70] &quot;Sale_Type&quot; &quot;Sale_Condition&quot; &quot;Sale_Price&quot; ## [73] &quot;Longitude&quot; &quot;Latitude&quot; 5.1.1.2 Separación de los datos El primer paso para crear un modelo de regresión es dividir nuestros datos originales en un conjunto de entrenamiento y prueba. No hay que olvidar usar siempre una semilla con la función set.seed() para que sus resultados sean reproducibles. Primero usaremos la función initial_split() de rsample para dividir los datos ames en conjuntos de entrenamiento y prueba. Usamos el parámetro prop para indicar la proporción de los conjuntos train y test. set.seed(4595) ames_split &lt;- initial_split(ames, prop = 0.75) El objeto ames_split es un objeto rsplit y solo contiene la información de partición, para obtener los conjuntos de datos resultantes, aplicamos dos funciones adicionales, training y testing. ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) Estos objetos son data frames con las mismas columnas que los datos originales, pero solo las filas apropiadas para cada conjunto. También existe la función vfold_cv que se usa para crear v particiones del conjunto de entrenamiento. set.seed(2453) ames_folds&lt;- vfold_cv(ames_train) Ya con los conjuntos de entrenamiento y prueba definidos, iniciaremos con feature engeeniring sobre el conjunto de entrenamiento. 5.1.1.3 Definición de la receta Ahora usaremos la función vista en la sección anterior, recipe(), para definir los pasos de preprocesamiento antes de usar los datos para modelado. Usamos la función step_mutate() para generar nuevas variables dentro de la receta. La función step_interact() nos ayuda a crear nuevas variables que son interacciones entre las variables especificadas. Con la función step_ratio() creamos proporciones con las variables especificadas. forcats::fct_collapse() se usa para recategorizar variables, colapsando categorías de la variable. step_relevel nos ayuda a asignar la categoria deseada de una variable como referencia. step_normalize() es de gran utilidad ya que sirve para normalizar las variables que se le indiquen. step_dummy() Nos ayuda a crear variables One Hot Encoding. Por último usamos la función step_rm() para eliminar variables que no son de utilidad para el modelo. Ahora crearemos algunas variables auxiliares que podrían ser de utilidad para el ajuste de un modelo de regresión, entre ella: Log(Sale_Price), la cual será la variable a predecir. Una vez que el modelo haga la predicción del logaritmo del precio, es importante calcular con la función exponencial el precio real. receta_casas &lt;- recipe(Sale_Price ~ . , data = ames_train) %&gt;% step_log(Sale_Price, skip = T) %&gt;% step_unknown(Alley) %&gt;% step_rename(Year_Remod = Year_Remod_Add) %&gt;% step_rename(ThirdSsn_Porch = Three_season_porch) %&gt;% step_ratio(Bedroom_AbvGr, denom = denom_vars(Gr_Liv_Area)) %&gt;% step_mutate( Age_House = Year_Sold - Year_Remod, TotalSF = Gr_Liv_Area + Total_Bsmt_SF, AvgRoomSF = Gr_Liv_Area / TotRms_AbvGrd, Pool = if_else(Pool_Area &gt; 0, 1, 0), Exter_Cond = forcats::fct_collapse(Exter_Cond, Good = c(&quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;))) %&gt;% step_relevel(Exter_Cond, ref_level = &quot;Good&quot;) %&gt;% step_normalize(all_predictors(), -all_nominal()) %&gt;% step_dummy(all_nominal()) %&gt;% step_interact(~ Second_Flr_SF:First_Flr_SF) %&gt;% step_interact(~ matches(&quot;Bsmt_Cond&quot;):TotRms_AbvGrd) %&gt;% step_rm( First_Flr_SF, Second_Flr_SF, Year_Remod, Bsmt_Full_Bath, Bsmt_Half_Bath, Kitchen_AbvGr, BsmtFin_Type_1_Unf, Total_Bsmt_SF, Kitchen_AbvGr, Pool_Area, Gr_Liv_Area, Sale_Type_Oth, Sale_Type_VWD ) %&gt;% prep() Recordemos que la función recipe() solo son los pasos a seguir, necesitamos usar la función prep() que nos devuelve una receta actualizada con las estimaciones y la función juice() que nos devuelve la matriz de diseño. casa_juiced &lt;- juice(receta_casas) casa_juiced ## # A tibble: 2,197 × 275 ## Lot_Frontage Lot_Area Year_Built Mas_Vnr_Area BsmtFin_SF_1 BsmtFin_SF_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.214 0.604 0.848 -0.572 -0.512 -0.293 ## 2 0.363 -0.216 -0.114 -0.572 -0.512 -0.293 ## 3 1.05 -0.0159 1.08 -0.572 1.26 -0.293 ## 4 -0.173 4.87 1.15 3.49 -0.512 -0.293 ## 5 0.512 0.256 1.11 0.526 1.26 -0.293 ## 6 -0.233 -0.696 -0.844 -0.572 1.26 -0.293 ## 7 0.125 -0.261 -0.446 -0.572 -0.512 -0.293 ## 8 0.571 -0.193 -0.479 -0.572 0.819 -0.293 ## 9 0.274 0.704 0.981 -0.572 1.26 -0.293 ## 10 0.0651 -0.356 -0.712 -0.572 0.375 -0.293 ## # … with 2,187 more rows, and 269 more variables: Bsmt_Unf_SF &lt;dbl&gt;, ## # Full_Bath &lt;dbl&gt;, Half_Bath &lt;dbl&gt;, Bedroom_AbvGr &lt;dbl&gt;, TotRms_AbvGrd &lt;dbl&gt;, ## # Fireplaces &lt;dbl&gt;, Garage_Cars &lt;dbl&gt;, Garage_Area &lt;dbl&gt;, Wood_Deck_SF &lt;dbl&gt;, ## # Open_Porch_SF &lt;dbl&gt;, Enclosed_Porch &lt;dbl&gt;, ThirdSsn_Porch &lt;dbl&gt;, ## # Screen_Porch &lt;dbl&gt;, Misc_Val &lt;dbl&gt;, Mo_Sold &lt;dbl&gt;, Year_Sold &lt;dbl&gt;, ## # Longitude &lt;dbl&gt;, Latitude &lt;dbl&gt;, Sale_Price &lt;dbl&gt;, ## # Bedroom_AbvGr_o_Gr_Liv_Area &lt;dbl&gt;, Age_House &lt;dbl&gt;, TotalSF &lt;dbl&gt;, … casa_test_bake &lt;- bake(receta_casas, new_data = ames_test) glimpse(casa_test_bake) ## Rows: 733 ## Columns: 275 ## $ Lot_Frontage &lt;dbl&gt; 0.6607556, -1.72… ## $ Lot_Area &lt;dbl&gt; 0.15995167, -0.2… ## $ Year_Built &lt;dbl&gt; -0.34660802, 0.6… ## $ Mas_Vnr_Area &lt;dbl&gt; -0.5721904, -0.5… ## $ BsmtFin_SF_1 &lt;dbl&gt; 0.81885215, -1.3… ## $ BsmtFin_SF_2 &lt;dbl&gt; 0.5851851, -0.29… ## $ Bsmt_Unf_SF &lt;dbl&gt; -0.65580176, -0.… ## $ Full_Bath &lt;dbl&gt; -1.0284858, 0.79… ## $ Half_Bath &lt;dbl&gt; -0.7465678, -0.7… ## $ Bedroom_AbvGr &lt;dbl&gt; -1.0749072, 0.15… ## $ TotRms_AbvGrd &lt;dbl&gt; -0.9221672, -0.2… ## $ Fireplaces &lt;dbl&gt; -0.9297733, -0.9… ## $ Garage_Cars &lt;dbl&gt; -1.0124349, 0.29… ## $ Garage_Area &lt;dbl&gt; 1.19047388, -0.2… ## $ Wood_Deck_SF &lt;dbl&gt; 0.3353735, 3.013… ## $ Open_Porch_SF &lt;dbl&gt; -0.70298891, -0.… ## $ Enclosed_Porch &lt;dbl&gt; -0.3536614, -0.3… ## $ ThirdSsn_Porch &lt;dbl&gt; -0.1029207, -0.1… ## $ Screen_Porch &lt;dbl&gt; 1.9128593, -0.28… ## $ Misc_Val &lt;dbl&gt; -0.09569659, 0.6… ## $ Mo_Sold &lt;dbl&gt; -0.09320608, -1.… ## $ Year_Sold &lt;dbl&gt; 1.672416, 1.6724… ## $ Longitude &lt;dbl&gt; 0.90531385, 0.27… ## $ Latitude &lt;dbl&gt; 1.00647452, 1.24… ## $ Sale_Price &lt;int&gt; 105000, 185000, … ## $ Bedroom_AbvGr_o_Gr_Liv_Area &lt;dbl&gt; 0.34141287, 0.83… ## $ Age_House &lt;dbl&gt; 1.21786294, -0.9… ## $ TotalSF &lt;dbl&gt; -0.94110413, -0.… ## $ AvgRoomSF &lt;dbl&gt; -1.11699593, -0.… ## $ Pool &lt;dbl&gt; -0.0709206, -0.0… ## $ MS_SubClass_One_Story_1945_and_Older &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_One_Story_with_Finished_Attic_All_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_One_and_Half_Story_Unfinished_All_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_One_and_Half_Story_Finished_All_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Two_Story_1946_and_Newer &lt;dbl&gt; 0, 0, 1, 0, 0, 1… ## $ MS_SubClass_Two_Story_1945_and_Older &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Two_and_Half_Story_All_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Split_or_Multilevel &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Split_Foyer &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Duplex_All_Styles_and_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_One_Story_PUD_1946_and_Newer &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_One_and_Half_Story_PUD_All_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Two_Story_PUD_1946_and_Newer &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_PUD_Multilevel_Split_Level_Foyer &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Two_Family_conversion_All_Styles_and_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_Zoning_Residential_High_Density &lt;dbl&gt; 1, 0, 0, 0, 0, 0… ## $ MS_Zoning_Residential_Low_Density &lt;dbl&gt; 0, 1, 1, 1, 1, 0… ## $ MS_Zoning_Residential_Medium_Density &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_Zoning_A_agr &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_Zoning_C_all &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_Zoning_I_all &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Street_Pave &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Alley_No_Alley_Access &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Alley_Paved &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Alley_unknown &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Lot_Shape_Slightly_Irregular &lt;dbl&gt; 0, 1, 1, 0, 0, 0… ## $ Lot_Shape_Moderately_Irregular &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Lot_Shape_Irregular &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Land_Contour_HLS &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Land_Contour_Low &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Land_Contour_Lvl &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Utilities_NoSeWa &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Utilities_NoSewr &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Lot_Config_CulDSac &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Lot_Config_FR2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Lot_Config_FR3 &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Lot_Config_Inside &lt;dbl&gt; 1, 1, 1, 0, 1, 1… ## $ Land_Slope_Mod &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Land_Slope_Sev &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_College_Creek &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Old_Town &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Edwards &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Somerset &lt;dbl&gt; 0, 0, 0, 0, 0, 1… ## $ Neighborhood_Northridge_Heights &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Gilbert &lt;dbl&gt; 0, 1, 1, 1, 0, 0… ## $ Neighborhood_Sawyer &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Northwest_Ames &lt;dbl&gt; 0, 0, 0, 0, 1, 0… ## $ Neighborhood_Sawyer_West &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Mitchell &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Brookside &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Crawford &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Iowa_DOT_and_Rail_Road &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Timberland &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Northridge &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Stone_Brook &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_South_and_West_of_Iowa_State_University &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Clear_Creek &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Meadow_Village &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Briardale &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Bloomington_Heights &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Veenker &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Northpark_Villa &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Blueste &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Greens &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Green_Hills &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Landmark &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Hayden_Lake &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_1_Feedr &lt;dbl&gt; 1, 0, 0, 0, 0, 0… ## $ Condition_1_Norm &lt;dbl&gt; 0, 1, 1, 1, 1, 1… ## $ Condition_1_PosA &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_1_PosN &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_1_RRAe &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_1_RRAn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_1_RRNe &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_1_RRNn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_2_Feedr &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_2_Norm &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Condition_2_PosA &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_2_PosN &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_2_RRAe &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_2_RRAn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_2_RRNn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_TwoFmCon &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_Duplex &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_Twnhs &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_TwnhsE &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ House_Style_One_and_Half_Unf &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ House_Style_One_Story &lt;dbl&gt; 1, 1, 0, 1, 1, 0… ## $ House_Style_SFoyer &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ House_Style_SLvl &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ House_Style_Two_and_Half_Fin &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ House_Style_Two_and_Half_Unf &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ House_Style_Two_Story &lt;dbl&gt; 0, 0, 1, 0, 0, 1… ## $ Overall_Cond_Poor &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Overall_Cond_Fair &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Overall_Cond_Below_Average &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Overall_Cond_Average &lt;dbl&gt; 0, 0, 1, 1, 0, 1… ## $ Overall_Cond_Above_Average &lt;dbl&gt; 1, 0, 0, 0, 1, 0… ## $ Overall_Cond_Good &lt;dbl&gt; 0, 1, 0, 0, 0, 0… ## $ Overall_Cond_Very_Good &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Overall_Cond_Excellent &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Overall_Cond_Very_Excellent &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Style_Gable &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Roof_Style_Gambrel &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Style_Hip &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Style_Mansard &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Style_Shed &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Matl_CompShg &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Roof_Matl_Membran &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Matl_Metal &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Matl_Roll &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Matl_Tar.Grv &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Matl_WdShake &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Matl_WdShngl &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_AsphShn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_BrkComm &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_BrkFace &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_CBlock &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_CemntBd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_HdBoard &lt;dbl&gt; 0, 1, 0, 0, 0, 0… ## $ Exterior_1st_ImStucc &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_MetalSd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_Plywood &lt;dbl&gt; 0, 0, 0, 0, 1, 0… ## $ Exterior_1st_PreCast &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_Stone &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_Stucco &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_VinylSd &lt;dbl&gt; 1, 0, 1, 1, 0, 1… ## $ Exterior_1st_Wd.Sdng &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_WdShing &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_AsphShn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_Brk.Cmn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_BrkFace &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_CBlock &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_CmentBd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_HdBoard &lt;dbl&gt; 0, 1, 0, 0, 0, 0… ## $ Exterior_2nd_ImStucc &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_MetalSd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_Other &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_Plywood &lt;dbl&gt; 0, 0, 0, 0, 1, 0… ## $ Exterior_2nd_PreCast &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_Stone &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_Stucco &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_VinylSd &lt;dbl&gt; 1, 0, 1, 1, 0, 1… ## $ Exterior_2nd_Wd.Sdng &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_Wd.Shng &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Mas_Vnr_Type_BrkFace &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Mas_Vnr_Type_CBlock &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Mas_Vnr_Type_None &lt;dbl&gt; 1, 1, 1, 1, 0, 1… ## $ Mas_Vnr_Type_Stone &lt;dbl&gt; 0, 0, 0, 0, 1, 0… ## $ Exter_Cond_Fair &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exter_Cond_Poor &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Foundation_CBlock &lt;dbl&gt; 1, 0, 0, 1, 1, 0… ## $ Foundation_PConc &lt;dbl&gt; 0, 1, 1, 0, 0, 1… ## $ Foundation_Slab &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Foundation_Stone &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Foundation_Wood &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Fair &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Good &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_No_Basement &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Poor &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Typical &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Bsmt_Exposure_Gd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Exposure_Mn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Exposure_No &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Bsmt_Exposure_No_Basement &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_1_BLQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_1_GLQ &lt;dbl&gt; 0, 0, 0, 0, 0, 1… ## $ BsmtFin_Type_1_LwQ &lt;dbl&gt; 0, 0, 0, 1, 0, 0… ## $ BsmtFin_Type_1_No_Basement &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_1_Rec &lt;dbl&gt; 1, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_2_BLQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_2_GLQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_2_LwQ &lt;dbl&gt; 1, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_2_No_Basement &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_2_Rec &lt;dbl&gt; 0, 0, 0, 0, 1, 0… ## $ BsmtFin_Type_2_Unf &lt;dbl&gt; 0, 1, 1, 1, 0, 1… ## $ Heating_GasA &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Heating_GasW &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Heating_Grav &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Heating_OthW &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Heating_Wall &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Heating_QC_Fair &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Heating_QC_Good &lt;dbl&gt; 0, 0, 1, 0, 0, 0… ## $ Heating_QC_Poor &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Heating_QC_Typical &lt;dbl&gt; 1, 0, 0, 0, 1, 0… ## $ Central_Air_Y &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Electrical_FuseF &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Electrical_FuseP &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Electrical_Mix &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Electrical_SBrkr &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Electrical_Unknown &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Functional_Maj2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Functional_Min1 &lt;dbl&gt; 0, 0, 0, 0, 1, 0… ## $ Functional_Min2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Functional_Mod &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Functional_Sal &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Functional_Sev &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Functional_Typ &lt;dbl&gt; 1, 1, 1, 1, 0, 1… ## $ Garage_Type_Basment &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Type_BuiltIn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Type_CarPort &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Type_Detchd &lt;dbl&gt; 0, 0, 0, 1, 0, 0… ## $ Garage_Type_More_Than_Two_Types &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Type_No_Garage &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Finish_No_Garage &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Finish_RFn &lt;dbl&gt; 0, 0, 0, 0, 0, 1… ## $ Garage_Finish_Unf &lt;dbl&gt; 1, 0, 0, 1, 1, 0… ## $ Garage_Cond_Fair &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Cond_Good &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Cond_No_Garage &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Cond_Poor &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Cond_Typical &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Paved_Drive_Partial_Pavement &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Paved_Drive_Paved &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Pool_QC_Fair &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Pool_QC_Good &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Pool_QC_No_Pool &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Pool_QC_Typical &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Fence_Good_Wood &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Fence_Minimum_Privacy &lt;dbl&gt; 1, 0, 0, 0, 1, 0… ## $ Fence_Minimum_Wood_Wire &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Fence_No_Fence &lt;dbl&gt; 0, 0, 1, 1, 0, 1… ## $ Misc_Feature_Gar2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Misc_Feature_None &lt;dbl&gt; 1, 0, 1, 1, 1, 1… ## $ Misc_Feature_Othr &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Misc_Feature_Shed &lt;dbl&gt; 0, 1, 0, 0, 0, 0… ## $ Misc_Feature_TenC &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_Con &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_ConLD &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_ConLI &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_ConLw &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_CWD &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_New &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_WD. &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Sale_Condition_AdjLand &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Condition_Alloca &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Condition_Family &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Condition_Normal &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Sale_Condition_Partial &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Second_Flr_SF_x_First_Flr_SF &lt;dbl&gt; 0.52453728, -0.0… ## $ Bsmt_Cond_Fair_x_TotRms_AbvGrd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Good_x_TotRms_AbvGrd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_No_Basement_x_TotRms_AbvGrd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Poor_x_TotRms_AbvGrd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Typical_x_TotRms_AbvGrd &lt;dbl&gt; -0.9221672, -0.2… 5.1.2 Clasificación: Preparación de Datos Ahora prepararemos los datos para un ejemplo de churn, es decir, la tasa de cancelación de clientes. Usaremos datos de Telco. telco &lt;- read_csv(&quot;data/Churn.csv&quot;) glimpse(telco) ## Rows: 7,043 ## Columns: 21 ## $ customerID &lt;chr&gt; &quot;7590-VHVEG&quot;, &quot;5575-GNVDE&quot;, &quot;3668-QPYBK&quot;, &quot;7795-CFOCW… ## $ gender &lt;chr&gt; &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;,… ## $ SeniorCitizen &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ Partner &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ Dependents &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;… ## $ tenure &lt;dbl&gt; 1, 34, 2, 45, 2, 8, 22, 10, 28, 62, 13, 16, 58, 49, 2… ## $ PhoneService &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, … ## $ MultipleLines &lt;chr&gt; &quot;No phone service&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No phone service&quot;, &quot;… ## $ InternetService &lt;chr&gt; &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;Fiber optic&quot;, &quot;Fiber opt… ## $ OnlineSecurity &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;… ## $ OnlineBackup &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;N… ## $ DeviceProtection &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Y… ## $ TechSupport &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ StreamingTV &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Ye… ## $ StreamingMovies &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ Contract &lt;chr&gt; &quot;Month-to-month&quot;, &quot;One year&quot;, &quot;Month-to-month&quot;, &quot;One … ## $ PaperlessBilling &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, … ## $ PaymentMethod &lt;chr&gt; &quot;Electronic check&quot;, &quot;Mailed check&quot;, &quot;Mailed check&quot;, &quot;… ## $ MonthlyCharges &lt;dbl&gt; 29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89.10, 29.7… ## $ TotalCharges &lt;dbl&gt; 29.85, 1889.50, 108.15, 1840.75, 151.65, 820.50, 1949… ## $ Churn &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Y… Como en el ejemplo de regresión, primero crearemos los conjuntos de entrenamiento y de prueba. set.seed(1234) telco_split &lt;- initial_split(telco, prop = .7) telco_train &lt;- training(telco_split) telco_test &lt;- testing(telco_split) En el siguiente chunk definiremos la receta con funciones usadas en el ejemplo anterior más la función step_num2factor() que nos ayuda a categorizar una variable continua. binner &lt;- function(x) { x &lt;- cut(x, breaks = c(0, 12, 24, 36,48,60,72), include.lowest = TRUE) as.numeric(x) } telco_rec &lt;- recipe(Churn ~ ., data = telco_train) %&gt;% update_role(customerID, new_role = &quot;id variable&quot;) %&gt;% step_num2factor(tenure, transform = binner, levels = c(&quot;0-1 year&quot;, &quot;1-2 years&quot;, &quot;2-3 years&quot;, &quot;3-4 years&quot;, &quot;4-5 years&quot;, &quot;5-6 years&quot;)) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_impute_median(all_numeric_predictors()) %&gt;% step_rm(customerID, skip=T) %&gt;% prep() Ahora recuperamos la matriz de diseño con las funciones prep() y juice(). telco_juiced &lt;- juice(telco_rec) telco_juiced ## # A tibble: 4,930 × 35 ## SeniorCitizen MonthlyCharges TotalCharges Churn gender_Male Partner_Yes ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.442 -1.50 -0.663 No 0 0 ## 2 -0.442 0.664 0.472 No 1 1 ## 3 -0.442 -0.508 -0.546 No 0 0 ## 4 2.26 0.649 -0.850 Yes 0 0 ## 5 -0.442 0.167 -0.979 Yes 0 0 ## 6 -0.442 0.842 0.975 No 1 1 ## 7 -0.442 0.260 -0.648 No 1 0 ## 8 -0.442 -0.668 0.178 No 1 1 ## 9 -0.442 -1.20 -0.706 No 1 1 ## 10 -0.442 -0.407 -0.348 No 0 0 ## # … with 4,920 more rows, and 29 more variables: Dependents_Yes &lt;dbl&gt;, ## # tenure_X1.2.years &lt;dbl&gt;, tenure_X2.3.years &lt;dbl&gt;, tenure_X3.4.years &lt;dbl&gt;, ## # tenure_X4.5.years &lt;dbl&gt;, tenure_X5.6.years &lt;dbl&gt;, PhoneService_Yes &lt;dbl&gt;, ## # MultipleLines_No.phone.service &lt;dbl&gt;, MultipleLines_Yes &lt;dbl&gt;, ## # InternetService_Fiber.optic &lt;dbl&gt;, InternetService_No &lt;dbl&gt;, ## # OnlineSecurity_No.internet.service &lt;dbl&gt;, OnlineSecurity_Yes &lt;dbl&gt;, ## # OnlineBackup_No.internet.service &lt;dbl&gt;, OnlineBackup_Yes &lt;dbl&gt;, … telco_test_bake &lt;- bake(telco_rec, new_data = telco_test) glimpse(telco_test_bake) ## Rows: 2,113 ## Columns: 36 ## $ customerID &lt;chr&gt; &quot;5575-GNVDE&quot;, &quot;9305-CDSKC&quot;, &quot;671… ## $ SeniorCitizen &lt;dbl&gt; -0.4417148, -0.4417148, -0.44171… ## $ MonthlyCharges &lt;dbl&gt; -0.27067882, 1.14914329, -1.1751… ## $ TotalCharges &lt;dbl&gt; -0.1752116, -0.6472105, -0.87618… ## $ Churn &lt;fct&gt; No, Yes, No, No, No, No, No, No,… ## $ gender_Male &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,… ## $ Partner_Yes &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,… ## $ Dependents_Yes &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,… ## $ tenure_X1.2.years &lt;dbl&gt; 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,… ## $ tenure_X2.3.years &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ tenure_X3.4.years &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ tenure_X4.5.years &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,… ## $ tenure_X5.6.years &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,… ## $ PhoneService_Yes &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,… ## $ MultipleLines_No.phone.service &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,… ## $ MultipleLines_Yes &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,… ## $ InternetService_Fiber.optic &lt;dbl&gt; 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,… ## $ InternetService_No &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ OnlineSecurity_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ OnlineSecurity_Yes &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,… ## $ OnlineBackup_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ OnlineBackup_Yes &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,… ## $ DeviceProtection_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ DeviceProtection_Yes &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,… ## $ TechSupport_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ TechSupport_Yes &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,… ## $ StreamingTV_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ StreamingTV_Yes &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,… ## $ StreamingMovies_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ StreamingMovies_Yes &lt;dbl&gt; 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,… ## $ Contract_One.year &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,… ## $ Contract_Two.year &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,… ## $ PaperlessBilling_Yes &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,… ## $ PaymentMethod_Credit.card..automatic. &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,… ## $ PaymentMethod_Electronic.check &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,… ## $ PaymentMethod_Mailed.check &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,… Estos fueron dos ejemplos aplicados de la paquetería recipies, existen distintas funciones step que pueden implementarse en recetas para usarse con tidymodels, en las secciones siguientes les daremos su uso para ajustar un modelo completo. 5.2 Regresión Lineal En esta sección aprenderemos sobre regresión lineal simple y múltiple, como se ajusta un modelo de regresión en R, las métricas de desempeño para problemas de regresión y como podemos comparar modelos con estas métricas. Existen dos tipos de modelos de regresión lineal: Regresión lineal simple: En la regresión lineal simple se utiliza una variable independiente o explicativa “X” (numérica o categórica) para estimar una variable dependiente o de respuesta numérica “Y” mediante el ajuste de una recta permita conocer la relación existente entre ambas variables. Dicha relación entre variables se expresa como: \\[Y = \\beta_0 + \\beta_1X_1 + \\epsilon \\approx b + mx\\] Donde: \\(\\epsilon \\sim Norm(0,\\sigma^2)\\) (error aleatorio) \\(\\beta_0\\) = Coeficiente de regresión 0 (Ordenada al origen o intercepto) \\(\\beta_1\\) = Coeficiente de regresión 1 (Pendiente o regresor de variable \\(X_1\\)) \\(X_1\\) = Variable explicativa observada \\(Y\\) = Respuesta numérica Debido a que los valores reales de \\(\\beta_0\\) y \\(\\beta_1\\) son desconocidos, procedemos a estimarlos estadísticamente: \\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X_1\\] Con \\(\\hat{\\beta}_0\\) el estimado de la ordenada al origen y \\(\\hat{\\beta}_1\\) el estimado de la pendiente. Regresión lineal múltiple: Cuando se utiliza más de una variable independiente, el proceso se denomina regresión lineal múltiple. En este escenario no es una recta sino un hiper-plano lo que se ajusta a partir de las covariables explicativas \\(\\{X_1, X_2, X_3, ...,X_n\\}\\) El objetivo de un modelo de regresión múltiple es tratar de explicar la relación que existe entre una variable dependiente (variable respuesta) \\(&quot;Y&quot;\\) un conjunto de variables independientes (variables explicativas) \\(\\{X1,..., Xm\\}\\), el modelo es de la forma: \\[Y = \\beta_0 + \\beta_1X_1 + \\cdot \\cdot \\cdot + \\beta_mX_m + \\epsilon\\] Donde: \\(Y\\) como variable respuesta. \\(X_1,X_2,...,X_m\\) como las variables explicativas, independientes o regresoras. \\(\\beta_1, \\beta_2,...,\\beta_m\\) Se conocen como coeficientes parciales de regresión. Cada una de ellas puede interpretarse como el efecto promedio que tiene el incremento de una unidad de la variable predictora \\(X_i\\) sobre la variable dependiente \\(Y\\), manteniéndose constantes el resto de variables. 5.2.1 Ajuste de modelo 5.2.1.1 Estimación de parámetros: Regresión lineal simple En la gran mayoría de casos, los valores \\(\\beta_0\\) y \\(\\beta_1\\) poblacionales son desconocidos, por lo que, a partir de una muestra, se obtienen sus estimaciones \\(\\hat{\\beta_0}\\) y \\(\\hat{\\beta_1}\\). Estas estimaciones se conocen como coeficientes de regresión o least square coefficient estimates, ya que toman aquellos valores que minimizan la suma de cuadrados residuales, dando lugar a la recta que pasa más cerca de todos los puntos. En términos analíticos, la expresión matemática a optimizar y solución están dadas por: \\[min(\\epsilon) \\Rightarrow min(y-\\hat{y}) = min\\{y -(\\hat{\\beta}_0 + \\hat{\\beta}_1x)\\}\\] \\[\\begin{aligned} \\hat{\\beta}_0 &amp;= \\overline{y} - \\hat{\\beta}_1\\overline{x} \\\\ \\hat{\\beta}_1 &amp;= \\frac{\\sum^n_{i=1}(x_i - \\overline{x})(y_i - \\overline{y})}{\\sum^n_{i=1}(x_i - \\overline{x})^2} =\\frac{S_{xy}}{S^2_x} \\end{aligned}\\] Donde: \\(S_{xy}\\) es la covarianza entre \\(x\\) y \\(y\\). \\(S_{x}^{2}\\) es la varianza de \\(x\\). \\(\\hat{\\beta}_0\\) es el valor esperado la variable \\(Y\\) cuando \\(X = 0\\), es decir, la intersección de la recta con el eje y. 5.2.1.2 Estimación de parámetros: Regresión lineal múltiple En el caso de múltiples parámetros, la notación se vuelve más sencilla al expresar el modelo mediante una combinación lineal dada por la multiplicación de matrices (álgebra lineal). \\[Y = X\\beta + \\epsilon\\] Donde: \\[Y = \\begin{pmatrix}y_1\\\\y_2\\\\.\\\\.\\\\.\\\\y_n\\end{pmatrix} \\quad \\beta = \\begin{pmatrix}\\beta_0\\\\\\beta_1\\\\.\\\\.\\\\.\\\\\\beta_m\\end{pmatrix} \\quad \\epsilon = \\begin{pmatrix}\\epsilon_1\\\\\\epsilon_2\\\\.\\\\.\\\\.\\\\\\epsilon_n\\end{pmatrix} \\quad \\quad X = \\begin{pmatrix}1 &amp; x_{11} &amp; x_{12} &amp; ... &amp; x_{1m}\\\\1 &amp; x_{21} &amp; x_{22} &amp; ... &amp; x_{2m}\\\\\\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; ... &amp; x_{nm}\\end{pmatrix}\\\\\\] El estimador por mínimos cuadrados está dado por: \\[\\hat{\\beta} = (X^TX)^{-1}X^TY\\] IMPORTANTE: Es necesario entender que para cada uno de los coeficientes de regresión se realiza una prueba de hipótesis. Una vez calculado el valor estimado, se procede a determinar si este valor es significativamente distinto de cero, por lo que la hipótesis de cada coeficiente se plantea de la siguiente manera: \\[H_0:\\beta_i=0 \\quad Vs \\quad H_1:\\beta_i\\neq0\\] El software R nos devuelve el p-value asociado a cada coeficiente de regresión. Recordemos que valores pequeños de p sugieren que al rechazar \\(H_0\\), la probabilidad de equivocarnos es baja, por lo que procedemos a rechazar la hipótesis nula. 5.2.2 Residuos del modelo El residuo de una estimación se define como la diferencia entre el valor observado y el valor esperado acorde al modelo. \\[\\epsilon_i= y_i -\\hat{y}_i\\] A la hora de contemplar el conjunto de residuos hay dos posibilidades: La suma del valor absoluto de cada residuo. \\[RAS=\\sum_{i=1}^{n}{|e_i|}=\\sum_{i=1}^{n}{|y_i-\\hat{y}_i|}\\] La suma del cuadrado de cada residuo (RSS). Esta es la aproximación más empleada (mínimos cuadrados) ya que magnifica las desviaciones más extremas. \\[RSS=\\sum_{i=1}^{n}{e_i^2}=\\sum_{i=1}^{n}{(y_i-\\hat{y}_i)^2}\\] Los residuos son muy importantes puesto que en ellos se basan las diferentes métricas de desempeño del modelo. 5.2.2.1 Condiciones para el ajuste de una regresión lineal: Existen ciertas condiciones o supuestos que deben ser validados para el correcto ajuste de un modelo de regresión lineal, los cuales se enlistan a continuación: Linealidad: La relación entre ambas variables debe ser lineal. Distribución normal de los residuos: Los residuos se tiene que distribuir de forma normal, con media igual a 0. Varianza de residuos constante (homocedasticidad): La varianza de los residuos tiene que ser aproximadamente constante. Independencia: Las observaciones deben ser independientes unas de otras. Dado que las condiciones se verifican a partir de los residuos, primero se suele generar el modelo y después se valida. 5.2.3 Métricas de desempeño Dado que nuestra variable a predecir es numérica, podemos medir qué tan cerca o lejos estuvimos del número esperado dada una predicción. Las métricas de desempeño asociadas a los problemas de regresión ocupan esa distancia cómo cuantificación del desempeño o de los errores cometidos por el modelo. Las métricas más utilizadas son: MEA: Mean Absolute Error MAPE: Mean Absolute Percentual Error \\(\\quad \\Rightarrow \\quad\\) más usada para reportar resultados RMSE: Root Mean Squared Error \\(\\quad \\quad \\quad \\Rightarrow \\quad\\) más usada para entrenar modelos \\(R^2\\) : R cuadrada \\(R^2\\) : \\(R^2\\) ajustada RMSLE: Root Mean Squared Logarithmic Loss Error MAE: Mean Absolute Error \\[MAE = \\frac{1}{N}\\sum_{i=1}^{N}{|y_{i}-\\hat{y}_{i}|}\\] Donde: \\(N:\\) Número de observaciones predichas. \\(y_{i}:\\) Valor real. \\(\\hat{y}_{i}:\\) Valor de la predicción. Esta métrica suma los errores absolutos de cada predicción y los divide entre el número de observaciones, para obtener el promedio absoluto del error del modelo. Ventajas Vs Desventajas: Todos los errores pesan lo mismo sin importar qué tan pequeños o qué tan grandes sean, es muy sensible a valores atípicos, y dado que obtiene el promedio puede ser que un solo error en la predicción que sea muy grande afecte al valor de todo el modelo, aún y cuando el modelo no tuvo errores tan malos para el resto de las observaciones. Se recomienda utilizar esta métrica cuando los errores importan lo mismo, es decir, importa lo mismo si se equivocó muy poco o se equivocó mucho. MAPE: Mean Absolute Percentage Error \\[MAPE = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{{|y_{i}-\\hat{y}_{i}|}}{|y_{i}|}\\] Donde: \\(N:\\) Número de observaciones predichas. \\(y_{i}:\\) Valor real. \\(\\hat{y}_{i}:\\) Valor de la predicción. Esta métrica es la métrica MAE expresada en porcentaje, por lo que mide el error del modelo en términos de porcentaje, al igual que con MAE, no hay errores negativos por el valor absoluto, y mientras más pequeño el error es mejor. Ventajas Vs Desventajas: Cuando existe un valor real de 0 esta métrica no se puede calcular, por otro lado, una de las ventajas sobre MAE es que no es sensible a valores atípicos. Se recomienda utilizar esta métrica cuando en tu problema no haya valores a predecir que puedan ser 0, por ejemplo, en ventas puedes llegar a tener 0 ventas, en este caso no podemos ocupar esta métrica. En general a las personas de negocio les gusta esta métrica pues es fácil de comprender. RMSE: Root Mean Squared Error \\[RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}{(y_{i}-\\hat{y}_{i})^2}}\\] Donde: \\(N:\\) Número de observaciones predichas. \\(y_{i}:\\) Valor real. \\(\\hat{y}_{i}:\\) Valor de la predicción. Esta métrica es muy parecida a MAE, solo que en lugar de sacar el valor absoluto de la diferencia entre el valor real y el valor predicho, para evitar valores negativos eleva esta diferencia al cuadrado, y saca el promedio de esa diferencia, al final, para dejar el valor en la escala inicial saca la raíz cuadrada. Esta es la métrica más utilizada en problemas de regresión, debido a que es más fácil de optimizar que el MAE. Ventajas Vs Desventaja: Todos los errores pesan lo mismo sin importar qué tan pequeños o qué tan grandes sean, es más sensible a valores atípicos que MAE pues eleva al cuadrado diferencias, y dado que obtiene el promedio puede ser que un solo error en la predicción que sea muy grande afecte al valor de todo el modelo, aún y cuando el modelo no tuvo errores tan malos para el resto de las observaciones. Se recomienda utilizar esta métrica cuando en el problema que queremos resolver es muy costoso tener equivocaciones grandes, podemos tener varios errores pequeños, pero no grandes. \\(R^2\\): R cuadrada \\[R^{2} = \\frac{\\sum_{i=1}^{N}{(\\hat{y}_{i}-\\bar{y}_{i})^2}}{\\sum_{i=1}^{N}{(y_{i}-\\bar{y}_{i})^2}}\\] Donde: \\(N:\\) Número de observaciones predichas. \\(y_{i}:\\) Valor real. \\(\\hat{y}_{i}:\\) Valor de la predicción. \\(\\bar{y}_{i}:\\) Valor promedio de la variable y. El coeficiente de determinación es la proporción de la varianza total de la variable explicada por la regresión. El coeficiente de determinación, también llamado R cuadrado, refleja la bondad del ajuste de un modelo a la variable que pretender explicar. Es importante saber que el resultado del coeficiente de determinación oscila entre 0 y 1. Cuanto más cerca de 1 se sitúe su valor, mayor será el ajuste del modelo a la variable que estamos intentando explicar. De forma inversa, cuanto más cerca de cero, menos ajustado estará el modelo y, por tanto, menos fiable será. Ventajas Vs Desventaja: El problema del coeficiente de determinación, y razón por el cual surge el coeficiente de determinación ajustado, radica en que no penaliza la inclusión de variables explicativas no significativas, es decir, el valor de \\(R^2\\) siempre será más grande cuantas más variables sean incluidas en el modelo, aún cuando estas no sean significativas en la predicción. \\(\\bar{R}^2\\): \\(R^2\\) ajustada \\[\\bar{R}^2=1-\\frac{N-1}{N-k-1}[1-R^2]\\] Donde: \\(\\bar{R}²:\\) Es el valor de R² ajustado \\(R²:\\) Es el valor de R² original \\(N:\\) Es el total de observaciones en el ajuste \\(k:\\) Es el número de variables usadas en el modelo El coeficiente de determinación ajustado (R cuadrado ajustado) es la medida que define el porcentaje explicado por la varianza de la regresión en relación con la varianza de la variable explicada. Es decir, lo mismo que el R cuadrado, pero con una diferencia: El coeficiente de determinación ajustado penaliza la inclusión de variables. En la fórmula, N es el tamaño de la muestra y k el número de variables explicativas. RMSLE: Root Mean Squared Logarithmic Loss Error \\[e = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (log(1+p_i)-log(1+o_i))^2}\\] Donde: \\(p_i\\) son las predicciones y \\(o_i\\) son las observaciones, esto implica que nos interesa más evaluar error relativo que absoluto, pues: \\[log(1+p)-log(1+o)=log(\\frac{1+p}{1+o})\\approx log(\\frac{p}{o})=log(1+\\frac{p-o}{o})\\approx\\frac{p-o}{p}\\] Ventajas Vs Desventaja: Suele ser más complicada el entendimiento de esta métrica, a diferencia de las métricas anteriores, sin embargo, esta métrica realiza la penalización de los errores grandes al mismo tiempo que apromixa a al error relativo (\\(\\approx\\) raiz de error relativo cuadrático medio). 5.2.4 Implementación en R Ajustaremos un modelo de regresión usando la receta antes vista. library(tidymodels) modelo1 &lt;- linear_reg() %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;lm&quot;) lm_fit1 &lt;- fit(modelo1, Sale_Price ~ ., casa_juiced) p_test &lt;- predict(lm_fit1, casa_test_bake) %&gt;% mutate(.pred = exp(.pred)) %&gt;% bind_cols(ames_test) %&gt;% select(.pred, Sale_Price) %&gt;% mutate(error = Sale_Price - .pred) p_test ## # A tibble: 733 × 3 ## .pred Sale_Price error ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 116703. 105000 -11703. ## 2 177029. 185000 7971. ## 3 166772. 180400 13628. ## 4 109872. 141000 31128. ## 5 219261. 210000 -9261. ## 6 212340. 216000 3660. ## 7 164850. 149900 -14950. ## 8 99545. 105500 5955. ## 9 102023. 88000 -14023. ## 10 137758. 146000 8242. ## # … with 723 more rows 5.2.5 Coeficientes del modelo Podemos recuperar los coeficientes de nuestro modelo con la función tidy() y observar cuales variables explicativas son las más significativas de acuerdo con el p-value. lm_fit1 %&gt;% tidy() %&gt;% arrange(desc(p.value)) ## # A tibble: 275 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Roof_Style_Gambrel -3.73e-4 0.0778 -0.00479 0.996 ## 2 Exterior_1st_Stone -7.39e-4 0.141 -0.00524 0.996 ## 3 House_Style_Two_Story 3.78e-4 0.0281 0.0135 0.989 ## 4 Neighborhood_South_and_West_of_Iowa_Sta… -5.82e-4 0.0369 -0.0158 0.987 ## 5 Exterior_2nd_HdBoard -1.17e-3 0.0520 -0.0224 0.982 ## 6 Bsmt_Cond_Typical -2.76e-3 0.0799 -0.0346 0.972 ## 7 Electrical_Unknown 4.00e-3 0.111 0.0362 0.971 ## 8 Latitude -5.38e-4 0.0142 -0.0380 0.970 ## 9 Neighborhood_Mitchell 1.66e-3 0.0421 0.0395 0.968 ## 10 Neighborhood_Sawyer 1.64e-3 0.0331 0.0494 0.961 ## # … with 265 more rows 5.2.6 Métricas de desempeño Ahora para calcular las métricas de desempeño usaremos la paqueteria MLmetrics y las funciones de dplyr para resumir y estructurar los resultados. library(MLmetrics) p_test %&gt;% summarise( MAE = MLmetrics::MAE(.pred, Sale_Price), MAPE = MLmetrics::MAPE(.pred, Sale_Price), RMSE = MLmetrics::RMSE(.pred, Sale_Price), R2 = MLmetrics::R2_Score(.pred, Sale_Price), RMSLE = MLmetrics::RMSLE(.pred, Sale_Price) ) ## # A tibble: 1 × 5 ## MAE MAPE RMSE R2 RMSLE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 16515. 0.0951 26794. 0.889 0.207 5.2.6.1 Gráfica de ajuste library(patchwork) pred_obs_plot &lt;- p_test %&gt;% ggplot(aes(x = .pred, y = Sale_Price)) + geom_point(alpha = 0.2) + geom_abline(color = &quot;red&quot;) + xlab(&quot;Predicciones&quot;) + ylab(&quot;Observaciones&quot;) + ggtitle(&quot;Predicción vs Observación&quot;) error_line &lt;- p_test %&gt;% ggplot(aes(x = Sale_Price, y = error)) + geom_line() + geom_hline(yintercept = 0, color = &quot;red&quot;) + xlab(&quot;Observaciones&quot;) + ylab(&quot;Errores&quot;) + ggtitle(&quot;Varianza de errores&quot;) pred_obs_plot + error_line error_dist &lt;- p_test %&gt;% ggplot(aes(x = error)) + geom_histogram(color = &quot;white&quot;, fill = &quot;black&quot;) + geom_vline(xintercept = 0, color = &quot;red&quot;) + ylab(&quot;Conteos de clase&quot;) + xlab(&quot;Errores&quot;) + ggtitle(&quot;Distribución de error&quot;) error_qqplot &lt;- p_test %&gt;% ggplot(aes(sample = error)) + geom_qq(alpha = 0.3) + stat_qq_line(color = &quot;red&quot;) + xlab(&quot;Distribución normal&quot;) + ylab(&quot;Distribución de errores&quot;) + ggtitle(&quot;QQ-Plot&quot;) error_dist + error_qqplot 5.2.7 Métodos se selección de variables 5.2.7.1 Forward selection (selección hacia adelante) Comienza sin predictores en el modelo, agrega iterativamente los predictores más contribuyentes y se detiene cuando la mejora del modelo ya no es estadísticamente significativa. 5.2.7.2 Backward selection (selección hacia atrás) Comienza con todos los predictores en el modelo (modelo completo), y elimina iterativamente los predictores menos contribuyentes y se detiene cuando tiene un modelo en el que todos los predictores son estadísticamente significativos. 5.2.7.3 Stepwise selection (selección paso a paso) Combinación de selecciones hacia adelante y hacia atrás. Comienza sin predictores, luego agrega secuencialmente los predictores más contribuyentes (como la selección hacia adelante). Después de agregar cada nueva variable, elimina cualquier variable que ya no proporcione una mejora en el ajuste del modelo (como la selección hacia atrás). 5.2.7.4 Desventajas de la selección forward, backward y stepwise Inflación de resultados falsos positivos: la selección hacia adelante, hacia atrás y paso a paso utiliza muchas pruebas de hipótesis repetidas para tomar decisiones sobre la inclusión o exclusión de predictores individuales. Los valores \\(p\\) correspondientes no están ajustados, lo que lleva a una selección excesiva de características (es decir, resultados falsos positivos). Además, este problema se agrava cuando están presentes predictores altamente correlacionados. Sobreajuste del modelo: Las estadísticas del modelo resultante, incluidas las estimaciones de los parámetros y la incertidumbre asociada, son muy optimistas ya que no tienen en cuenta el proceso de selección. 5.3 Regresión Logística El nombre de este modelo es: Regresión Bernoulli con liga logit, pero todo mundo la conoce solo por regresión logística. Es importante saber que la liga puede ser elegida dentro de un conjunto de ligas comunes, por lo que puede dejar de ser logit y seguir siendo regresión Bernoulli, pero ya no podría ser llamada “logística.” Al igual que en regresión lineal, existe la regresión simple y regresión múltiple. La regresión logística simple se utiliza una variable independiente, mientras que cuando se utiliza más de una variable independiente, el proceso se denomina regresión logística múltiple. Objetivo: Estimar la probabilidad de pertenecer a la categoría positiva de una variable de respuesta categórica. Posteriormente, se determina el umbral de probabilidad a partir del cual se clasifica a una observación como positiva o negativa. 5.3.1 Función sigmoide Si una variable cualitativa con dos categorías se codifica como 1 y 0, matemáticamente es posible ajustar un modelo de regresión lineal por mínimos cuadrados. El problema de esta aproximación es que, al tratarse de una recta, para valores extremos del predictor, se obtienen valores de \\(Y\\) menores que 0 o mayores que 1, lo que entra en contradicción con el hecho de que las probabilidades siempre están dentro del rango [0,1]. Para evitar estos problemas, la regresión logística transforma el valor devuelto por la regresión lineal empleando una función cuyo resultado está siempre comprendido entre 0 y 1. Existen varias funciones que cumplen esta descripción, una de las más utilizadas es la función logística (también conocida como función sigmoide): \\[\\sigma(Z)=\\frac{e^{Z}}{1+e^{Z}}\\] Función sigmoide: Para valores de \\(Z\\) muy grandes, el valor de \\(e^{Z}\\) tiende a infinito por lo que el valor de la función sigmoide es 1. Para valores de \\(Z\\) muy negativos, el valor \\(e^{Z}\\) tiende a cero, por lo que el valor de la función sigmoide es 0. Sustituyendo la \\(Z\\) de la función sigmoide por la función lineal \\(\\beta_0+\\beta_1X\\) se obtiene que: \\[\\pi=P(Y=k|X=x)=\\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\\] donde \\(P(Y=k|X=x)\\) puede interpretarse como: la probabilidad de que la variable cualitativa \\(Y\\) adquiera el valor \\(k\\), dado que el predictor \\(X\\) tiene el valor \\(x\\). 5.3.2 Ajuste del modelo Esta función, puede ajustarse de forma sencilla con métodos de regresión lineal si se emplea su versión logarítmica: \\[logit(\\pi)= ln(\\frac{\\pi}{1-\\pi}) = ln(\\frac{p(Y=k|X=x)}{1−p(Y=k|X=x)})=\\beta_0+\\beta_1X\\] \\[P(Y=k|X=x)=\\frac{e^{\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_ix_i}}{1+e^{\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_ix_i}}\\] La combinación óptima de coeficientes \\(\\beta_0\\) y \\(\\beta_1\\) será aquella que tenga la máxima verosimilitud (maximum likelihood), es decir el valor de los parámetros \\(\\beta_0\\) y \\(\\beta_1\\) con los que se maximiza la probabilidad de obtener los datos observados. El método de maximum likelihood está ampliamente extendido en la estadística aunque su implementación no siempre es trivial. Otra forma para ajustar un modelo de regresión logística es empleando descenso de gradiente. Si bien este no es el método de optimización más adecuado para resolver la regresión logística, está muy extendido en el ámbito de machine learning para ajustar otros modelos. 5.3.3 Clasificación Una de las principales aplicaciones de un modelo de regresión logística es clasificar la variable cualitativa en función de valor que tome el predictor. Para conseguir esta clasificación, es necesario establecer un threshold de probabilidad a partir de la cual se considera que la variable pertenece a uno de los niveles. Por ejemplo, se puede asignar una observación al grupo 1 si \\(p̂ (Y=1|X)&gt;0.3\\) y al grupo 0 si ocurre lo contrario. Es importante mencionar que el punto de corte no necesariamente tiene que ser 0.5, este puede ser seleccionado a convenecía de la métrica a optimizar. 5.3.4 Métricas de desempeño Existen distintas métricas de desempeño para problemas de clasificación, debido a que contamos con la respuesta correcta podemos contar cuántos aciertos tuvimos y cuántos fallos tuvimos. Primero, por simplicidad ocuparemos un ejemplo de clasificación binaria, Cancelación (1) o No Cancelación (0). En este tipo de algoritmos definimos cuál de las categorías será nuestra etiqueta positiva y cuál será la negativa. La positiva será la categoría que queremos predecir -en nuestro ejemplo, Cancelación- y la negativa lo opuesto -en el caso binario- en nuestro ejemplo, no cancelación. Dadas estas definiciones tenemos 4 posibilidades: True positives: Nuestra predicción dijo que la transacción es fraude y la etiqueta real dice que es fradue. False positives: Nuestra predicción dijo que la transacción es fraude y la etiqueta real dice que no es fraude. True negatives: Nuestra predicción dijo que la transacción es no fraude y la etiqueta real dice que no es fraude. False negatives: Nuestra predicción dijo que la transacción es no fraude y la etiqueta real dice que es fraude. Matriz de confusión Esta métrica corresponde a una matriz en donde se plasma el conteo de los aciertos y los errores que haya hecho el modelo, esto es: los verdaderos positivos (TP), los verdaderos negativos (TN), los falsos positivos (FP) y los falsos negativos (FN). Normalmente los renglones representan las etiquetas predichas, ya sean positivas o negativas, y las columnas a las etiquetas reales, aunque esto puede cambiar en cualquier software. Accuracy Número de aciertos totales entre todas las predicciones. \\[accuracy = \\frac{TP + TN}{ TP+FP+TN+FN}\\] La métrica más utilizada, en datasets no balanceados esta métrica no nos sirve, al contrario, nos engaña. Adicionalmente, cuando la identificación de una categoría es más importante que otra es mejor recurrir a otras métricas. Precision: Eficiencia De los que identificamos como clase positiva, cuántos identificamos correctamente. ¿Qué tan eficientes somos en la predicción? \\[precision = \\frac{TP}{TP + FP}\\] ¿Cuándo utilizar precision? Esta es la métrica que ocuparás más, pues en un contexto de negocio, donde los recursos son finitos y tiene un costo asociado, ya sea monetario o de tiempo o de recursos, necesitarás que las predicciones de tu etiqueta positiva sean muy eficientes. Al utilizar esta métrica estaremos optimizando el modelo para minimizar el número de falsos positivos. Recall o Sensibilidad: Cobertura Del universo posible de nuestra clase positiva, cuántos identificamos correctamente. \\[recall = \\frac{TP}{TP + FN }\\] Esta métrica la ocuparás cuando en el contexto de negocio de tu problema sea más conveniente aumentar la cantidad de positivos o disminuir los falsos negativos. Esto se realiza debido al impacto que estos pueden tener en las personas en quienes se implementará la predicción. Especificidad Es el número de observaciones correctamente identificados como negativos fuera del total de negativos. \\[Specificity = \\frac{TN}{TN+FP}\\] F1-score Combina precision y recall para optimizar ambos. \\[F = 2 *\\frac{precision * recall}{precision + recall} \\] Se recomienda utilizar esta métrica de desempeño cuando quieres balancear tanto los falsos positivos como los falsos negativos. Aunque es una buena solución para tomar en cuenta ambos errores, pocas veces hay problemas reales que permiten ocuparla, esto es porque en más del 90% de los casos tenemos una restricción en recursos. Ahora con esto en mente podemos definir las siguientes métricas: AUC y ROC: Area Under the Curve y Receiver operator characteristic Una curva ROC es un gráfico que muestra el desempeño de un modelo de clasificación en todos los puntos de corte. AUC significa “Área bajo la curva ROC.” Es decir, AUC mide el área debajo de la curva ROC. 5.3.5 Implementación en R Ajustaremos un modelo de regresión logística usando la receta antes vista. logistic_model &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) logistic_fit1 &lt;- parsnip::fit(logistic_model, Churn ~ ., telco_juiced) logistic_p_test &lt;- predict(logistic_fit1, telco_test_bake) %&gt;% bind_cols(telco_test_bake) %&gt;% select(.pred_class, Churn) logistic_p_test ## # A tibble: 2,113 × 2 ## .pred_class Churn ## &lt;fct&gt; &lt;fct&gt; ## 1 No No ## 2 Yes Yes ## 3 No No ## 4 No No ## 5 No No ## 6 Yes No ## 7 No No ## 8 No No ## 9 No Yes ## 10 No No ## # … with 2,103 more rows 5.3.6 Métricas de desempeño Matriz de Confusión logistic_p_test %&gt;% yardstick::conf_mat(truth = Churn, estimate = .pred_class) %&gt;% autoplot(type = &quot;heatmap&quot;) bind_rows( yardstick::accuracy(logistic_p_test, Churn, .pred_class, event_level = &quot;second&quot;), yardstick::precision(logistic_p_test, Churn, .pred_class, event_level = &quot;second&quot;), yardstick::recall(logistic_p_test, Churn, .pred_class, event_level = &quot;second&quot;), yardstick::specificity(logistic_p_test, Churn, .pred_class, event_level = &quot;second&quot;), yardstick::f_meas(logistic_p_test, Churn, .pred_class, event_level = &quot;second&quot;) ) ## # A tibble: 5 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.803 ## 2 precision binary 0.684 ## 3 recall binary 0.493 ## 4 spec binary 0.917 ## 5 f_meas binary 0.573 ¿Y si se quiere un corte diferente? ¿el negocio qué necesita? logistic_p_test_prob &lt;- predict(logistic_fit1, telco_test_bake, type = &quot;prob&quot;) %&gt;% bind_cols(telco_test_bake) %&gt;% select(.pred_Yes, .pred_No, Churn) logistic_p_test_prob ## # A tibble: 2,113 × 3 ## .pred_Yes .pred_No Churn ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.0367 0.963 No ## 2 0.837 0.163 Yes ## 3 0.335 0.665 No ## 4 0.0130 0.987 No ## 5 0.0348 0.965 No ## 6 0.509 0.491 No ## 7 0.0753 0.925 No ## 8 0.0449 0.955 No ## 9 0.424 0.576 Yes ## 10 0.0187 0.981 No ## # … with 2,103 more rows logistic_p_test_prob &lt;- logistic_p_test_prob %&gt;% mutate(.pred_class = as_factor(if_else ( .pred_Yes &gt;= 0.30, &#39;Yes&#39;, &#39;No&#39;))) %&gt;% relocate(.pred_class , .after = .pred_No) logistic_p_test_prob ## # A tibble: 2,113 × 4 ## .pred_Yes .pred_No .pred_class Churn ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 0.0367 0.963 No No ## 2 0.837 0.163 Yes Yes ## 3 0.335 0.665 Yes No ## 4 0.0130 0.987 No No ## 5 0.0348 0.965 No No ## 6 0.509 0.491 Yes No ## 7 0.0753 0.925 No No ## 8 0.0449 0.955 No No ## 9 0.424 0.576 Yes Yes ## 10 0.0187 0.981 No No ## # … with 2,103 more rows bind_rows( yardstick::accuracy(logistic_p_test_prob, Churn, .pred_class, event_level = &quot;second&quot;), yardstick::precision(logistic_p_test_prob, Churn, .pred_class, event_level = &quot;second&quot;), yardstick::recall(logistic_p_test_prob, Churn, .pred_class, event_level = &quot;second&quot;), yardstick::specificity(logistic_p_test_prob, Churn, .pred_class, event_level = &quot;second&quot;), yardstick::f_meas(logistic_p_test_prob, Churn, .pred_class, event_level = &quot;second&quot;) ) ## # A tibble: 5 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.769 ## 2 precision binary 0.548 ## 3 recall binary 0.783 ## 4 spec binary 0.764 ## 5 f_meas binary 0.645 Para poder determinar cual es el mejor punto de corte, es indispensable conocer el comportamiento y efecto de los diferentes puntos de corte. Veamos un ejemplo visual en nuestra aplicación de Shiny: ConfusionMatrixShiny 5.4 Regularización En muchas técnicas de aprendizaje automático, el aprendizaje consiste en encontrar los coeficientes que minimizan una función de costo. Un modelo estándar de mínimos cuadrados tiende a tener alguna variación, es decir, este modelo no se generalizará bien para un conjunto de datos diferente a sus datos de entrenamiento. La regularización consiste en añadir una penalización a la función de costo. Esta penalización produce modelos más simples que generalizan mejor y evita el riesgo de sobreajuste. El procedimiento de ajuste implica una función de pérdida, conocida como suma de cuadrados residual o RSS. Los coeficientes \\(\\beta\\) se eligen de manera que minimicen esta función de pérdida. \\[RSS = \\sum_{i=1}^n\\left(y_i - \\beta_0- \\sum_{i=1}^p \\beta_jx_{ij}\\right)^2\\] Esto ajustará los coeficientes en función de sus datos de entrenamiento. Si hay ruido en los datos de entrenamiento, los coeficientes estimados no se generalizarán bien a los datos futuros. Aquí es donde entra la regularización y reduce o regulariza estas estimaciones aprendidas hacia cero. En esta sección se verán las regularizaciones más usadas en machine learning: Ridge (conocida también como L2) Lasso (también conocida como L1) ElasticNet que combina tanto Lasso como Ridge. Para cada una de estas regularizaciones ajustaremos un modelo de regresión lineal al conjunto de datos de viviendas de Ames con ayuda del paquete de tidymodels llamado parsnip. 5.4.1 Regularización Ridge En este tipo de regularización RSS se modifica agregando una cantidad de contracción a los coeficientes, los cuales se estiman minimizando esta función. \\(\\lambda\\) es el parámetro de ajuste que decide cuánto queremos penalizar la flexibilidad de el modelo. \\[\\sum_{i=1}^n\\left(y_i - \\beta_0- \\sum_{i=1}^p \\beta_jx_{ij}\\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 = RSS + \\lambda \\sum_{j=1}^p \\beta_j^2\\] El aumento de la flexibilidad de un modelo está representado por el aumento de sus coeficientes, si se desea minimizar la función anterior, los coeficientes deben ser pequeños. Así es como la técnica de regresión de Ridge evita que los coeficientes aumenten demasiado. Además, reduce la asociación estimada de cada variable con la respuesta excepto la intersección \\(\\beta_0\\). Esta intersección es una medida del valor medio de la respuesta cuando \\(x_{i1} = x_{i2} =\\dots= x_{ip} = 0\\). Cuando \\(\\lambda = 0\\), el término de penalización no tiene efecto y las estimaciones serán iguales a mínimos cuadrados. A medida que \\(\\lambda \\rightarrow \\infty\\), el impacto de la penalización por contracción aumenta, y las estimaciones se acercarán a cero. La selección de un buen valor de \\(\\lambda\\) es fundamental. Las estimaciones de coeficientes producidas por este método también se conocen como la norma L2. Nota: Es necesario estandarizar los predictores o llevarlos a la misma escala antes de aplicar esta regularización. 5.4.2 Regularización Lasso Lasso es otra variación, en la que se minimiza la función RSS. Utiliza \\(|\\beta_j|\\) en lugar de los cuadrados de \\(\\beta\\) como penalización. Las estimaciones de coeficientes producidas por este método también se conocen como la norma L1. \\[\\sum_{i=1}^n\\left(y_i - \\beta_0- \\sum_{i=1}^p \\beta_jx_{ij}\\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| = RSS + \\lambda \\sum_{j=1}^p |\\beta_j|\\] Cuando \\(\\lambda = 0\\), el término de penalización no tiene efecto y las estimaciones serán iguales a mínimos cuadrados. A medida que \\(\\lambda \\rightarrow \\infty\\), el impacto de la penalización por contracción aumenta, y las estimaciones se convierten en cero (eliminando variables). Este método de regularizacion permite eliminar coeficientes con alta variación, lo cual ayuda a la selección de variables. 5.4.3 Comparación entre Ridge y Lasso La regresión Ridge se puede considerar como la solución de una ecuación, donde la suma de los cuadrados de los coeficientes es menor o igual que \\(s\\), donde \\(s\\) es una constante que existe para cada valor del factor de contracción \\(\\lambda\\) \\[\\beta_1^2 + \\beta_2^2 \\leq s\\] Esto implica que los coeficientes de la regresión Ridge tienen el RSS (función de pérdida) más pequeño para todos los puntos que se encuentran dentro del círculo dado por la función de restricción \\(\\beta_1^2 + \\beta_2^2 \\leq s\\). Y en la regresión Lasso se puede considerar como una ecuación en la que la suma del módulo de coeficientes es menor o igual que \\(s\\). \\[|\\beta_1| + |\\beta_2| \\leq s\\] Esto implica que los coeficientes de lasso tienen la RSS (función de pérdida) más pequeña para todos los puntos que se encuentran dentro del diamante dado por la función de restricción \\(|\\beta_1| + |\\beta_2| \\leq s\\) La imagen de arriba muestra las funciones de restricción (áreas verdes) para Lasso (izquierda) y Ridge (derecha), junto con contornos para RSS (elipse roja). Para un valor muy grande de \\(s\\), las regiones verdes contendrán el centro de la elipse, lo que hará que las estimaciones de los coeficientes de ambas técnicas de regresión sean iguales a las estimaciones de mínimos cuadrados. Pero este no es el caso en la imagen de arriba. En este caso, las estimaciones del coeficiente de regresión de Lasso y Ridge vienen dadas por el primer punto en el que una elipse contacta con la región de restricción. Dado que la regresión Ridge tiene una restricción circular sin puntos agudos, esta intersección generalmente no ocurrirá en un eje, por lo que las estimaciones del coeficiente de regresión de Ridge serán exclusivamente distintas de cero. Sin embargo, la restricción de Lasso tiene esquinas en cada uno de los ejes, por lo que la elipse a menudo intersectará la región de restricción en un eje. Cuando esto ocurre, uno de los coeficientes será igual a cero. En dimensiones más altas, muchas de las estimaciones de coeficientes pueden ser guales a cero simultáneamente. Desventajas Regresión Ridge: Reducirá los coeficientes de los predictores menos importantes, muy cerca de cero. Pero nunca los hará exactamente cero. En otras palabras, el modelo final incluirá todos los predictores. Regresión Lasso: La penalización L1 tiene el efecto de forzar algunas de las estimaciones de coeficientes a ser exactamente iguales a cero cuando el parámetro de ajuste \\(\\lambda\\) es suficientemente grande. Por lo tanto, este método realiza una selección de variables. 5.4.4 ElasticNet ElasticNet surgió por primera vez como resultado de la crítica a Lasso, cuya selección de variables puede ser demasiado dependiente de los datos y, por lo tanto, inestable. La solución es combinar las penalizaciones de la regresión de Ridge y Lasso para obtener lo mejor de ambas regularizaciones. ElasticNet tiene como objetivo minimizar la siguiente función de pérdida: \\[\\frac{\\sum_{i=1}^n\\left(y_i - \\beta_0- \\sum_{i=1}^p \\beta_jx_{ij}\\right)^2}{2n} + \\lambda\\left( ({1-\\alpha}) \\sum_{j=1}^p|\\beta_j| + \\alpha \\sum_{j=1}^p \\beta_j ^2\\right)\\] \\[= \\frac{RSS}{2n}+ \\lambda\\left( ({1-\\alpha}) \\sum_{j=1}^p|\\beta_j| + \\alpha \\sum_{j=1}^p \\beta_j ^2\\right)\\] donde \\(\\alpha \\in [0,1]\\) es el parámetro de mezcla entre la regularización Ridge \\((\\alpha = 0)\\) y la regularización Lasso \\((\\alpha = 1)\\). La combinación de ambas penalizaciones suele dar lugar a buenos resultados. Una estrategia frecuentemente utilizada es asignarle casi todo el peso a la penalización L1 ( \\(\\alpha \\approx 1\\)) para conseguir seleccionar predictores y menos peso a la regularización \\(L2\\) para dar cierta estabilidad en el caso de que algunos predictores estén correlacionados. 5.4.5 ElasticNet para regresión lineal Utilizando el modelo linear_reg() del paquete parsnip. Hay varios mecanismos que pueden realizar la regularización/penalización, los paquetes glmnet, sparklyr, keras o stan. Usemos el primero aquí. El paquete glmnet solo implementa un método que no es de fórmula, pero parsnip permitirá que se use cualquiera de ellos. Cuando se utiliza la regularización, los predictores deben de centrarse y escalarse primero antes de pasar al modelo. El método de la fórmula no lo hará automáticamente, por lo que tendremos que hacerlo nosotros mismos como se hizo en la sección 4.6 Preparación de conjunto de datos con la receta receta_casas. En R existen dos parámetros que nos permiten hacer la regularización: penalty: Es un número no negativo que representa la cantidad total de regularización (solo glmnet, keras y spark). mixture: Es un número entre cero y uno (inclusivo) que es la proporción de regularización L1 en el modelo. Cuando mixture = 1, es un modelo de Lasso puro, mientras que mixture = 0 indica que se está utilizando un modelo Ridge. library(parallel) library(doParallel) #Partición set.seed(4595) ames_split &lt;- initial_split(ames, prop = 0.75) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) set.seed(2453) ames_folds&lt;- vfold_cv(ames_train) # Receta receta_casas &lt;- recipe(Sale_Price ~ . , data = ames_train) %&gt;% step_log(Sale_Price, skip = T) %&gt;% step_unknown(Alley) %&gt;% step_rename(Year_Remod = Year_Remod_Add) %&gt;% step_rename(ThirdSsn_Porch = Three_season_porch) %&gt;% step_ratio(Bedroom_AbvGr, denom = denom_vars(Gr_Liv_Area)) %&gt;% step_mutate( Age_House = Year_Sold - Year_Remod, TotalSF = Gr_Liv_Area + Total_Bsmt_SF, AvgRoomSF = Gr_Liv_Area / TotRms_AbvGrd, Pool = if_else(Pool_Area &gt; 0, 1, 0), Exter_Cond = forcats::fct_collapse(Exter_Cond, Good = c(&quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;))) %&gt;% step_relevel(Exter_Cond, ref_level = &quot;Good&quot;) %&gt;% step_normalize(all_predictors(), -all_nominal()) %&gt;% step_dummy(all_nominal()) %&gt;% step_interact(~ Second_Flr_SF:First_Flr_SF) %&gt;% step_interact(~ matches(&quot;Bsmt_Cond&quot;):TotRms_AbvGrd) %&gt;% step_rm( First_Flr_SF, Second_Flr_SF, Year_Remod, Bsmt_Full_Bath, Bsmt_Half_Bath, Kitchen_AbvGr, BsmtFin_Type_1_Unf, Total_Bsmt_SF, Kitchen_AbvGr, Pool_Area, Gr_Liv_Area, Sale_Type_Oth, Sale_Type_VWD ) %&gt;% prep() # Declaración del modelo lasso_ridge_regression_model &lt;- linear_reg( mixture = tune(), penalty = tune()) %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;glmnet&quot;) # Declaración del flujo de trabajo lasso_ridge_regression_workflow &lt;- workflow() %&gt;% add_model(lasso_ridge_regression_model) %&gt;% add_recipe(receta_casas) # Parámetros a probar lasso_ridge_regression_parameters_set &lt;- parameters( penalty(range = c(-8, 1), trans = log10_trans()), mixture(range = c(0,1)) ) # Grid lasso_ridge_regression_parameters_grid &lt;- grid_regular( lasso_ridge_regression_parameters_set, levels = c(40, 20) ) ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) lasso_ridge_regression_tunning &lt;- tune_grid( lasso_ridge_regression_workflow, resamples = ames_folds, grid = lasso_ridge_regression_parameters_grid, metrics = metric_set(rmse, rsq, mae, mape), control = ctrl_grid ) stopCluster(cluster) # Se guarda tune_grid en RDS lasso_ridge_regression_tunning %&gt;% saveRDS(&quot;models/lasso_ridge_regression_tunning.rds&quot;) lasso_ridge_regression_tunning &lt;- readRDS(&quot;models/lasso_ridge_regression_tunning.rds&quot;) lasso_ridge_regression_tunning ## # Tuning results ## # 10-fold cross-validation ## # A tibble: 10 × 5 ## splits id .metrics .notes .predictions ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [1977/220]&gt; Fold01 &lt;tibble [3,200 × 6]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble&gt; ## 2 &lt;split [1977/220]&gt; Fold02 &lt;tibble [3,200 × 6]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble&gt; ## 3 &lt;split [1977/220]&gt; Fold03 &lt;tibble [3,200 × 6]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble&gt; ## 4 &lt;split [1977/220]&gt; Fold04 &lt;tibble [3,200 × 6]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble&gt; ## 5 &lt;split [1977/220]&gt; Fold05 &lt;tibble [3,200 × 6]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble&gt; ## 6 &lt;split [1977/220]&gt; Fold06 &lt;tibble [3,200 × 6]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble&gt; ## 7 &lt;split [1977/220]&gt; Fold07 &lt;tibble [3,200 × 6]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble&gt; ## 8 &lt;split [1978/219]&gt; Fold08 &lt;tibble [3,200 × 6]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble&gt; ## 9 &lt;split [1978/219]&gt; Fold09 &lt;tibble [3,200 × 6]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble&gt; ## 10 &lt;split [1978/219]&gt; Fold10 &lt;tibble [3,200 × 6]&gt; &lt;tibble [1 × 1]&gt; &lt;tibble&gt; # Métrica R cuadrada lasso_ridge_regression_tunning %&gt;% unnest(.metrics) %&gt;% filter(.metric == &quot;rsq&quot;) %&gt;% arrange(desc(.estimate)) %&gt;% head(10) ## # A tibble: 10 × 10 ## splits id penalty mixture .metric .estimator .estimate .config ## &lt;list&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 &lt;split [1977/220]&gt; Fold… 1 e-4 0.0526 rsq standard 0.734 Prepro… ## 2 &lt;split [1977/220]&gt; Fold… 1.34e-4 0.0526 rsq standard 0.734 Prepro… ## 3 &lt;split [1977/220]&gt; Fold… 1.80e-4 0.0526 rsq standard 0.734 Prepro… ## 4 &lt;split [1977/220]&gt; Fold… 2.42e-4 0.0526 rsq standard 0.734 Prepro… ## 5 &lt;split [1977/220]&gt; Fold… 3.26e-4 0.0526 rsq standard 0.734 Prepro… ## 6 &lt;split [1977/220]&gt; Fold… 4.38e-4 0.0526 rsq standard 0.734 Prepro… ## 7 &lt;split [1977/220]&gt; Fold… 5.88e-4 0.0526 rsq standard 0.734 Prepro… ## 8 &lt;split [1977/220]&gt; Fold… 7.90e-4 0.0526 rsq standard 0.734 Prepro… ## 9 &lt;split [1977/220]&gt; Fold… 1.06e-3 0.0526 rsq standard 0.734 Prepro… ## 10 &lt;split [1977/220]&gt; Fold… 1.43e-3 0.0526 rsq standard 0.734 Prepro… ## # … with 2 more variables: .notes &lt;list&gt;, .predictions &lt;list&gt; # Analizando las métricas de manera visual graf_rsq &lt;- lasso_ridge_regression_tunning %&gt;% unnest(.metrics) %&gt;% filter(.metric == &quot;rsq&quot;) %&gt;% ggplot(aes(x = penalty, y = .estimate)) + scale_x_log10() + geom_line(aes(color = id))+ theme_minimal()+ ggtitle(&#39;R cuadrada vs Penalización&#39;) graf_rsq ## Warning: Removed 190 row(s) containing missing values (geom_path). lasso_ridge_regression_tunning %&gt;% collect_metrics() ## # A tibble: 3,200 × 8 ## penalty mixture .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.0001 0 mae standard 180544. 10 1014. Preprocess… ## 2 0.0001 0 mape standard 100. 10 0.0000689 Preprocess… ## 3 0.0001 0 rmse standard 197259. 10 1734. Preprocess… ## 4 0.0001 0 rsq standard 0.687 10 0.0110 Preprocess… ## 5 0.000134 0 mae standard 180544. 10 1014. Preprocess… ## 6 0.000134 0 mape standard 100. 10 0.0000689 Preprocess… ## 7 0.000134 0 rmse standard 197259. 10 1734. Preprocess… ## 8 0.000134 0 rsq standard 0.687 10 0.0110 Preprocess… ## 9 0.000180 0 mae standard 180544. 10 1014. Preprocess… ## 10 0.000180 0 mape standard 100. 10 0.0000689 Preprocess… ## # … with 3,190 more rows lasso_ridge_regression_tunning %&gt;% collect_metrics() %&gt;% filter(mixture == 0) %&gt;% ggplot(aes(penalty, mean, color = .metric)) + geom_errorbar(aes( ymin = mean - std_err, ymax = mean + std_err), alpha = 0.5) + geom_line(size = 0.8) + facet_wrap(~.metric, scales = &quot;free&quot;, nrow = 2) + scale_x_log10() + theme(legend.position = &quot;none&quot;) autoplot(lasso_ridge_regression_tunning) # Selección de los mejores 10 modelos según la métrica R cuadrada lasso_ridge_regression_tunning %&gt;% show_best(n = 10, metric = &quot;rsq&quot;) ## # A tibble: 10 × 8 ## penalty mixture .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.0367 0 rsq standard 0.687 10 0.0110 Preprocessor1_Model0… ## 2 0.0492 0 rsq standard 0.687 10 0.0110 Preprocessor1_Model0… ## 3 0.0273 0 rsq standard 0.687 10 0.0110 Preprocessor1_Model0… ## 4 0.0001 0 rsq standard 0.687 10 0.0110 Preprocessor1_Model0… ## 5 0.000134 0 rsq standard 0.687 10 0.0110 Preprocessor1_Model0… ## 6 0.000180 0 rsq standard 0.687 10 0.0110 Preprocessor1_Model0… ## 7 0.000242 0 rsq standard 0.687 10 0.0110 Preprocessor1_Model0… ## 8 0.000326 0 rsq standard 0.687 10 0.0110 Preprocessor1_Model0… ## 9 0.000438 0 rsq standard 0.687 10 0.0110 Preprocessor1_Model0… ## 10 0.000588 0 rsq standard 0.687 10 0.0110 Preprocessor1_Model0… # Selección del mejor modelo según la métrica R cuadrada lasso_ridge_regression_best_model &lt;- select_best( lasso_ridge_regression_tunning, metric = &quot;rsq&quot;) lasso_ridge_regression_best_model ## # A tibble: 1 × 3 ## penalty mixture .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.0367 0 Preprocessor1_Model021 # Selección del mejor modelo por un error estándar lasso_ridge_regression_best_1se_model &lt;- select_by_one_std_err( lasso_ridge_regression_tunning, metric = &quot;rsq&quot;, &quot;rsq&quot;) lasso_ridge_regression_best_1se_model ## # A tibble: 1 × 10 ## penalty mixture .metric .estimator mean n std_err .config .best .bound ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0001 0 rsq standard 0.687 10 0.0110 Preproces… 0.687 0.676 # Selección del mejor modelo por porcentaje de pérdida lasso_ridge_regression_best_pct_loss_model &lt;- select_by_pct_loss( lasso_ridge_regression_tunning, metric = &quot;rsq&quot;, &quot;rsq&quot;, limit = 5) lasso_ridge_regression_best_pct_loss_model ## # A tibble: 1 × 10 ## penalty mixture .metric .estimator mean n std_err .config .best .loss ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0001 0 rsq standard 0.687 10 0.0110 Preproce… 0.687 0.00578 # Modelo final lasso_ridge_regression_final_model &lt;- lasso_ridge_regression_workflow %&gt;% finalize_workflow(lasso_ridge_regression_best_1se_model) %&gt;% parsnip::fit(data = ames_train) lasso_ridge_regression_final_model %&gt;% tidy() %&gt;% as.data.frame() %&gt;% arrange(desc(abs(estimate))) %&gt;% head(20) ## term estimate penalty ## 1 (Intercept) 12.02013159 1e-04 ## 2 Year_Built 0.10133358 1e-04 ## 3 TotRms_AbvGrd 0.07467521 1e-04 ## 4 Fireplaces 0.06871314 1e-04 ## 5 Garage_Area 0.05345236 1e-04 ## 6 Full_Bath 0.05180514 1e-04 ## 7 Garage_Cars 0.04578311 1e-04 ## 8 BsmtFin_SF_1 -0.04297080 1e-04 ## 9 Mas_Vnr_Area 0.03498704 1e-04 ## 10 Bsmt_Unf_SF 0.03260094 1e-04 ## 11 Wood_Deck_SF 0.02892588 1e-04 ## 12 Screen_Porch 0.02266798 1e-04 ## 13 Open_Porch_SF 0.02162425 1e-04 ## 14 Lot_Area 0.02144120 1e-04 ## 15 Enclosed_Porch 0.02071560 1e-04 ## 16 Latitude 0.01936046 1e-04 ## 17 Longitude -0.01905589 1e-04 ## 18 Half_Bath 0.01890797 1e-04 ## 19 Misc_Val -0.01608877 1e-04 ## 20 Bedroom_AbvGr -0.01417964 1e-04 # Predicciones results &lt;- predict(lasso_ridge_regression_final_model, ames_test) %&gt;% dplyr::bind_cols(truth = ames_test$Sale_Price) %&gt;% mutate(.pred = exp(.pred)) %&gt;% dplyr::rename(pred_laso_ridge_reg = .pred, Sale_Price = truth) head(results) ## # A tibble: 6 × 2 ## pred_laso_ridge_reg Sale_Price ## &lt;dbl&gt; &lt;int&gt; ## 1 131177. 105000 ## 2 177281. 185000 ## 3 191728. 180400 ## 4 122011. 141000 ## 5 242145. 210000 ## 6 188283. 216000 results %&gt;% yardstick::metrics(Sale_Price, pred_laso_ridge_reg) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 40274. ## 2 rsq standard 0.759 ## 3 mae standard 27272. results %&gt;% ggplot(aes(x = pred_laso_ridge_reg, y = Sale_Price)) + geom_point() + geom_abline(color = &quot;red&quot;) + xlab(&quot;Prediction&quot;) + ylab(&quot;Observation&quot;) + ggtitle(&quot;Comparisson&quot;) 5.4.6 ElasticNet para regresión logística tilizando el modelo logistic_reg() del paquete parsnip. Hay varios mecanismos que pueden realizar la regularización/penalización, los paquetes glmnet, sparklyr, keras o stan. Usemos el primero aquí. El paquete glmnet solo implementa un método que no es de fórmula, pero parsnip permitirá que se use cualquiera de ellos. Cuando se utiliza la regularización, los predictores deben de centrarse y escalarse primero antes de pasar al modelo. El método de la fórmula no lo hará automáticamente, por lo que tendremos que hacerlo nosotros mismos como se hizo en la sección 4.6 Preparación de conjunto de datos con la receta receta_casas. En R existen dos parámetros que nos permiten hacer la regularización: penalty: Es un número no negativo que representa la cantidad total de regularización (solo glmnet, keras y spark). mixture: Es un número entre cero y uno (inclusivo) que es la proporción de regularización L1 en el modelo. Cuando mixture = 1, es un modelo de Lasso puro, mientras que mixture = 0 indica que se está utilizando un modelo Ridge. # Lectura de datos telco &lt;- read_csv(&quot;data/Churn.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_character(), ## SeniorCitizen = col_double(), ## tenure = col_double(), ## MonthlyCharges = col_double(), ## TotalCharges = col_double() ## ) ## ℹ Use `spec()` for the full column specifications. # Partición de muestras set.seed(1234) telco_split &lt;- initial_split(telco, prop = .7) telco_train &lt;- training(telco_split) telco_test &lt;- testing(telco_split) set.seed(1234) telco_folds &lt;- vfold_cv(telco_train) # Receta binner &lt;- function(x) { cut(x, breaks = c(0, 12, 24, 36,48,60,72), include.lowest = TRUE) %&gt;% as.numeric() } telco_rec &lt;- recipe(Churn ~ ., data = telco_train) %&gt;% update_role(customerID, new_role = &quot;id variable&quot;) %&gt;% step_num2factor(tenure, transform = binner, levels = c(&quot;0-1 year&quot;, &quot;1-2 years&quot;, &quot;2-3 years&quot;, &quot;3-4 years&quot;, &quot;4-5 years&quot;, &quot;5-6 years&quot;)) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_impute_median(all_numeric_predictors()) %&gt;% step_rm(customerID, skip=T) %&gt;% prep() # Declaración del modelo de regresión logística regularized_logistic_model &lt;- logistic_reg( mixture = tune(), penalty = tune()) %&gt;% set_mode(&#39;classification&#39;) %&gt;% set_engine(&quot;glmnet&quot;) # Declaración del workflow regularized_logistic_workflow &lt;- workflow() %&gt;% add_recipe(telco_rec) %&gt;% add_model(regularized_logistic_model) # Fijación de parámetros regularized_parameters_set &lt;- parameters( penalty( range = c(-10, 2), trans = log10_trans() ), dials::mixture() ) # Declaración del grid regularized_parameters_grid &lt;- grid_regular( regularized_parameters_set, levels = c(15, 15) ) ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) # Se crea el cluster de trabajo UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) regularized_tune_result &lt;- tune_grid( regularized_logistic_workflow, resamples = telco_folds, grid = regularized_parameters_grid, metrics = metric_set(roc_auc, pr_auc), control = ctrl_grid ) # Se detiene el cluster de trabajo stopCluster(cluster) # Se guarda tune_grid en formato RDS regularized_tune_result %&gt;% saveRDS(&quot;models/regularized_logistic_regression.rds&quot;) # Se carga el modelo regularized_tune_result &lt;- readRDS(&quot;models/regularized_logistic_regression.rds&quot;) # Métricas regularized_tune_result %&gt;% unnest(.metrics) ## # A tibble: 4,500 × 10 ## splits id penalty mixture .metric .estimator .estimate ## &lt;list&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 &lt;split [4437/493]&gt; Fold01 1 e-10 0 roc_auc binary 0.777 ## 2 &lt;split [4437/493]&gt; Fold01 7.20e-10 0 roc_auc binary 0.777 ## 3 &lt;split [4437/493]&gt; Fold01 5.18e- 9 0 roc_auc binary 0.777 ## 4 &lt;split [4437/493]&gt; Fold01 3.73e- 8 0 roc_auc binary 0.777 ## 5 &lt;split [4437/493]&gt; Fold01 2.68e- 7 0 roc_auc binary 0.777 ## 6 &lt;split [4437/493]&gt; Fold01 1.93e- 6 0 roc_auc binary 0.777 ## 7 &lt;split [4437/493]&gt; Fold01 1.39e- 5 0 roc_auc binary 0.777 ## 8 &lt;split [4437/493]&gt; Fold01 1 e- 4 0 roc_auc binary 0.777 ## 9 &lt;split [4437/493]&gt; Fold01 7.20e- 4 0 roc_auc binary 0.777 ## 10 &lt;split [4437/493]&gt; Fold01 5.18e- 3 0 roc_auc binary 0.777 ## # … with 4,490 more rows, and 3 more variables: .config &lt;chr&gt;, .notes &lt;list&gt;, ## # .predictions &lt;list&gt; # Analizando las métricas de manera visual graf_prec &lt;- regularized_tune_result %&gt;% unnest(.metrics) %&gt;% filter(.metric == &quot;pr_auc&quot;) %&gt;% ggplot(aes(x = penalty, y = .estimate)) + scale_x_log10() + geom_line(aes(color = id))+ theme_minimal()+ ggtitle(&#39;Precisión: Penalización vs. PR AUC&#39;) graf_prec graf_roc &lt;- regularized_tune_result %&gt;% unnest(.metrics) %&gt;% filter(.metric == &quot;roc_auc&quot;) %&gt;% ggplot(aes(x = penalty, y = .estimate)) + scale_x_log10() + geom_line(aes(color = id))+ theme_minimal()+ ggtitle(&#39;AUC: Penalización vs. ROC AUC&#39;) graf_roc regularized_tune_result %&gt;% collect_metrics() %&gt;% filter(mixture == 0.5) %&gt;% ggplot(aes(penalty, mean, color = .metric)) + geom_errorbar( aes(ymin = mean - std_err, ymax = mean + std_err), alpha = 0.5 ) + geom_line(size = 0.8) + facet_wrap(~.metric, scales = &quot;free&quot;, nrow = 2) + scale_x_log10() + theme(legend.position = &quot;none&quot;) autoplot(regularized_tune_result) regularized_tune_result %&gt;% collect_metrics() %&gt;% group_by(.metric) %&gt;% summarise( max = max(mean), min = min(mean), mean = mean(mean), median = median(mean), .groups = &quot;drop&quot; ) ## # A tibble: 2 × 5 ## .metric max min mean median ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pr_auc 0.912 0.868 0.900 0.900 ## 2 roc_auc 0.800 0.5 0.728 0.728 # Mejores 10 resultados para la métrica de precisión show_best(regularized_tune_result, n = 10, metric = &quot;pr_auc&quot;) ## # A tibble: 10 × 8 ## penalty mixture .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.0373 0.929 pr_auc binary 0.912 10 0.00492 Preprocessor1_Model206 ## 2 0.0373 0.857 pr_auc binary 0.912 10 0.00494 Preprocessor1_Model191 ## 3 0.0373 1 pr_auc binary 0.912 10 0.00493 Preprocessor1_Model221 ## 4 0.0373 0.786 pr_auc binary 0.912 10 0.00496 Preprocessor1_Model176 ## 5 0.0373 0.714 pr_auc binary 0.911 10 0.00497 Preprocessor1_Model161 ## 6 0.0373 0.643 pr_auc binary 0.911 10 0.00497 Preprocessor1_Model146 ## 7 0.0373 0.571 pr_auc binary 0.911 10 0.00498 Preprocessor1_Model131 ## 8 0.00518 1 pr_auc binary 0.911 10 0.00501 Preprocessor1_Model220 ## 9 0.00518 0.857 pr_auc binary 0.911 10 0.00502 Preprocessor1_Model190 ## 10 0.00518 0.929 pr_auc binary 0.911 10 0.00501 Preprocessor1_Model205 # Selección del mejor modelo según la métrica de precisión best_regularized_logistic_model &lt;- select_best(regularized_tune_result, metric = &quot;pr_auc&quot;) # Selección del mejor modelo por un error estándar best_regularized_logistic_model_1se &lt;- select_by_one_std_err( regularized_tune_result, metric = &quot;pr_auc&quot;, &quot;pr_auc&quot;) # Modelo final final_regularized_logistic_model &lt;- regularized_logistic_workflow %&gt;% finalize_workflow(best_regularized_logistic_model_1se) %&gt;% parsnip::fit(data = telco_train) final_regularized_logistic_model %&gt;% tidy() ## # A tibble: 4 × 3 ## term estimate penalty ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1.23 0.0000000001 ## 2 SeniorCitizen 0.234 0.0000000001 ## 3 MonthlyCharges 1.12 0.0000000001 ## 4 TotalCharges -1.17 0.0000000001 results &lt;- predict(final_regularized_logistic_model, telco_test, type = &#39;prob&#39;) %&gt;% dplyr::bind_cols(truth =telco_test$Churn) %&gt;% mutate(truth = factor(truth, levels = c(&#39;No&#39;, &#39;Yes&#39;), labels = c(&#39;No&#39;, &#39;Yes&#39;))) head(results, 10) ## # A tibble: 10 × 3 ## .pred_No .pred_Yes truth ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.808 0.192 No ## 2 0.331 0.669 Yes ## 3 0.835 0.165 No ## 4 0.885 0.115 No ## 5 0.919 0.0812 No ## 6 0.548 0.452 No ## 7 0.875 0.125 No ## 8 0.869 0.131 No ## 9 0.813 0.187 Yes ## 10 0.925 0.0749 No pr_curve_data &lt;- pr_curve( results, truth = truth, estimate = .pred_Yes, event_level = &#39;second&#39; ) roc_curve_data &lt;- roc_curve( results, truth = truth, estimate = .pred_Yes, event_level = &#39;second&#39; ) pr_curve_plot &lt;- pr_curve_data %&gt;% ggplot(aes(x = recall, y = precision)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + coord_equal() + ggtitle(&quot;Precision vs Recall&quot;)+ theme_minimal() pr_curve_plot roc_curve_plot &lt;- roc_curve_data %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + geom_abline() + coord_equal() + ggtitle(&quot;ROC Curve&quot;)+ theme_minimal() roc_curve_plot La curva Recall vs. Precision muestra la compensación entre precisión y recall para diferentes umbrales. Un área alta debajo de la curva representa tanto un alto recall como una alta precisión, donde la alta precisión se relaciona con una tasa baja de falsos positivos y recall alto se relaciona con una tasa baja de falsos negativos. Por ejemplo, en caso de que queramos tener una sensibilidad al rededor de 0.75, inevitablemente se obtendrá una tasa de falsos positivos de 0.45. Una curva ROC es un gráfico que muestra el rendimiento de un modelo de clasificación en todos los umbrales de clasificación. Esta curva traza dos parámetros: Tasa de verdaderos positivos: \\[TPR=\\frac{TP}{TP + FN}\\] Tasa de falsos positivos: \\[FPR = \\frac{FP}{FP + TN}\\] Una curva ROC traza TPR frente a FPR en diferentes umbrales de clasificación. Si se reduce el umbral de clasificación, se clasifican más elementos como positivos, lo que aumenta tanto los falsos positivos como los verdaderos positivos. Por ejemplo, si queremos tener una precisión de 0.80, la cobertura será apenas de 0.125 del total de positivos. 5.5 K-Nearest-Neighbor KNN es un algoritmo de aprendizaje supervisado que podemos usar tanto para regresión como clasificación. Es un algoritmo fácil de interpretar y que permite ser flexible en el balance entre sesgo y varianza (dependiendo de los hiper-parámetros seleccionados). El algoritmo de K vecinos más cercanos realiza comparaciones entre un nuevo elemento y las observaciones anteriores que ya cuentan con etiqueta. La esencia de este algoritmo está en etiquetar a un nuevo elemento de manera similar a como están etiquetados aquellos K elementos que más se le parecen. Veremos este proceso para cada uno de los posibles casos: 5.5.1 Clasificación La idea detrás del algoritmo es sencilla, eletiqueta una nueva observación en la categoria que tenga mas elementos de las k observaciones más cercanas, es decir: Seleccionamos el hiper-parámetro K como el número elegido de vecinos. Se calculará la similitud (distancia) de esta nueva observación a cada observación existente. Ordenaremos estas distancias de menor a mayor. Tomamos las K primeras entradas de la lista ordenada. La nueva observación será asignada al grupo que tenga mayor número de observaciones en estas k primeras distancias (asignación por moda) A continuación se ejemplifica este proceso: Ejemplo: Otro método que permite tener mayor control sobre las clasificaciones es asignar la probabilidad de pertenencia a cada clase de acuerdo con la proporción existente de cada una de las mismas. A partir de dichas probabilidades, el usuario puede determinar el punto de corte que sea más conveniente para el problema a resolver. 5.5.2 Regresión En el caso de regresión, la eletiqueta de una nueva observación se realiza a través del promedio del valor en las k observaciones más cercanas, es decir: Seleccionamos el hiper-parámetro K como el número elegido de vecinos. Se calculará la similitud (distancia) de esta nueva observación a cada observación existente Ordenaremos estas distancias de menor a mayor Tomamos las K primeras entradas de la lista ordenada. La nueva observación será etiquetada mediante el promedio del valor de las observaciones en estas k primeras distancias. Considerando un modelo de 3 vecinos más cercanos, las siguientes imágenes muestran el proceso de ajuste y predicción de nuevas observaciones. Ejemplo de balance de sesgo y varianza 5.5.3 Ajuste del modelo En contraste con otros algoritmos de aprendizaje supervisado, K-NN no genera un modelo del aprendizaje con datos de entrenamiento, sino que el aprendizaje sucede en el mismo momento en el que se prueban los datos de prueba. A este tipo de algoritmos se les llama lazy learning methods porque no aprende del conjunto de entrenamiento inmediatamente, sino que almacena el conjunto de datos y, en el momento de la clasificación, realiza una acción en el conjunto de datos. El algoritmo KNN en la fase de entrenamiento simplemente almacena el conjunto de datos y cuando obtiene nuevos datos, clasifica esos datos en una categoría que es muy similar a los nuevos datos. 5.5.3.1 Selección de Hiper-parámetro K Al configurar un modelo KNN, sólo hay algunos parámetros que deben elegirse/ajustarse para mejorar el rendimiento, uno de estos parámetros es el valor de la K. No existe una forma particular de determinar el mejor valor para “K,” por lo que debemos probar algunos valores para encontrar “el mejor” de ellos. Para los modelos de clasificación, especialmente si solo hay dos clases, generalmente se elige un número impar para k. Esto es para que el algoritmo nunca llegue a un “empate” Una opción para seleccionar la K adecuada es ejecutar el algoritmo KNN varias veces con diferentes valores de K y elegimos la K que reduce la cantidad de errores mientras se mantiene la capacidad del algoritmo para hacer predicciones con precisión. Observemos lo siguiente: Estas gráficas se conoce como “gráfica de codo” y generalmente se usan para determinar el valor K. A medida que disminuimos el valor de K a 1, nuestras predicciones se vuelven menos estables. Imaginemos que tomamos K = 1 y tenemos un punto de consulta rodeado por varios rojos y uno verde, pero el verde es el vecino más cercano. Razonablemente, pensaríamos que el punto de consulta es probablemente rojo, pero como K = 1, KNN predice incorrectamente que el punto de consulta es verde. Inversamente, a medida que aumentamos el valor de K, nuestras predicciones se vuelven más estables debido a que tenemos más observaciones con quienes comparar, por lo tanto, es más probable que hagan predicciones más precisas. Eventualmente, comenzamos a presenciar un número creciente de errores, es en este punto que sabemos que hemos llevado el valor de K demasiado lejos. 5.5.3.2 Métodos de cálculo de la distancia entre observaciones Otro parámetro que podemos ajustar para el modelo es la distancia usada, existen diferentes formas de medir qué tan “cerca” están dos puntos entre sí, y las diferencias entre estos métodos pueden volverse significativas en dimensiones superiores. La más utilizada es la distancia euclidiana, el tipo estándar de distancia. \\[d(X,Y) = \\sqrt{\\sum_{i=1}^{n} (x_i-y_i)^2}\\] Otra métrica es la llamada distancia de Manhattan, que mide la distancia tomada en cada dirección cardinal, en lugar de a lo largo de la diagonal. \\[d(X,Y) = \\sum_{i=1}^{n} |x_i - y_i|\\] De manera más general, las anteriores son casos particulares de la distancia de Minkowski, cuya fórmula es: \\[d(X,Y) = (\\sum_{i=1}^{n} |x_i-y_i|^p)^{\\frac{1}{p}}\\] La distancia de coseno es ampliamente en análisis de texto, sistemas de recomendación \\[d(X,Y)= 1 - \\frac{\\sum_{i=1}^{n}{X_iY_i}}{\\sqrt{\\sum_{i=1}^{n}{X_i^2}}\\sqrt{\\sum_{i=1}^{n}{Y_i^2}}}\\] Un link interesante Otro link interesante 5.5.4 Implementación en R Usaremos las recetas antes implementadas para ajustar tanto el modelo de regresión como el de clasificación. Exploraremos un conjunto de hiperparámetros para elegir el mejor modelo. 5.5.4.1 Regresión knn_model &lt;- nearest_neighbor( mode = &quot;regression&quot;, neighbors = tune(&quot;K&quot;), weight_func = tune()) %&gt;% set_engine(&quot;kknn&quot;) knn_workflow &lt;- workflow() %&gt;% add_recipe(receta_casas) %&gt;% add_model(knn_model) knn_parameters_set &lt;- parameters(knn_workflow) %&gt;% update(K = dials::neighbors(c(10,80)), weight_func = weight_func(values = c(&quot;rectangular&quot;, &quot;inv&quot;, &quot;gaussian&quot;, &quot;cos&quot;)) ) set.seed(123) knn_grid &lt;- knn_parameters_set %&gt;% grid_max_entropy(size = 150) ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) library(doParallel) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) casas_folds &lt;- vfold_cv(ames_train) knnt1 &lt;- Sys.time() knn_tune_result &lt;- tune_grid( knn_workflow, resamples = casas_folds, grid = knn_grid, metrics = metric_set(rmse, mae, mape), control = ctrl_grid ) knnt2 &lt;- Sys.time(); knnt2 - knnt1 stopCluster(cluster) knn_tune_result %&gt;% saveRDS(&quot;models/knn_model_reg.rds&quot;) Podemos obtener las métricas de cada fold con el siguiente código: knn_tune_result &lt;- readRDS(&quot;models/knn_model_reg.rds&quot;) knn_tune_result %&gt;% unnest(.metrics) ## # A tibble: 3,660 × 10 ## splits id K weight_func .metric .estimator .estimate ## &lt;list&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 &lt;split [1845/206]&gt; Fold01 11 cos rmse standard 0.246 ## 2 &lt;split [1845/206]&gt; Fold01 12 cos rmse standard 0.245 ## 3 &lt;split [1845/206]&gt; Fold01 22 cos rmse standard 0.247 ## 4 &lt;split [1845/206]&gt; Fold01 27 cos rmse standard 0.248 ## 5 &lt;split [1845/206]&gt; Fold01 28 cos rmse standard 0.248 ## 6 &lt;split [1845/206]&gt; Fold01 30 cos rmse standard 0.248 ## 7 &lt;split [1845/206]&gt; Fold01 33 cos rmse standard 0.249 ## 8 &lt;split [1845/206]&gt; Fold01 35 cos rmse standard 0.249 ## 9 &lt;split [1845/206]&gt; Fold01 37 cos rmse standard 0.249 ## 10 &lt;split [1845/206]&gt; Fold01 39 cos rmse standard 0.249 ## # … with 3,650 more rows, and 3 more variables: .config &lt;chr&gt;, .notes &lt;list&gt;, ## # .predictions &lt;list&gt; En la siguiente gráfica observamos el error cuadrático medio de las distintas métricas con distintos números de vecinos. En los argumentos de la funcion, se puede seleccionar el kernel, esto es las opciones posibles para ponderar el promedio respecto a la distancia seleccinada. “Rectangular” (que es knn estándar no ponderado), “triangular,” “cos,” “inv,” “gaussiano,” “rango” y “óptimo.” Para conocer más a cerca de las distintas métricas de distancia pueden consultar: Measures y KNN function knn_tune_result %&gt;% autoplot(metric = &quot;rmse&quot;) En la siguiente gráfica observamos el error absoluto promedio de las distintas métricas con distintos números de vecinos. knn_tune_result %&gt;% autoplot(metric = &quot;mae&quot;) Con el siguiente código obtenemos los mejores 10 modelos respecto al rmse. show_best(knn_tune_result, n = 10, metric = &quot;rmse&quot;) ## # A tibble: 10 × 8 ## K weight_func .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 12 cos rmse standard 0.203 10 0.00892 Preprocessor1_Model… ## 2 22 cos rmse standard 0.203 10 0.00887 Preprocessor1_Model… ## 3 11 cos rmse standard 0.203 10 0.00885 Preprocessor1_Model… ## 4 27 cos rmse standard 0.204 10 0.00887 Preprocessor1_Model… ## 5 28 cos rmse standard 0.204 10 0.00890 Preprocessor1_Model… ## 6 30 cos rmse standard 0.204 10 0.00888 Preprocessor1_Model… ## 7 13 gaussian rmse standard 0.204 10 0.00873 Preprocessor1_Model… ## 8 15 gaussian rmse standard 0.205 10 0.00868 Preprocessor1_Model… ## 9 11 gaussian rmse standard 0.205 10 0.00855 Preprocessor1_Model… ## 10 33 cos rmse standard 0.205 10 0.00890 Preprocessor1_Model… Ahora obtendremos el modelo que mejor desempeño tiene tomando en cuenta el rmse y haremos las predicciones del conjunto de prueba con este modelo. best_knn_model_reg &lt;- select_best(knn_tune_result, metric = &quot;rmse&quot;) final_knn_model_reg &lt;- knn_workflow %&gt;% finalize_workflow(best_knn_model_reg) %&gt;% parsnip::fit(data = ames_train) results_reg &lt;- predict(final_knn_model_reg, ames_test) %&gt;% dplyr::bind_cols(truth = ames_test$Sale_Price) %&gt;% mutate(.pred = exp(.pred) ) Métricas de desempeño Ahora para calcular las métricas de desempeño usaremos la paqueteria MLmetrics library(MLmetrics) results_reg %&gt;% summarise( mae = MLmetrics::MAE(.pred, truth), mape = MLmetrics::MAPE(.pred, truth), rmse = MLmetrics::RMSE(.pred, truth), r2 = MLmetrics::R2_Score(.pred, truth), rmsle = MLmetrics::RMSLE(.pred, truth) ) ## # A tibble: 1 × 5 ## mae mape rmse r2 rmsle ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 25772. 0.146 40840. 0.743 0.209 5.5.4.2 Clasificación Repetiremos el proceso para el problema de clasificación. knn_model &lt;- nearest_neighbor( mode = &quot;classification&quot;, neighbors = tune(&quot;K&quot;), weight_func = tune()) %&gt;% set_engine(&quot;kknn&quot;) knn_workflow &lt;- workflow() %&gt;% add_recipe(telco_rec) %&gt;% add_model(knn_model) knn_parameters_set &lt;- parameters(knn_workflow) %&gt;% update(K = dials::neighbors(c(10,80)), weight_func = weight_func(values = c(&quot;rectangular&quot;, &quot;inv&quot;, &quot;gaussian&quot;, &quot;cos&quot;)) ) set.seed(123) knn_grid &lt;- knn_parameters_set %&gt;% grid_max_entropy(size = 150) ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) library(doParallel) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) telco_folds &lt;- vfold_cv(telco_train) knnt1 &lt;- Sys.time() knn_tune_result &lt;- tune_grid( knn_workflow, resamples = telco_folds, grid = knn_grid, metrics = metric_set(roc_auc, pr_auc, sens), control = ctrl_grid ) knnt2 &lt;- Sys.time(); knnt2 - knnt1 stopCluster(cluster) knn_tune_result %&gt;% saveRDS(&quot;models/knn_model_cla.rds&quot;) knn_tune_result &lt;- readRDS(&quot;models/knn_model_cla.rds&quot;) autoplot(knn_tune_result, metric = &quot;pr_auc&quot;) autoplot(knn_tune_result, metric = &quot;roc_auc&quot;) show_best(knn_tune_result, n = 10, metric = &quot;pr_auc&quot;) ## # A tibble: 10 × 8 ## K weight_func .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 79 gaussian pr_auc binary 0.925 10 0.00268 Preprocessor1_Model… ## 2 78 gaussian pr_auc binary 0.925 10 0.00270 Preprocessor1_Model… ## 3 71 gaussian pr_auc binary 0.924 10 0.00276 Preprocessor1_Model… ## 4 74 gaussian pr_auc binary 0.924 10 0.00275 Preprocessor1_Model… ## 5 79 cos pr_auc binary 0.924 10 0.00272 Preprocessor1_Model… ## 6 77 cos pr_auc binary 0.924 10 0.00274 Preprocessor1_Model… ## 7 73 gaussian pr_auc binary 0.924 10 0.00276 Preprocessor1_Model… ## 8 67 gaussian pr_auc binary 0.924 10 0.00280 Preprocessor1_Model… ## 9 66 gaussian pr_auc binary 0.924 10 0.00281 Preprocessor1_Model… ## 10 68 cos pr_auc binary 0.924 10 0.00278 Preprocessor1_Model… best_knn_model_cla &lt;- select_best(knn_tune_result, metric = &quot;pr_auc&quot;) final_knn_model_cla &lt;- knn_workflow %&gt;% finalize_workflow(best_knn_model_cla) %&gt;% parsnip::fit(data = telco_train) results_cla &lt;- predict(final_knn_model_cla, telco_test, type = &#39;prob&#39;) %&gt;% dplyr::bind_cols(truth =telco_test$Churn) %&gt;% mutate(truth = factor(truth, levels = c(&#39;No&#39;, &#39;Yes&#39;), labels = c(&#39;No&#39;, &#39;Yes&#39;))) head(results_cla, 10) ## # A tibble: 10 × 3 ## .pred_No .pred_Yes truth ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.887 0.113 No ## 2 0.290 0.710 Yes ## 3 0.750 0.250 No ## 4 0.933 0.0669 No ## 5 0.857 0.143 No ## 6 0.554 0.446 No ## 7 0.950 0.0498 No ## 8 0.846 0.154 No ## 9 0.665 0.335 Yes ## 10 0.982 0.0182 No pr_curve_data &lt;- pr_curve( results_cla, truth = truth, estimate = .pred_Yes, event_level = &#39;second&#39; ) roc_curve_data &lt;- roc_curve( results_cla, truth = truth, estimate = .pred_Yes, event_level = &#39;second&#39; ) pr_curve_plot &lt;- pr_curve_data %&gt;% ggplot(aes(x = recall, y = precision)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + coord_equal() + ggtitle(&quot;Precision vs Recall&quot;)+ theme_minimal() pr_curve_plot roc_curve_plot &lt;- roc_curve_data %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + geom_abline() + coord_equal() + ggtitle(&quot;ROC Curve&quot;)+ theme_minimal() roc_curve_plot 5.6 Árboles de decisión Un árbol de decisiones es un algoritmo del aprendizaje supervisado que se puede utilizar tanto para problemas de clasificación como de regresión. Es un clasificador estructurado en árbol, donde los nodos internos representan las características de un conjunto de datos, las ramas representan las reglas de decisión y cada nodo hoja representa el resultado. La idea básica de los árboles es buscar puntos de cortes en las variables de entrada para hacer predicciones, ir dividiendo la muestra, y encontrar cortes sucesivos para refinar las predicciones. En un árbol de decisión, hay dos tipos nodos, el nodo de decisión o nodos internos (Decision Node) y el nodo hoja o nodo terminal (Leaf node). Los nodos de decisión se utilizan para tomar cualquier decisión y tienen múltiples ramas, mientras que los nodos hoja son el resultado de esas decisiones y no contienen más ramas. Regresión: Clasificación: 5.6.1 Ajuste del modelo En un árbol de decisión, para predecir la clase del conjunto de datos, el algoritmo comienza desde el nodo raíz del árbol. Este algoritmo compara los valores de la variable raíz con la variable de registro y, según la comparación, sigue una rama y salta al siguiente nodo. Para el siguiente nodo, el algoritmo vuelve a comparar el valor de la siguiente variable con los otros subnodos y avanza. Continúa el proceso hasta que se llega a un nodo hoja. El proceso completo se puede comprender mejor con los siguientes pasos: Comenzamos el árbol con el nodo raíz, llamémoslo S, que contiene el conjunto de entrenamiento completo. Encuentre la mejor variable en el conjunto de datos usando Attribute Selective Measure (ASM). Divida la S en subconjuntos que contengan valores posibles para la mejor variable. Genere el nodo del árbol de decisión, que contiene la mejor variable. Cree de forma recursiva nuevos árboles de decisión utilizando los subconjuntos del conjunto de datos creado en el paso 3. Continúe este proceso hasta que se alcance una etapa en la que no pueda particionar más los nodos y este nodo final sera un nodo hoja. Para clasificación nos quedaremos la moda de la variable respuesta del nodo hoja y para regresión usaremos la media de la variable respuesta. 5.6.1.1 Attribute Selective Measure (ASM) Al implementar un árbol de decisión, surge el problema principal de cómo seleccionar la mejor variable para el nodo raíz y para los subnodos. Para resolver este problemas existe una técnica que se llama medida de selección de atributos o ASM. Mediante esta medición, podemos seleccionar fácilmente la mejor variable para los nodos del árbol. Hay dos técnicas populares para ASM, que son: Índice de Gini La medida del grado de probabilidad de que una variable en particular se clasifique incorrectamente cuando se elige al azar se llama índice de Gini o impureza de Gini. Los datos se distribuyen por igual según el índice de Gini. \\[Gini = \\sum_{i=1}^{n}\\hat{p_i}(1-\\hat{p}_i)\\] Con \\(p_i\\) como la probabilidad de que un objeto se clasifique en una clase particular. Esta métrica puede analizarse como una métrica de impureza. Cuando todos o la mayoría de elementos dentro de un nodo pertenecen a una misma clase, el índice de Gini toma valores cercanos a cero. Cuando se utiliza el índice de Gini como criterio seleccionar la variable para el nodo raíz, seleccionaremos la variable con el índice de Gini menor. 5.6.1.2 ¿Cuándo dejar de dividir un nodo? Podríamos preguntarnos cuándo dejar de crecer un árbol. Pueden existir problemas que tengan un gran conjunto de variables y esto da como resultado una gran cantidad de divisiones, lo que a su vez genera un árbol de decisión muy grande. Estos árboles son complejos y pueden provocar un sobreajuste. Entonces, necesitamos saber cuándo parar. Una forma de hacer esto es establecer un número mínimo de entradas de entrenamiento para dividir un nodo. Otra forma es establecer la profundidad máxima de su modelo. La profundidad máxima se refiere a la longitud del camino más largo desde el nodo raíz hasta un nodo hoja. 5.6.1.3 ¿Cuándo podar un árbol? El rendimiento de un árbol se puede aumentar aún más mediante la poda del árbol. Esto se refiere a eliminar las ramas que hacen uso de variables de poca importancia. De esta manera, reducimos la complejidad del árbol y, por lo tanto, aumentamos su poder predictivo al reducir el sobreajuste. La poda puede comenzar en la raíz o en las hojas. El método más simple de poda comienza en las hojas y elimina cada nodo con la clase más popular en esa hoja, este cambio se mantiene si no deteriora la precisión. Se pueden usar métodos de poda más sofisticados, como la poda de complejidad de costos, donde se usa un parámetro de aprendizaje (alfa) para observar si los nodos se pueden eliminar en función del tamaño del subárbol. 5.6.2 Implementación en R Usaremos las recetas antes implementadas para ajustar tanto el modelo de regresión como el de clasificación. Exploraremos un conjunto de hiperparámetros para elegir el mejor modelo. En esta ocasión usamos la función grid_regular() donde con el parámetro levels le indicamos cuantos distintos números queremos para cada parámetro a ajustar. 5.6.2.1 Regresión: tree_model &lt;- decision_tree( mode = &quot;regression&quot;, tree_depth = tune(), cost_complexity = tune(), min_n = tune()) %&gt;% set_engine(&quot;rpart&quot;) tree_workflow &lt;- workflow() %&gt;% add_recipe(receta_casas) %&gt;% add_model(tree_model) tree_grid &lt;- grid_regular( min_n(), tree_depth(), cost_complexity(), levels = 5 ) ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) casas_folds &lt;- vfold_cv(ames_train) knnt1 &lt;- Sys.time() tree_tune_result &lt;- tune_grid( tree_workflow, resamples = casas_folds, grid = tree_grid, metrics = metric_set(rmse, mae, mape), control = ctrl_grid ) knnt2 &lt;- Sys.time(); knnt2 - knnt1 stopCluster(cluster) tree_tune_result %&gt;% saveRDS(&quot;models/tree_model_reg.rds&quot;) Podemos obtener las métricas de cada fold con el siguiente código: tree_tune_result &lt;- readRDS(&quot;models/tree_model_reg.rds&quot;) tree_tune_result %&gt;% unnest(.metrics) ## # A tibble: 3,750 × 11 ## splits id cost_complexity tree_depth min_n .metric .estimator ## &lt;list&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &lt;split [1977/220]&gt; Fold01 0.0000000001 1 2 rmse standard ## 2 &lt;split [1977/220]&gt; Fold01 0.0000000001 1 2 mae standard ## 3 &lt;split [1977/220]&gt; Fold01 0.0000000001 1 2 mape standard ## 4 &lt;split [1977/220]&gt; Fold01 0.0000000001 1 11 rmse standard ## 5 &lt;split [1977/220]&gt; Fold01 0.0000000001 1 11 mae standard ## 6 &lt;split [1977/220]&gt; Fold01 0.0000000001 1 11 mape standard ## 7 &lt;split [1977/220]&gt; Fold01 0.0000000001 1 21 rmse standard ## 8 &lt;split [1977/220]&gt; Fold01 0.0000000001 1 21 mae standard ## 9 &lt;split [1977/220]&gt; Fold01 0.0000000001 1 21 mape standard ## 10 &lt;split [1977/220]&gt; Fold01 0.0000000001 1 30 rmse standard ## # … with 3,740 more rows, and 4 more variables: .estimate &lt;dbl&gt;, .config &lt;chr&gt;, ## # .notes &lt;list&gt;, .predictions &lt;list&gt; En la siguiente gráfica observamos el error cuadrático medio de las distintas métricas con distintos números de vecinos. tree_tune_result %&gt;% autoplot(metric = &quot;rmse&quot;) En la siguiente gráfica observamos el error absoluto promedio de las distintas métricas con distintos números de vecinos. tree_tune_result %&gt;% autoplot(metric = &quot;mae&quot;) Con el siguiente código obtenemos los mejores 10 modelos respecto al rmse. show_best(tree_tune_result, n = 10, metric = &quot;rmse&quot;) ## # A tibble: 10 × 9 ## cost_complexity tree_depth min_n .metric .estimator mean n std_err ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.0000000001 15 11 rmse standard 197132. 10 3010. ## 2 0.0000000178 15 11 rmse standard 197132. 10 3010. ## 3 0.00000316 15 11 rmse standard 197132. 10 3010. ## 4 0.0000000001 11 11 rmse standard 197132. 10 3010. ## 5 0.0000000178 11 11 rmse standard 197132. 10 3010. ## 6 0.00000316 11 11 rmse standard 197132. 10 3010. ## 7 0.0000000001 15 2 rmse standard 197132. 10 3010. ## 8 0.0000000178 15 2 rmse standard 197132. 10 3010. ## 9 0.00000316 15 2 rmse standard 197132. 10 3010. ## 10 0.0000000001 8 21 rmse standard 197132. 10 3010. ## # … with 1 more variable: .config &lt;chr&gt; Ahora obtendremos el modelo que mejor desempeño tiene tomando en cuenta el rmse y haremos las predicciones del conjunto de prueba con este modelo. best_tree_model_reg &lt;- select_best(tree_tune_result, metric = &quot;rmse&quot;) final_tree_model_reg &lt;- tree_workflow %&gt;% finalize_workflow(best_tree_model_reg) %&gt;% parsnip::fit(data = ames_train) results_reg &lt;- predict(final_tree_model_reg, ames_test) %&gt;% dplyr::bind_cols(truth = ames_test$Sale_Price) %&gt;% mutate(.pred = exp(.pred)) Nuestro árbol final se ve de la siguiente manera: library(rpart.plot) final_tree_model_reg %&gt;% extract_fit_engine() %&gt;% rpart.plot(roundint = FALSE) Podemos obtener la impotancia de las variables: library(vip) final_tree_model_reg %&gt;% extract_fit_parsnip() %&gt;% vip() + ggtitle(&quot;Importancia de las variables&quot;) 5.6.2.1.1 Métricas de desempeño Ahora para calcular las métricas de desempeño usaremos la paqueteria MLmetrics library(MLmetrics) results_reg %&gt;% summarise( mae = MLmetrics::MAE(.pred, truth), mape = MLmetrics::MAPE(.pred, truth), rmse = MLmetrics::RMSE(.pred, truth), r2 = MLmetrics::R2_Score(.pred, truth), rmsle = MLmetrics::RMSLE(.pred, truth) ) ## # A tibble: 1 × 5 ## mae mape rmse r2 rmsle ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 29305. 0.172 44333. 0.697 0.234 5.6.2.2 Clasificación Repetiremos el proceso para el problema de clasificación. tree_model &lt;- decision_tree( mode = &quot;classification&quot;, tree_depth = tune(), cost_complexity = tune(), min_n = tune()) %&gt;% set_engine(&quot;rpart&quot;) tree_workflow &lt;- workflow() %&gt;% add_recipe(telco_rec) %&gt;% add_model(tree_model) tree_grid &lt;- grid_regular( min_n(), tree_depth(), cost_complexity(), levels = 5 ) ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) library(doParallel) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) telco_folds &lt;- vfold_cv(telco_train) treet1 &lt;- Sys.time() tree_tune_result &lt;- tune_grid( tree_workflow, resamples = telco_folds, grid = tree_grid, metrics = metric_set(roc_auc, pr_auc, sens), control = ctrl_grid ) treet2 &lt;- Sys.time(); knnt2 - knnt1 stopCluster(cluster) tree_tune_result %&gt;% saveRDS(&quot;models/tree_model_cla.rds&quot;) tree_tune_result &lt;- readRDS(&quot;models/tree_model_cla.rds&quot;) tree_tune_result %&gt;% unnest(.metrics) ## # A tibble: 3,750 × 11 ## splits id cost_complexity tree_depth min_n .metric .estimator ## &lt;list&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &lt;split [4437/493]&gt; Fold01 0.0000000001 1 2 sens binary ## 2 &lt;split [4437/493]&gt; Fold01 0.0000000001 1 2 roc_auc binary ## 3 &lt;split [4437/493]&gt; Fold01 0.0000000001 1 2 pr_auc binary ## 4 &lt;split [4437/493]&gt; Fold01 0.0000000001 1 11 sens binary ## 5 &lt;split [4437/493]&gt; Fold01 0.0000000001 1 11 roc_auc binary ## 6 &lt;split [4437/493]&gt; Fold01 0.0000000001 1 11 pr_auc binary ## 7 &lt;split [4437/493]&gt; Fold01 0.0000000001 1 21 sens binary ## 8 &lt;split [4437/493]&gt; Fold01 0.0000000001 1 21 roc_auc binary ## 9 &lt;split [4437/493]&gt; Fold01 0.0000000001 1 21 pr_auc binary ## 10 &lt;split [4437/493]&gt; Fold01 0.0000000001 1 30 sens binary ## # … with 3,740 more rows, and 4 more variables: .estimate &lt;dbl&gt;, .config &lt;chr&gt;, ## # .notes &lt;list&gt;, .predictions &lt;list&gt; tree_tune_result %&gt;% autoplot(metric = &quot;pr_auc&quot;) tree_tune_result %&gt;% autoplot(metric = &quot;roc_auc&quot;) show_best(tree_tune_result, n = 10, metric = &quot;pr_auc&quot;) ## # A tibble: 10 × 9 ## cost_complexity tree_depth min_n .metric .estimator mean n std_err ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.0000000001 11 30 pr_auc binary 0.903 10 0.00584 ## 2 0.0000000178 11 30 pr_auc binary 0.903 10 0.00584 ## 3 0.00000316 11 30 pr_auc binary 0.903 10 0.00584 ## 4 0.0000000001 8 30 pr_auc binary 0.900 10 0.00812 ## 5 0.0000000178 8 30 pr_auc binary 0.900 10 0.00812 ## 6 0.00000316 8 30 pr_auc binary 0.900 10 0.00812 ## 7 0.0000000001 4 2 pr_auc binary 0.899 10 0.00745 ## 8 0.0000000001 4 11 pr_auc binary 0.899 10 0.00745 ## 9 0.0000000178 4 2 pr_auc binary 0.899 10 0.00745 ## 10 0.0000000178 4 11 pr_auc binary 0.899 10 0.00745 ## # … with 1 more variable: .config &lt;chr&gt; best_tree_model_cla &lt;- select_best(tree_tune_result, metric = &quot;pr_auc&quot;) final_tree_model_cla &lt;- tree_workflow %&gt;% finalize_workflow(best_tree_model_cla) %&gt;% parsnip::fit(data = telco_train) results_cla &lt;- predict(final_tree_model_cla, telco_test, type = &#39;prob&#39;) %&gt;% dplyr::bind_cols(truth =telco_test$Churn) %&gt;% mutate(truth = factor(truth, levels = c(&#39;No&#39;, &#39;Yes&#39;), labels = c(&#39;No&#39;, &#39;Yes&#39;))) head(results_cla) ## # A tibble: 6 × 3 ## .pred_No .pred_Yes truth ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.918 0.0822 No ## 2 0.227 0.773 Yes ## 3 0.3 0.7 No ## 4 0.981 0.0186 No ## 5 0.881 0.119 No ## 6 0.652 0.348 No results_cla_tree &lt;- results_cla pr_curve_data &lt;- pr_curve( results_cla, truth = truth, estimate = .pred_Yes, event_level = &#39;second&#39; ) roc_curve_data &lt;- roc_curve( results_cla, truth = truth, estimate = .pred_Yes, event_level = &#39;second&#39; ) pr_curve_plot &lt;- pr_curve_data %&gt;% ggplot(aes(x = recall, y = precision)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + ylim(c(0,1)) + coord_equal() + ggtitle(&quot;Precision vs Recall&quot;)+ theme_minimal() pr_curve_plot roc_curve_plot &lt;- roc_curve_data %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + geom_abline() + coord_equal() + ggtitle(&quot;ROC Curve&quot;)+ theme_minimal() roc_curve_plot Nuestro árbol final se ve de la siguiente manera: library(rpart.plot) final_tree_model_cla %&gt;% extract_fit_engine() %&gt;% rpart.plot(roundint = FALSE) ## Warning: labs do not fit even at cex 0.15, there may be some overplotting Podemos obtener la impotancia de las variables: library(vip) final_tree_model_cla %&gt;% extract_fit_parsnip() %&gt;% vip() + ggtitle(&quot;Importancia de las variables&quot;) library(magrittr) results_cla %&lt;&gt;% mutate(truth = as_factor(truth)) 5.7 Bagging Primero tenemos que definir qué es la Agregación de Bootstrap o Bagging. Este es un aalgoritmo de aprendizaje automático diseñado para mejorar la estabilidad y precisión de algoritmos de ML usados en clasificación estadística y regresión. Además reduce la varianza y ayuda a evitar el sobreajuste. Aunque es usualmente aplicado a métodos de árboles de decisión, puede ser usado con cualquier tipo de método. Bagging es un caso especial del promediado de modelos. Los métodos de bagging son métodos donde los algoritmos simples son usados en paralelo. El principal objetivo de los métodos en paralelo es el de aprovecharse de la independencia que hay entre los algoritmos simples, ya que el error se puede reducir bastante al promediar las salidas de los modelos simples. Es como si, queriendo resolver un problema entre varias personas independientes unas de otras, damos por bueno lo que eligiese la mayoría de las personas. Para obtener la agregación de las salidas de cada modelo simple e independiente, bagging puede usar la votación para los métodos de clasificiación y el promedio para los métodos de regresión. 5.8 Random Forest Un bosque aleatorio es un algoritmo de aprendizaje automático supervisado que se construye a partir de algoritmos de árbol de decisión. Este algoritmo se aplica en diversas industrias, como la banca y el comercio electrónico, para predecir el comportamiento y los resultados. En esta clase se dará una descripción general del algoritmo de bosque aleatorio, cómo funciona y las características del algoritmo. También se señalan las ventajas y desventajas de este algoritmo. 5.8.1 ¿Qué es? Un bosque aleatorio es una técnica de aprendizaje automático que se utiliza para resolver problemas de regresión y clasificación. Utiliza el aprendizaje por conjuntos, que es una técnica que combina muchos clasificadores para proporcionar soluciones a problemas complejos. Este algoritmo consta de muchos árboles de decisión. El “bosque” generado se entrena mediante agregación de bootstrap (bagging), el cual es es un meta-algoritmo de conjunto que mejora la precisión de los algoritmos de aprendizaje automático. El algoritmo establece el resultado en función de las predicciones de los árboles de decisión. Predice tomando el promedio o la media de la salida de varios árboles. El aumento del número de árboles aumenta la precisión del resultado. Un bosque aleatorio erradica las limitaciones de un algoritmo de árbol de decisión. Reduce el sobreajuste de conjuntos de datos y aumenta la precisión. Genera predicciones sin requerir muchas configuraciones. 5.8.2 Características de los bosques aleatorios Es más preciso que el algoritmo árbol de decisiones. Proporciona una forma eficaz de gestionar los datos faltantes. Puede producir una predicción razonable sin ajuste de hiperparámetros. Resuelve el problema del sobreajuste en los árboles de decisión. En cada árbol forestal aleatorio, se selecciona aleatoriamente un subconjunto de características en el punto de división del nodo. 5.8.3 Aplicar árboles de decisión en un bosque aleatorio La principal diferencia entre el algoritmo de árbol de decisión y el algoritmo de bosque aleatorio es que el establecimiento de nodos raíz y la desagregación de nodos se realiza de forma aleatoria en este último. El bosque aleatorio emplea el método de bagging para generar la predicción requerida. El método bagging implica el uso de diferentes muestras de datos (datos de entrenamiento) en lugar de una sola muestra. Los árboles de decisión producen diferentes resultados, dependiendo de los datos de entrenamiento alimentados al algoritmo de bosque aleatorio. Nuestro primer ejemplo todavía se puede utilizar para explicar cómo funcionan los bosques aleatorios. Supongamos que solo tenemos cuatro árboles de decisión. En este caso, los datos de entrenamiento que comprenden las observaciones y características de estudio se dividirán en cuatro nodos raíz. Supongamos que queremos modelar si un cliente compra o no compra un teléfono. Los nodos raíz podrían representar cuatro características que podrían influir en la elección de un cliente (precio, almacenamiento interno, cámara y RAM). El bosque aleatorio dividirá los nodos seleccionando características al azar. La predicción final se seleccionará en función del resultado de los cuatro árboles. El resultado elegido por la mayoría de los árboles de decisión será la elección final. Si tres árboles predicen la compra y un árbol predice que no comprará, entonces la predicción final será la compra. En este caso, se prevé que el cliente comprará. El siguiente diagrama muestra un clasificador de bosque aleatorio simple. 5.8.4 Ventajas y desventjas de bosques aleatorios Ventajas Puede realizar tareas de regresión y clasificación. Un bosque aleatorio produce buenas predicciones que se pueden entender fácilmente. Puede manejar grandes conjuntos de datos de manera eficiente. Proporciona un mayor nivel de precisión en la predicción de resultados sobre el algoritmo del árbol de decisión. Desventajas Cuando se usa un bosque aleatorio, se requieren bastantes recursos para el cálculo. Consume más tiempo en comparación con un algoritmo de árbol de decisiones. No producen buenos resultados cuando los datos son muy escasos. En este caso, el subconjunto de características y la muestra de arranque producirán un espacio invariante. Esto conducirá a divisiones improductivas, que afectarán el resultado. 5.8.5 Bosques aleatorios para modelo de regresión Para implementar este algoritmo en R utilizaremos la función rand_forest de la paquetería parsnip, esta función define un modelo que crea una gran cantidad de árboles de decisión, cada uno independiente de los demás. La predicción final utiliza todas las predicciones de los árboles individuales y las combina. # Se declara el modelo de clasificación library(ranger) rforest_model &lt;- rand_forest( mode = &quot;regression&quot;, trees = 1000, mtry = tune(), min_n = tune()) %&gt;% set_engine(&quot;ranger&quot;, importance = &quot;impurity&quot;) # Se declara el flujo de trabajo rforest_workflow &lt;- workflow() %&gt;% add_model(rforest_model) %&gt;% add_recipe(receta_casas) # Declaración del grid rforest_param_grid &lt;- grid_regular( mtry(range = c(8,15)), min_n(range = c(2,16)), levels = c(13, 18) ) ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) library(doParallel) # Creación del cluster de trabajo UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) # V fold cross validation ames_folds &lt;- vfold_cv(ames_train, v = 10) # Ajuste de parámetros rft1 &lt;- Sys.time() rforest_tune_result &lt;- tune_grid( rforest_workflow, resamples = ames_folds, grid = rforest_param_grid, metrics = metric_set(rmse, rsq, mae), control = ctrl_grid ) rft2 &lt;- Sys.time(); rft2 - rft1 # Se detiene el cluster stopCluster(cluster) # Se guardan los resultados en formato RDS rforest_tune_result %&gt;% saveRDS(&quot;models/random_forest_model_reg.rds&quot;) rforest_tune_result &lt;- readRDS(&quot;models/random_forest_model_reg.rds&quot;) # R cuadrada rforest_tune_result %&gt;% autoplot(metric = &quot;rsq&quot;) + xlab(&#39;Número mínimo de elementos por nodo&#39;)+ ylab(&#39;R cuadrada&#39;)+ ggtitle(&#39;Gráfica R cuadrada&#39;)+ guides(color = guide_legend(title = &#39;Número de ramas&#39;))+ theme_minimal() # Resumen de métricas rforest_tune_result %&gt;% collect_metrics() %&gt;% group_by(.metric) %&gt;% summarise( mean_max = max(mean), mean_min = min(mean), mean_mean = mean(mean), mean_median = median(mean), .groups = &quot;drop&quot; ) ## # A tibble: 3 × 5 ## .metric mean_max mean_min mean_mean mean_median ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mae 180543. 180543. 180543. 180543. ## 2 rmse 197194. 197194. 197194. 197194. ## 3 rsq 0.777 0.766 0.772 0.772 # Mejores 10 resultados show_best(rforest_tune_result, n = 10, metric = &quot;rsq&quot;) %&gt;% as.data.frame() ## mtry min_n .metric .estimator mean n std_err .config ## 1 8 2 rsq standard 0.7773207 10 0.01428446 Preprocessor1_Model001 ## 2 8 3 rsq standard 0.7769947 10 0.01416865 Preprocessor1_Model009 ## 3 8 4 rsq standard 0.7769923 10 0.01438773 Preprocessor1_Model017 ## 4 8 7 rsq standard 0.7767768 10 0.01426232 Preprocessor1_Model041 ## 5 8 5 rsq standard 0.7767364 10 0.01448987 Preprocessor1_Model025 ## 6 8 6 rsq standard 0.7765816 10 0.01450118 Preprocessor1_Model033 ## 7 8 9 rsq standard 0.7762883 10 0.01439961 Preprocessor1_Model057 ## 8 9 2 rsq standard 0.7761141 10 0.01466429 Preprocessor1_Model002 ## 9 9 6 rsq standard 0.7759585 10 0.01439145 Preprocessor1_Model034 ## 10 8 8 rsq standard 0.7757190 10 0.01460913 Preprocessor1_Model049 # Selección del mejor modelo según la métrica r cuadrada best_rforest_model &lt;- select_best(rforest_tune_result, metric = &quot;rsq&quot;) # Selección del mejor modelo por un error estándar best_rforest_model_1se &lt;- select_by_one_std_err( rforest_tune_result, metric = &quot;rsq&quot;, &quot;rsq&quot;) # Selección del mejor modelo final_rforest_model &lt;- rforest_workflow %&gt;% #finalize_workflow(best_rforest_model) %&gt;% finalize_workflow(best_rforest_model_1se) %&gt;% fit(data = ames_train) # Predicciones results &lt;- predict(final_rforest_model, ames_test) %&gt;% dplyr::bind_cols(truth = ames_test$Sale_Price) %&gt;% dplyr::rename(pred_rforest_model = .pred, Sale_Price = truth) head(results, 10) ## # A tibble: 10 × 2 ## pred_rforest_model Sale_Price ## &lt;dbl&gt; &lt;int&gt; ## 1 11.8 105000 ## 2 12.1 185000 ## 3 12.1 180400 ## 4 11.5 141000 ## 5 12.2 210000 ## 6 12.2 216000 ## 7 12.0 149900 ## 8 11.6 105500 ## 9 11.6 88000 ## 10 11.9 146000 5.8.6 Bosques aleatorios para modelo de clasificación # Se declara el modelo de clasificación rforest_model &lt;- rand_forest( mode = &quot;classification&quot;, trees = 1000, mtry = tune(), min_n = tune()) %&gt;% set_engine(&quot;ranger&quot;, importance = &quot;impurity&quot;) # Se declara el flujo de trabajo rforest_workflow &lt;- workflow() %&gt;% add_model(rforest_model) %&gt;% add_recipe(telco_rec) # Declaración del grid rforest_param_grid &lt;- grid_regular( mtry(range = c(8,15)), min_n(range = c(2,16)), levels = c(6, 8) ) ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) library(doParallel) library(ranger) # Creación del cluster de trabajo UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) # Ajuste de parámetros rft1 &lt;- Sys.time() rforest_tune_result &lt;- tune_grid( rforest_workflow, resamples = telco_folds, grid = rforest_param_grid, metrics = metric_set(roc_auc, pr_auc, sens), control = ctrl_grid ) rft2 &lt;- Sys.time(); rft2 - rft1 # Se detiene el cluster stopCluster(cluster) # Se guardan los resultados en formato RDS rforest_tune_result %&gt;% saveRDS(&quot;models/random_forest_model.rds&quot;) rforest_tune_result &lt;- readRDS(&quot;models/random_forest_model.rds&quot;) # Curva de precisión rforest_tune_result %&gt;% unnest(.metrics) %&gt;% filter(.metric == &quot;pr_auc&quot;) %&gt;% ggplot(aes(x = mtry, y = .estimate)) + scale_x_log10() + geom_line(aes(color = id))+ ggtitle(&#39;Gráfica de precisión&#39;) + theme_minimal() rforest_tune_result %&gt;% autoplot(metric = &quot;pr_auc&quot;) + xlab(&#39;Número mínimo de elementos por nodo&#39;)+ ylab(&#39;Precisión&#39;)+ ggtitle(&#39;Gráfica de precisión por tamaño de nodos&#39;)+ guides(color = guide_legend(title = &#39;Número de ramas&#39;))+ theme_minimal() # Resumen de métricas rforest_tune_result %&gt;% collect_metrics() %&gt;% group_by(.metric) %&gt;% summarise( mean_max = max(mean), mean_min = min(mean), mean_mean = mean(mean), mean_median = median(mean), .groups = &quot;drop&quot; ) ## # A tibble: 3 × 5 ## .metric mean_max mean_min mean_mean mean_median ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pr_auc 0.914 0.906 0.910 0.910 ## 2 roc_auc 0.798 0.781 0.790 0.790 ## 3 sens 0.893 0.871 0.883 0.884 # Mejores 20 resultados show_best(rforest_tune_result, n = 20, metric = &quot;pr_auc&quot;) ## # A tibble: 20 × 8 ## mtry min_n .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 10 16 pr_auc binary 0.914 10 0.00330 Preprocessor1_Model45 ## 2 12 16 pr_auc binary 0.913 10 0.00341 Preprocessor1_Model46 ## 3 8 16 pr_auc binary 0.913 10 0.00360 Preprocessor1_Model43 ## 4 9 16 pr_auc binary 0.913 10 0.00343 Preprocessor1_Model44 ## 5 9 14 pr_auc binary 0.913 10 0.00340 Preprocessor1_Model38 ## 6 13 16 pr_auc binary 0.913 10 0.00330 Preprocessor1_Model47 ## 7 10 14 pr_auc binary 0.913 10 0.00338 Preprocessor1_Model39 ## 8 15 16 pr_auc binary 0.913 10 0.00341 Preprocessor1_Model48 ## 9 15 14 pr_auc binary 0.913 10 0.00337 Preprocessor1_Model42 ## 10 12 14 pr_auc binary 0.912 10 0.00341 Preprocessor1_Model40 ## 11 13 14 pr_auc binary 0.912 10 0.00332 Preprocessor1_Model41 ## 12 8 14 pr_auc binary 0.912 10 0.00346 Preprocessor1_Model37 ## 13 13 12 pr_auc binary 0.912 10 0.00338 Preprocessor1_Model35 ## 14 10 12 pr_auc binary 0.912 10 0.00329 Preprocessor1_Model33 ## 15 12 12 pr_auc binary 0.912 10 0.00324 Preprocessor1_Model34 ## 16 9 12 pr_auc binary 0.912 10 0.00332 Preprocessor1_Model32 ## 17 15 12 pr_auc binary 0.912 10 0.00336 Preprocessor1_Model36 ## 18 8 12 pr_auc binary 0.912 10 0.00347 Preprocessor1_Model31 ## 19 15 10 pr_auc binary 0.911 10 0.00340 Preprocessor1_Model30 ## 20 10 10 pr_auc binary 0.911 10 0.00346 Preprocessor1_Model27 # Selección del mejor modelo según la métrica de precisión best_rforest_model &lt;- select_best(rforest_tune_result, metric = &quot;pr_auc&quot;) # Selección del mejor modelo por un error estándar best_rforest_model_1se &lt;- select_by_one_std_err( rforest_tune_result, metric = &quot;pr_auc&quot;, &quot;pr_auc&quot;) # Selección del mejor modelo final_rforest_model &lt;- rforest_workflow %&gt;% finalize_workflow(best_rforest_model_1se) %&gt;% fit(data = telco_train) ## Warning: 8 columns were requested but there were 3 predictors in the data. 3 ## will be used. # Predicciones results_rforest_clas &lt;- predict(final_rforest_model, telco_test, type = &#39;prob&#39;) %&gt;% dplyr::bind_cols(truth =telco_test$Churn) %&gt;% mutate(truth = factor(truth, levels = c(&#39;No&#39;, &#39;Yes&#39;), labels = c(&#39;No&#39;, &#39;Yes&#39;))) head(results_rforest_clas, 10) ## # A tibble: 10 × 3 ## .pred_No .pred_Yes truth ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.943 0.0566 No ## 2 0.199 0.801 Yes ## 3 0.589 0.411 No ## 4 0.989 0.0107 No ## 5 0.970 0.0303 No ## 6 0.407 0.593 No ## 7 0.990 0.0102 No ## 8 0.774 0.226 No ## 9 0.484 0.516 Yes ## 10 1.00 0.000393 No results_cla_rforest &lt;-results_rforest_clas # Curvas pecision recall y ROC pr_curve_rforest_clas &lt;- pr_curve(results_rforest_clas, truth = truth, estimate = .pred_Yes, event_level = &#39;second&#39;) roc_curve_rforest_clas &lt;- roc_curve(results_rforest_clas, truth = truth, estimate = .pred_Yes, event_level = &#39;second&#39;) library(patchwork) pr_curve_plot &lt;- pr_curve_rforest_clas %&gt;% ggplot(aes(x = recall, y = precision)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + coord_equal() + ggtitle(&quot;Precision vs Recall&quot;)+ theme_minimal() roc_curve_plot &lt;- roc_curve_rforest_clas %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + geom_abline() + coord_equal() + ggtitle(&quot;ROC Curve&quot;)+ theme_minimal() pr_curve_plot + roc_curve_plot 5.8.7 Importancia de las variables Después de entrenar un modelo, es natural preguntarse qué variables tienen el mayor poder predictivo. Las variables de gran importancia son impulsoras del resultado y sus valores tienen un impacto significativo en los valores del resultado. Por el contrario, las variables con poca importancia pueden omitirse de un modelo, lo que lo hace más simple y rápido de ajustar y predecir. library(vip) final_rforest_model %&gt;% pull_workflow_fit() %&gt;% vip::vip() + ggtitle(&quot;Importancia de las variables&quot;)+ theme_minimal() ## Warning: `pull_workflow_fit()` was deprecated in workflows 0.2.3. ## Please use `extract_fit_parsnip()` instead. Existen dos medidas de importancia para cada variable en el bosque aleatorio: La primera medida se basa en cuánto disminuye la precisión cuando se excluye la variable. Primero, se mide la precisión de la predicción en la muestra fuera de la bolsa. Luego, los valores de la variable en la muestra fuera de la bolsa se mezclan aleatoriamente, manteniendo todas las demás variables iguales. Finalmente, se mide la disminución en la precisión de la predicción en los datos mezclados. Se reporta la disminución media de la precisión en todos los árboles. La segunda medida se basa en la disminución de la impureza de Gini cuando se elige una variable para dividir un nodo. Cuando se construye un árbol, la decisión sobre qué variable dividir en cada nodo utiliza un cálculo de la impureza de Gini. Para cada variable, la suma de la disminución de Gini en cada árbol del bosque se acumula cada vez que se elige esa variable para dividir un nodo. La suma se divide entre la cantidad de árboles en el bosque para obtener un promedio. La escala es irrelevante: solo importan los valores relativos. Ve este artículo para más información sobre la impureza de Gini. 5.9 Comparación de modelos Comparemos las curvas ROC y Precision-Recall de los modelos de clasificación que hemos creado # Curvas pecision recall y ROC pr_curve_plot &lt;- results_pr_curve %&gt;% ggplot(aes(x = recall, y = precision, color = ID)) + geom_path(size = 1) + coord_equal() + ggtitle(&quot;Precision vs Recall&quot;)+ theme_minimal() roc_curve_plot &lt;- results_roc_curve %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity, color = ID)) + geom_path(size = 1) + geom_abline() + coord_equal() + ggtitle(&quot;ROC Curve&quot;)+ theme_minimal() pr_curve_plot roc_curve_plot "],["anexos.html", "Capítulo 6 Anexos 6.1 Orden y estructura de proyecto 6.2 Referencias:", " Capítulo 6 Anexos 6.1 Orden y estructura de proyecto Resulta elemental contar con una adecuada estructura de carpetas que permitan al analista mantener orden y control a lo largo de todo el proyecto. Gran parte del caos en los problemas de analítica de datos nace desde el momento en que no se sabe en donde ubicar cada uno de los archivos necesarios para el proyecto. 6.1.1 Plantilla de estructura proyecto En esta sección, se presenta una introducción a la librería ProjectTemplate, la cual facilita una estructura predeterminada que ayudará como punto de partida para mantener orden y control en cada momento del proyecto. library(ProjectTemplate) ProjectTemplate::create.project(project.name = &#39;intro2dsml&#39;, rstudio.project = T) ProjectTemplate::create.project() creará toda la estructura de carpetas para un nuevo proyecto. Configurará todos los directorios relevantes y sus contenidos iniciales. Para aquellos que solo desean la funcionalidad mínima, el argumento de template se puede establecer en minimal para crear un subconjunto de directorios predeterminados de ProjectTemplate. cache: En esta carpeta se almacenarán los datos que desear cargarse automáticamente cuando se cargue la sesión del proyecto. config: Se realiza la configuración de R y su sesión, la cual será establecida cada que se abra el proyecto. data: Se almacenan las fuentes de información crudas necesarias en el proyecto. En caso de encontrarse codificadas en algún formato de archivo soportado por la librería, automáticamente serán cargadas a la sesión con la función load.project() diagnostics: En este folder puedes almacenar cualquier script usado para realizar diagnósticos sobre los datos. Es particularmente útil para al análisis de elementos corruptos o problemáticos dentro del conjunto de datos. doc: En este folder puede almacenarse cualquier documentación que haya escrito sobre el análisis. También se puede usar como directorio raíz para las páginas de GitHub para crear un sitio web de proyecto. graphs: Sirve para almacenar las gráficas producidas por el análisis lib: Aquí se almacenarán todos los archivos que proporcionen una funcionalidad útil para su trabajo, pero que no constituyan un análisis estadístico per se. Específicamente, debe usar el script lib/helpers.R para organizar cualquier función que use en su proyecto que no sea lo suficientemente general como para pertenecer a un paquete. Si tiene una configuración específica del proyecto que le gustaría almacenar en el objeto de configuración, puede especificarla en lib/globals.R. logs: Aquí puede almacenarse un archivo de registro de cualquier trabajo que haya realizado en este proyecto. Si va a registrar su trabajo, se recomienda utilizar el paquete log4r, que ProjectTemplate cargará automáticamente si activa la opción de configuración de registro. El nivel de registro se puede establecer a través de la configuración logging_level en la configuración, el valor predeterminado es “INFO.” munge: En este folder puede almacenarse cualquier código de preprocesamiento o manipulación de datos para el proyecto. Por ejemplo, si necesita agregar columnas en tiempo de ejecución, fusionar conjuntos de datos normalizados o censurar globalmente cualquier punto de datos, ese código debe almacenarse en el directorio munge. Los scripts de preprocesamiento almacenados en munge se ejecutarán en orden alfabético cuando se llame a la función load.project(), por lo que debe anteponerse números a los nombres de archivo para indicar su orden secuencial. profiling: Aquí puede almacenar cualquier script que use para comparar y cronometrar su código. reports: Aquí puede almacenar cualquier informe de salida, como versiones de tablas HTML o LaTeX, que produzca. Los documentos de sweave o brew también deben ir en el directorio de informes. src: Aquí se almacenarán los scripts de análisis estadístico finales. Debe agregar el siguiente fragmento de código al comienzo de cada secuencia de comandos de análisis: library('ProjectTemplate); load.project(). También debe hacer todo lo posible para asegurarse de que cualquier código compartido entre los análisis en src se mueva al directorio munge; si lo hace, puede ejecutar todos los análisis en el directorio src en paralelo. Una versión futura de ProjectTemplate proporcionará herramientas para ejecutar automáticamente cada análisis individual de src en paralelo. tests: Aquí puede almacenarse cualquier caso de prueba para las funciones que ha escrito. Los archivos de prueba deben usar pruebas de estilo testthat para que pueda llamar a la función test.project() para ejecutar automáticamente todo su código de prueba. README: En este archivo, debe escribir algunas notas para ayudar a orientar a los recién llegados a su proyecto. TODO: En este archivo, debe escribir una lista de futuras mejoras y correcciones de errores que planea realizar en sus análisis. Si algunas o todas estas carpetas resultan innecesarias, puede comenzarse con una versión simplificada a través del comando: create.project(project.name = &#39;intro2dsml&#39;, template=&#39;minimal&#39;) 6.1.2 Reproducibilidad Trabajar de esta manera permitirá que un proyecto sea reproducible por cualquier persona con el acceso y los permisos adecuados para colaborar. Sin importar que existan nuevas versiones de R o sus librerías, este ambiente virtual creado será reisiliente a tales cambios, permitiendo que el proyecto perdure a lo largo del tiempo. library(renv) renv::init() renv::init() inicializa un nuevo entorno local de proyecto con una biblioteca R privada. La función renv::init() intenta garantizar que la biblioteca del proyecto recién creada incluya todos los paquetes de R utilizados actualmente por el proyecto. Lo hace rastreando archivos R dentro del proyecto en busca de dependencias. Los paquetes descubiertos luego se instalan en la biblioteca del proyecto, que también intentará ahorrar tiempo copiando paquetes de la biblioteca del usuario (en lugar de reinstalarlos desde CRAN) según corresponda. Al usar renv, es posible “salvar” y “cargar” el estado de las librerías del proyecto a través de las siguientes funciones: renv::snapshot() guarda el estado del proyecto en el archivo renv.lock renv::restore() Restablece el estado del proyecto desde la última actualización de renv.lock. 6.2 Referencias: What is Exploratory Data Analysis? What is EDA? Diagrama BoxPlot Ggplot2: Elegant Graphics for Data Analysis Plotly "]]
