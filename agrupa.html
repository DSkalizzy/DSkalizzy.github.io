<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 7 MACHINE LEARNING (No Supervisado) | Taller de Introducción a Ciencia de Datos y Machine Learning</title>
  <meta name="description" content="AMAT Curso 1 : Introduccion a Ciencia de Datos" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 7 MACHINE LEARNING (No Supervisado) | Taller de Introducción a Ciencia de Datos y Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="AMAT Curso 1 : Introduccion a Ciencia de Datos" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 7 MACHINE LEARNING (No Supervisado) | Taller de Introducción a Ciencia de Datos y Machine Learning" />
  
  <meta name="twitter:description" content="AMAT Curso 1 : Introduccion a Ciencia de Datos" />
  

<meta name="author" content="Karina Lizette Gamboa Puente" />
<meta name="author" content="Oscar Arturo Bringas López" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="muestreo.html"/>
<link rel="next" href="otros.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><img src="img/conac-logo.png" width="280"></a></li|

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> BIENVENIDA</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#objetivo"><i class="fa fa-check"></i><b>1.1</b> Objetivo</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#quienes-somos"><i class="fa fa-check"></i><b>1.2</b> ¿Quienes somos?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> INTRODUCCIÓN</a>
<ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#qué-es-ciencia-de-datos"><i class="fa fa-check"></i><b>2.1</b> ¿Qué es Ciencia de Datos?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="intro.html"><a href="intro.html#definiendo-conceptos"><i class="fa fa-check"></i><b>2.1.1</b> Definiendo conceptos:</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#objetivo-de-la-ciencia-se-datos"><i class="fa fa-check"></i><b>2.2</b> Objetivo de la Ciencia se Datos</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#qué-se-requiere-para-hacer-ciencia-de-datos"><i class="fa fa-check"></i><b>2.3</b> ¿Qué se requiere para hacer Ciencia de Datos?</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#tipos-de-problemas-que-se-pueden-resolver-con-ciencia-de-datos"><i class="fa fa-check"></i><b>2.4</b> Tipos de problemas que se pueden resolver con Ciencia de Datos</a></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#tipos-de-aprendizaje"><i class="fa fa-check"></i><b>2.5</b> Tipos de aprendizaje</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="intro.html"><a href="intro.html#aprendizaje-supervisado"><i class="fa fa-check"></i><b>2.5.1</b> Aprendizaje supervisado</a></li>
<li class="chapter" data-level="2.5.2" data-path="intro.html"><a href="intro.html#aprendizaje-no-supervisado"><i class="fa fa-check"></i><b>2.5.2</b> Aprendizaje no supervisado</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ciclo.html"><a href="ciclo.html"><i class="fa fa-check"></i><b>3</b> CICLO DE VIDA</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ciclo.html"><a href="ciclo.html#ciclo-de-un-proyecto-de-ciencia-de-datos"><i class="fa fa-check"></i><b>3.1</b> Ciclo de un proyecto de Ciencia de Datos</a></li>
<li class="chapter" data-level="3.2" data-path="ciclo.html"><a href="ciclo.html#data-science-scoping"><i class="fa fa-check"></i><b>3.2</b> Data Science <em>scoping</em></a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ciclo.html"><a href="ciclo.html#data-maturity-framework"><i class="fa fa-check"></i><b>3.2.1</b> Data Maturity Framework</a></li>
<li class="chapter" data-level="3.2.2" data-path="ciclo.html"><a href="ciclo.html#scoping"><i class="fa fa-check"></i><b>3.2.2</b> Scoping</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="explora.html"><a href="explora.html"><i class="fa fa-check"></i><b>4</b> ANÁLISIS EXPLORATORIO</a>
<ul>
<li class="chapter" data-level="4.1" data-path="explora.html"><a href="explora.html#análisis-exploratorio-de-datos-eda"><i class="fa fa-check"></i><b>4.1</b> Análisis Exploratorio de Datos (EDA)</a></li>
<li class="chapter" data-level="4.2" data-path="explora.html"><a href="explora.html#eda-análisis-exploratorio-de-datos"><i class="fa fa-check"></i><b>4.2</b> EDA: Análisis Exploratorio de Datos</a></li>
<li class="chapter" data-level="4.3" data-path="explora.html"><a href="explora.html#geda-análisis-exploratorio-de-datos-gráficos"><i class="fa fa-check"></i><b>4.3</b> GEDA: Análisis Exploratorio de Datos Gráficos</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="explora.html"><a href="explora.html#lo-que-no-se-debe-hacer"><i class="fa fa-check"></i><b>4.3.1</b> Lo que no se debe hacer…</a></li>
<li class="chapter" data-level="4.3.2" data-path="explora.html"><a href="explora.html#principios-de-visualización"><i class="fa fa-check"></i><b>4.3.2</b> Principios de visualización</a></li>
<li class="chapter" data-level="4.3.3" data-path="explora.html"><a href="explora.html#principios-generales-del-diseño-analítico"><i class="fa fa-check"></i><b>4.3.3</b> Principios generales del diseño analítico:</a></li>
<li class="chapter" data-level="4.3.4" data-path="explora.html"><a href="explora.html#técnicas-de-visualización"><i class="fa fa-check"></i><b>4.3.4</b> Técnicas de visualización:</a></li>
<li class="chapter" data-level="4.3.5" data-path="explora.html"><a href="explora.html#indicadores-de-calidad-gráfica"><i class="fa fa-check"></i><b>4.3.5</b> Indicadores de calidad gráfica:</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="explora.html"><a href="explora.html#uso-decisión-e-implementación-de-técnicas-gráficas."><i class="fa fa-check"></i><b>4.4</b> Uso, decisión e implementación de técnicas gráficas.</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="explora.html"><a href="explora.html#gráficos-univariados"><i class="fa fa-check"></i><b>4.4.1</b> Gráficos univariados:</a></li>
<li class="chapter" data-level="4.4.2" data-path="explora.html"><a href="explora.html#gráficos-multivariados"><i class="fa fa-check"></i><b>4.4.2</b> Gráficos multivariados</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="explora.html"><a href="explora.html#análisis-y-visualización-con-r"><i class="fa fa-check"></i><b>4.5</b> Análisis y Visualización con R</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="explora.html"><a href="explora.html#análisis-estadístico"><i class="fa fa-check"></i><b>4.5.1</b> Análisis estadístico</a></li>
<li class="chapter" data-level="4.5.2" data-path="explora.html"><a href="explora.html#análisis-gráfico-de-datos"><i class="fa fa-check"></i><b>4.5.2</b> Análisis Gráfico de datos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="modela.html"><a href="modela.html"><i class="fa fa-check"></i><b>5</b> MACHINE LEARNING (Supervisado)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="modela.html"><a href="modela.html#ml-y-algoritmos"><i class="fa fa-check"></i><b>5.1</b> ML y Algoritmos</a></li>
<li class="chapter" data-level="5.2" data-path="modela.html"><a href="modela.html#sesgo-vs-varianza"><i class="fa fa-check"></i><b>5.2</b> Sesgo vs varianza</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="modela.html"><a href="modela.html#balance-entre-sesgo-y-varianza-o-trade-off"><i class="fa fa-check"></i><b>5.2.1</b> Balance entre sesgo y varianza o Trade-off</a></li>
<li class="chapter" data-level="5.2.2" data-path="modela.html"><a href="modela.html#overfitting"><i class="fa fa-check"></i><b>5.2.2</b> Overfitting</a></li>
<li class="chapter" data-level="5.2.3" data-path="modela.html"><a href="modela.html#underfitting"><i class="fa fa-check"></i><b>5.2.3</b> Underfitting</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="modela.html"><a href="modela.html#estimación-de-errores"><i class="fa fa-check"></i><b>5.3</b> Estimación de errores</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="modela.html"><a href="modela.html#errores-reducibles"><i class="fa fa-check"></i><b>5.3.1</b> Errores reducibles</a></li>
<li class="chapter" data-level="5.3.2" data-path="modela.html"><a href="modela.html#error-irreducible"><i class="fa fa-check"></i><b>5.3.2</b> Error irreducible</a></li>
<li class="chapter" data-level="5.3.3" data-path="modela.html"><a href="modela.html#error-total"><i class="fa fa-check"></i><b>5.3.3</b> Error total</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="modela.html"><a href="modela.html#partición-de-datos"><i class="fa fa-check"></i><b>5.4</b> Partición de datos</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="modela.html"><a href="modela.html#métodos-comunes-para-particionar-datos"><i class="fa fa-check"></i><b>5.4.1</b> Métodos comunes para particionar datos</a></li>
<li class="chapter" data-level="5.4.2" data-path="modela.html"><a href="modela.html#qué-proporción-debería-ser-usada"><i class="fa fa-check"></i><b>5.4.2</b> ¿Qué proporción debería ser usada?</a></li>
<li class="chapter" data-level="5.4.3" data-path="modela.html"><a href="modela.html#conjunto-de-validación"><i class="fa fa-check"></i><b>5.4.3</b> Conjunto de validación</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="modela.html"><a href="modela.html#métricas-de-desempeño-y-estimación-de-errores"><i class="fa fa-check"></i><b>5.5</b> Métricas de desempeño y estimación de errores</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="modela.html"><a href="modela.html#métricas-de-desempeño-para-clasificación"><i class="fa fa-check"></i><b>5.5.1</b> Métricas de desempeño para clasificación</a></li>
<li class="chapter" data-level="5.5.2" data-path="modela.html"><a href="modela.html#implementación-con-r"><i class="fa fa-check"></i><b>5.5.2</b> Implementación con R</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="modela.html"><a href="modela.html#algotitmos-de-machine-learning"><i class="fa fa-check"></i><b>5.6</b> Algotitmos de machine learning</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="modela.html"><a href="modela.html#regresión-logística"><i class="fa fa-check"></i><b>5.6.1</b> Regresión logística</a></li>
<li class="chapter" data-level="5.6.2" data-path="modela.html"><a href="modela.html#knn-k-nearest-neighbor"><i class="fa fa-check"></i><b>5.6.2</b> KNN: K-Nearest-Neighbor</a></li>
<li class="chapter" data-level="5.6.3" data-path="modela.html"><a href="modela.html#árboles-de-decisión-decision-trees"><i class="fa fa-check"></i><b>5.6.3</b> Árboles de decisión (Decision trees)</a></li>
<li class="chapter" data-level="5.6.4" data-path="modela.html"><a href="modela.html#bagging"><i class="fa fa-check"></i><b>5.6.4</b> Bagging</a></li>
<li class="chapter" data-level="5.6.5" data-path="modela.html"><a href="modela.html#random-forest"><i class="fa fa-check"></i><b>5.6.5</b> Random Forest</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="muestreo.html"><a href="muestreo.html"><i class="fa fa-check"></i><b>6</b> DISEÑO DE EXPERIMENTOS</a>
<ul>
<li class="chapter" data-level="6.1" data-path="muestreo.html"><a href="muestreo.html#diseño-de-experimentos-para-machine-learning"><i class="fa fa-check"></i><b>6.1</b> Diseño de experimentos para Machine Learning</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="muestreo.html"><a href="muestreo.html#por-qué-hacer-un-experimento"><i class="fa fa-check"></i><b>6.1.1</b> ¿Por qué hacer un experimento?</a></li>
<li class="chapter" data-level="6.1.2" data-path="muestreo.html"><a href="muestreo.html#ab-testing"><i class="fa fa-check"></i><b>6.1.2</b> A/B Testing</a></li>
<li class="chapter" data-level="6.1.3" data-path="muestreo.html"><a href="muestreo.html#qué-es-y-para-qué-sirve-muestreo"><i class="fa fa-check"></i><b>6.1.3</b> ¿Qué es y para qué sirve muestreo?</a></li>
<li class="chapter" data-level="6.1.4" data-path="muestreo.html"><a href="muestreo.html#ventajas-del-muestreo-en-el-mundo-corporativo"><i class="fa fa-check"></i><b>6.1.4</b> Ventajas del muestreo en el mundo corporativo</a></li>
<li class="chapter" data-level="6.1.5" data-path="muestreo.html"><a href="muestreo.html#qué-se-requiere-para-formular-un-problema-de-muestreo"><i class="fa fa-check"></i><b>6.1.5</b> ¿Qué se requiere para formular un problema de muestreo?</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="muestreo.html"><a href="muestreo.html#muestreo-probabilístico"><i class="fa fa-check"></i><b>6.2</b> Muestreo probabilístico</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="agrupa.html"><a href="agrupa.html"><i class="fa fa-check"></i><b>7</b> MACHINE LEARNING (No Supervisado)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="agrupa.html"><a href="agrupa.html#clustering"><i class="fa fa-check"></i><b>7.1</b> Clustering</a></li>
<li class="chapter" data-level="7.2" data-path="agrupa.html"><a href="agrupa.html#k---means"><i class="fa fa-check"></i><b>7.2</b> K - means</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="agrupa.html"><a href="agrupa.html#ajuste-de-modelo-cómo-funciona-el-algortimo"><i class="fa fa-check"></i><b>7.2.1</b> Ajuste de modelo: ¿Cómo funciona el algortimo?</a></li>
<li class="chapter" data-level="7.2.2" data-path="agrupa.html"><a href="agrupa.html#implementación-en-r"><i class="fa fa-check"></i><b>7.2.2</b> Implementación en R</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="agrupa.html"><a href="agrupa.html#partitioning-around-medoids-pam"><i class="fa fa-check"></i><b>7.3</b> Partitioning Around Medoids (PAM)</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="agrupa.html"><a href="agrupa.html#implementación-en-r-1"><i class="fa fa-check"></i><b>7.3.1</b> Implementación en R</a></li>
<li class="chapter" data-level="7.3.2" data-path="agrupa.html"><a href="agrupa.html#algoritmo-pam"><i class="fa fa-check"></i><b>7.3.2</b> Algoritmo PAM</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="agrupa.html"><a href="agrupa.html#dbscan"><i class="fa fa-check"></i><b>7.4</b> DBSCAN</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="agrupa.html"><a href="agrupa.html#algoritmo"><i class="fa fa-check"></i><b>7.4.1</b> Algoritmo</a></li>
<li class="chapter" data-level="7.4.2" data-path="agrupa.html"><a href="agrupa.html#implementación-en-r-2"><i class="fa fa-check"></i><b>7.4.2</b> Implementación en R</a></li>
<li class="chapter" data-level="7.4.3" data-path="agrupa.html"><a href="agrupa.html#ventajas-de-dbscan"><i class="fa fa-check"></i><b>7.4.3</b> Ventajas de DBSCAN</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="otros.html"><a href="otros.html"><i class="fa fa-check"></i><b>8</b> OTRAS HERRAMIENTAS Y TECNOLOGÍAS</a>
<ul>
<li class="chapter" data-level="8.1" data-path="otros.html"><a href="otros.html#git-y-github"><i class="fa fa-check"></i><b>8.1</b> Git y Github</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="otros.html"><a href="otros.html#qué-es-git"><i class="fa fa-check"></i><b>8.1.1</b> <strong>¿Qué es Git?</strong></a></li>
<li class="chapter" data-level="8.1.2" data-path="otros.html"><a href="otros.html#qué-es-github"><i class="fa fa-check"></i><b>8.1.2</b> <strong>¿Qué es Github?</strong></a></li>
<li class="chapter" data-level="8.1.3" data-path="otros.html"><a href="otros.html#cómo-utilizarlos"><i class="fa fa-check"></i><b>8.1.3</b> <strong>¿Cómo utilizarlos?</strong></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="despedida.html"><a href="despedida.html"><i class="fa fa-check"></i><b>9</b> DESPEDIDA</a></li>
<li class="divider"></li>
<li><a href="./"><img src="img/amat-logo.png" width="280"></a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Taller de Introducción a Ciencia de Datos y Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="agrupa" class="section level1" number="7">
<h1><span class="header-section-number">Capítulo 7</span> MACHINE LEARNING (No Supervisado)</h1>
<div class="watermark">
<img src="img/header.png" width="400">
</div>
<div id="clustering" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Clustering</h2>
<p>Como ya habiamos mencionado, en el aprendizaje supervisado la idea principal es aprender bajo <strong>supervisión</strong>, donde la señal de supervisión se nombra como valor objetivo o etiqueta. En el aprendizaje no supervisado, carecemos de este tipo de etiqueta. Por lo tanto, necesitamos encontrar nuestro camino sin ninguna supervisión ni guía. Esto simplemente significa que necesitamos descubrir qué es qué por nosotros mismos.</p>
<p><img src="img/ml-ns/3-1-1-tipos_de_aprendizaje.png" width="750pt" height="350pt" style="display: block; margin: auto;" /></p>
<p><strong>Aplicaciones de agrupación en clusters</strong></p>
<ul>
<li><strong>Segmentación de clientes</strong></li>
</ul>
<p>Una de las aplicaciones más comunes de la agrupación en <em>clusters</em> es la segmentación de clientes, Esta estrategia abarca todas las sectores, incluidas las telecomunicaciones, el comercio electrónico, los deportes, la publicidad, las ventas, etc.</p>
<ul>
<li><strong>Agrupación de documentos</strong></li>
</ul>
<p>Esta es otra aplicación común de la agrupación. Supongamos que tiene varios documentos y necesita agrupar documentos similares. La agrupación en clústeres nos ayuda a agrupar estos documentos de manera que documentos similares estén en los mismos grupos.</p>
<ul>
<li><strong>Segmentación de imagen</strong></li>
</ul>
<p>También podemos utilizar la agrupación en <em>clusters</em> para realizar la segmentación de imágenes. Aquí, intentamos agrupar píxeles similares en la imagen.</p>
<ul>
<li><strong>Motores de recomendación</strong></li>
</ul>
<p>También se puede utilizar en motores de recomendación. Supongamos que desea recomendar canciones a sus amigos. Puede ver las canciones que le gustaron a esa persona y luego usar la agrupación para encontrar canciones similares.</p>
<ul>
<li><strong>Reducción de dimensiones mediante agrupamiento de variables similares</strong></li>
</ul>
<p>Podemos usar los grupos de K-means para agrupar variables similares y crear un indice de estas variables para entrenar un modelo supervisado sin necesidad de usar todas las variables de un solo grupo.</p>
</div>
<div id="k---means" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> K - means</h2>
<p>La agrupación en grupos con <em>K-means</em> es uno de los algoritmos de aprendizaje de máquina no supervisados más simples y populares.</p>
<p>K-medias es un método de <strong>agrupamiento</strong>, que tiene como objetivo la partición de un conjunto de n observaciones en k grupos en el que <strong>cada observación pertenece al grupo cuyo valor medio es más cercano</strong>.</p>
<p>Un <em>cluster</em> se refiere a una colección de puntos de datos agregadosa a un grupo debido a ciertas similitudes.</p>
<p><img src="img/ml-ns/3-12-1-kmeans.jpeg" width="500pt" height="300pt" style="display: block; margin: auto;" /></p>
<div id="ajuste-de-modelo-cómo-funciona-el-algortimo" class="section level3" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Ajuste de modelo: ¿Cómo funciona el algortimo?</h3>
<ul>
<li><strong>Paso 1:</strong> Seleccionar el número de <em>clusters</em> K</li>
</ul>
<p>El primer paso en <em>k-means</em> es elegir el número de conglomerados, K. Como estamos en un problema de análisis no supervisado, no hay K correcto, existen métodos para seleccionar algún K pero no hay respuesta correcta.</p>
<ul>
<li><strong>Paso 2:</strong> Seleccionar K puntos aleatorios de los datos como centroides.</li>
</ul>
<p>A continuación, seleccionamos aleatoriamente el centroide para cada grupo. Supongamos que queremos tener 2 grupos, por lo que K es igual a 2, seleccionamos aleatoriamente los centroides:</p>
<p><img src="img/ml-ns/3-12-1-paso2.png" width="350pt" height="300pt" style="display: block; margin: auto;" /></p>
<ul>
<li><strong>Paso 3:</strong> Asignamos todos los puntos al centroide del clúster más cercano.</li>
</ul>
<p>Una vez que hemos inicializado los centroides, asignamos cada punto al centroide del clúster más cercano:</p>
<p><img src="img/ml-ns/3-12-1-paso3.png" width="350pt" height="300pt" style="display: block; margin: auto;" /></p>
<ul>
<li><strong>Paso 4:</strong> Volvemos a calcular los centroides de los <em>clusters</em> recién formados.</li>
</ul>
<p>Ahora, una vez que hayamos asignado todos los puntos a cualquiera de los grupos, el siguiente paso es calcular los centroides de los grupos recién formados:</p>
<p><img src="img/ml-ns/3-12-1-paso4.png" width="350pt" height="300pt" style="display: block; margin: auto;" /></p>
<ul>
<li><strong>Paso 5:</strong> Repetir los pasos 3 y 4.</li>
</ul>
<p><img src="img/ml-ns/3-12-1-paso5.png" width="350pt" height="300pt" style="display: block; margin: auto;" /></p>
<ul>
<li><strong>Resumen de algoritmo</strong></li>
</ul>
<p><img src="img/ml-ns/kmeans%20step.png" width="500pt" height="500pt" style="display: block; margin: auto;" /></p>
<ul>
<li><strong>Criterios de paro:</strong></li>
</ul>
<p>Existen tres criterios de paro para detener el algoritmo:</p>
<ol style="list-style-type: decimal">
<li>Los centroides de los grupos recién formados no cambian:</li>
</ol>
<p>Podemos detener el algoritmo si <strong>los centroides no cambian</strong>. Incluso después de múltiples iteraciones, si obtenemos los mismos centroides para todos los clústeres, podemos decir que el algoritmo no está aprendiendo ningún patrón nuevo y es una señal para detener el entrenamiento.</p>
<ol start="2" style="list-style-type: decimal">
<li>Los puntos permanecen en el mismo grupo:</li>
</ol>
<p>Otra señal clara de que debemos detener el proceso de entrenamiento si <strong>los puntos permanecen en el mismo clúster</strong> incluso después de entrenar el algoritmo para múltiples iteraciones.</p>
<ol start="3" style="list-style-type: decimal">
<li>Se alcanza el número máximo de iteraciones:</li>
</ol>
<p>Finalmente, podemos detener el entrenamiento si se alcanza el número <strong>máximo de iteraciones</strong>. Supongamos que hemos establecido el número de iteraciones en 100. El proceso se repetirá durante 100 iteraciones antes de detenerse.</p>
</div>
<div id="implementación-en-r" class="section level3" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Implementación en R</h3>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="agrupa.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb15-2"><a href="agrupa.html#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb15-3"><a href="agrupa.html#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(factoextra)</span>
<span id="cb15-4"><a href="agrupa.html#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb15-5"><a href="agrupa.html#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb15-6"><a href="agrupa.html#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="agrupa.html#cb15-7" aria-hidden="true" tabindex="-1"></a>Churn <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;data/Churn.csv&quot;</span>)</span>
<span id="cb15-8"><a href="agrupa.html#cb15-8" aria-hidden="true" tabindex="-1"></a>Churn</span></code></pre></div>
<pre><code>## # A tibble: 7,043 × 21
##    customerID gender SeniorCitizen Partner Dependents tenure PhoneService
##    &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;       
##  1 7590-VHVEG Female             0 Yes     No              1 No          
##  2 5575-GNVDE Male               0 No      No             34 Yes         
##  3 3668-QPYBK Male               0 No      No              2 Yes         
##  4 7795-CFOCW Male               0 No      No             45 No          
##  5 9237-HQITU Female             0 No      No              2 Yes         
##  6 9305-CDSKC Female             0 No      No              8 Yes         
##  7 1452-KIOVK Male               0 No      Yes            22 Yes         
##  8 6713-OKOMC Female             0 No      No             10 No          
##  9 7892-POOKP Female             0 Yes     No             28 Yes         
## 10 6388-TABGU Male               0 No      Yes            62 Yes         
## # … with 7,033 more rows, and 14 more variables: MultipleLines &lt;chr&gt;,
## #   InternetService &lt;chr&gt;, OnlineSecurity &lt;chr&gt;, OnlineBackup &lt;chr&gt;,
## #   DeviceProtection &lt;chr&gt;, TechSupport &lt;chr&gt;, StreamingTV &lt;chr&gt;,
## #   StreamingMovies &lt;chr&gt;, Contract &lt;chr&gt;, PaperlessBilling &lt;chr&gt;,
## #   PaymentMethod &lt;chr&gt;, MonthlyCharges &lt;dbl&gt;, TotalCharges &lt;dbl&gt;, Churn &lt;chr&gt;</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="agrupa.html#cb17-1" aria-hidden="true" tabindex="-1"></a>Churn <span class="ot">&lt;-</span> Churn <span class="sc">%&gt;%</span> </span>
<span id="cb17-2"><a href="agrupa.html#cb17-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">tenure_scale =</span> <span class="fu">scale</span>(tenure, <span class="at">center =</span> T, <span class="at">scale =</span> T) ,</span>
<span id="cb17-3"><a href="agrupa.html#cb17-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">MonthlyCharges_scale =</span> <span class="fu">scale</span>(MonthlyCharges, <span class="at">center =</span> T, <span class="at">scale =</span> T) )</span>
<span id="cb17-4"><a href="agrupa.html#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="agrupa.html#cb17-5" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> Churn <span class="sc">%&gt;%</span> </span>
<span id="cb17-6"><a href="agrupa.html#cb17-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(tenure_scale, MonthlyCharges_scale) <span class="sc">%&gt;%</span> </span>
<span id="cb17-7"><a href="agrupa.html#cb17-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">na.omit</span>()</span>
<span id="cb17-8"><a href="agrupa.html#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="agrupa.html#cb17-9" aria-hidden="true" tabindex="-1"></a>km6 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(df, <span class="at">centers =</span> <span class="dv">6</span>, <span class="at">nstart =</span> <span class="dv">5</span>)</span>
<span id="cb17-10"><a href="agrupa.html#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="agrupa.html#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_cluster</span>(km6, <span class="at">data =</span> df, <span class="at">repel =</span> F)</span></code></pre></div>
<p><img src="Taller-de--Introducci%C3%B3n-a-Ciencia-de-Datos-y-Machine-Learning-_files/figure-html/unnamed-chunk-108-1.png" width="672" /></p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="agrupa.html#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb18-2"><a href="agrupa.html#cb18-2" aria-hidden="true" tabindex="-1"></a>wss_plot <span class="ot">&lt;-</span> <span class="fu">fviz_nbclust</span>(df, kmeans, <span class="at">method =</span> <span class="st">&quot;wss&quot;</span>)</span>
<span id="cb18-3"><a href="agrupa.html#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="agrupa.html#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb18-5"><a href="agrupa.html#cb18-5" aria-hidden="true" tabindex="-1"></a>sil_plot <span class="ot">&lt;-</span> <span class="fu">fviz_nbclust</span>(df, kmeans, <span class="at">method =</span> <span class="st">&quot;silhouette&quot;</span>)</span>
<span id="cb18-6"><a href="agrupa.html#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="agrupa.html#cb18-7" aria-hidden="true" tabindex="-1"></a>wss_plot <span class="sc">+</span> sil_plot</span></code></pre></div>
<p><img src="Taller-de--Introducci%C3%B3n-a-Ciencia-de-Datos-y-Machine-Learning-_files/figure-html/unnamed-chunk-108-2.png" width="672" /></p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="agrupa.html#cb19-1" aria-hidden="true" tabindex="-1"></a>Churn <span class="ot">&lt;-</span> Churn <span class="sc">%&gt;%</span> <span class="fu">bind_cols</span>(km6[<span class="dv">1</span>])</span>
<span id="cb19-2"><a href="agrupa.html#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="agrupa.html#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(Churn)</span></code></pre></div>
<pre><code>## Rows: 7,043
## Columns: 24
## $ customerID           &lt;chr&gt; &quot;7590-VHVEG&quot;, &quot;5575-GNVDE&quot;, &quot;3668-QPYBK&quot;, &quot;7795-C…
## $ gender               &lt;chr&gt; &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Fema…
## $ SeniorCitizen        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ Partner              &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, …
## $ Dependents           &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, …
## $ tenure               &lt;dbl&gt; 1, 34, 2, 45, 2, 8, 22, 10, 28, 62, 13, 16, 58, 4…
## $ PhoneService         &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;N…
## $ MultipleLines        &lt;chr&gt; &quot;No phone service&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No phone service…
## $ InternetService      &lt;chr&gt; &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;Fiber optic&quot;, &quot;Fiber…
## $ OnlineSecurity       &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes…
## $ OnlineBackup         &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;…
## $ DeviceProtection     &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;…
## $ TechSupport          &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, …
## $ StreamingTV          &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;,…
## $ StreamingMovies      &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, …
## $ Contract             &lt;chr&gt; &quot;Month-to-month&quot;, &quot;One year&quot;, &quot;Month-to-month&quot;, &quot;…
## $ PaperlessBilling     &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;N…
## $ PaymentMethod        &lt;chr&gt; &quot;Electronic check&quot;, &quot;Mailed check&quot;, &quot;Mailed check…
## $ MonthlyCharges       &lt;dbl&gt; 29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89.10, …
## $ TotalCharges         &lt;dbl&gt; 29.85, 1889.50, 108.15, 1840.75, 151.65, 820.50, …
## $ Churn                &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;…
## $ tenure_scale         &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ MonthlyCharges_scale &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ cluster              &lt;int&gt; 2, 6, 4, 3, 4, 5, 5, 2, 5, 6, 4, 2, 1, 1, 5, …</code></pre>
</div>
</div>
<div id="partitioning-around-medoids-pam" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Partitioning Around Medoids (PAM)</h2>
<p>El algoritmo <em>k-medoides</em> es un enfoque de agrupamiento relacionado con el agrupamiento de <em>k-medias</em> para particionar un conjunto de datos en <em>k</em> grupos o clústeres. En <em>k-medoides</em>, cada grupo está representado por uno de los puntos de datos pertenecientes a un grupo. Estos puntos son nombrados <strong>medoides</strong>.</p>
<p>El término <strong>medoide</strong> se refiere a un objeto dentro de un grupo para el cual la disimilitud promedio entre él y todos los demás miembros del clúster son mínimos. Corresponde a el punto más céntrico del grupo. Estos objetos (uno por grupo) pueden ser considerado como un ejemplo representativo de los miembros de ese grupo que puede ser útil en algunas situaciones.</p>
<p>Este algotimo es una alternativa sólida de <em>k-medias</em>. Debido a que este
algoritmo es menos sensible al ruido y los valores atípicos, en comparación con <em>k-medias</em>, pues usa medoides como centros de conglomerados en lugar de medias. El uso de medias implica que la agrupación de <em>k-medias</em> es muy sensible a los valores atípicos, lo cual
puede afectar gravemente la asignación de observaciones a los conglomerados.</p>
<p><img src="img/ml-ns/kmeans-kmedoids.png" width="500pt" height="250pt" style="display: block; margin: auto;" /></p>
<p>El método de agrupamiento de <em>k-medoides</em> más común es el algoritmo <em>PAM (Partitioning Around Medoids, Kaufman &amp; Rousseeuw, 1990)</em>.</p>
<div id="implementación-en-r-1" class="section level3" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> Implementación en R</h3>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="agrupa.html#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cluster)</span>
<span id="cb21-2"><a href="agrupa.html#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="agrupa.html#cb21-3" aria-hidden="true" tabindex="-1"></a>k_mediods <span class="ot">&lt;-</span> <span class="fu">pam</span>(df, <span class="dv">6</span>)</span>
<span id="cb21-4"><a href="agrupa.html#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="agrupa.html#cb21-5" aria-hidden="true" tabindex="-1"></a>pam_plot <span class="ot">&lt;-</span> <span class="fu">fviz_cluster</span>(</span>
<span id="cb21-6"><a href="agrupa.html#cb21-6" aria-hidden="true" tabindex="-1"></a>  k_mediods,</span>
<span id="cb21-7"><a href="agrupa.html#cb21-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">nstart =</span> <span class="dv">5</span>,</span>
<span id="cb21-8"><a href="agrupa.html#cb21-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">ggtheme =</span> <span class="fu">theme_minimal</span>()) <span class="sc">+</span></span>
<span id="cb21-9"><a href="agrupa.html#cb21-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&#39;K-Medoids Plot&#39;</span>) <span class="sc">+</span></span>
<span id="cb21-10"><a href="agrupa.html#cb21-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)</span>
<span id="cb21-11"><a href="agrupa.html#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="agrupa.html#cb21-12" aria-hidden="true" tabindex="-1"></a>pam_plot</span></code></pre></div>
<p><img src="Taller-de--Introducci%C3%B3n-a-Ciencia-de-Datos-y-Machine-Learning-_files/figure-html/unnamed-chunk-110-1.png" width="672" /></p>
</div>
<div id="algoritmo-pam" class="section level3" number="7.3.2">
<h3><span class="header-section-number">7.3.2</span> Algoritmo PAM</h3>
<p>El algoritmo <em>PAM</em> se basa en la búsqueda de <em>k</em> objetos representativos o medoides entre las observaciones del conjunto de datos.</p>
<p>Después de encontrar un conjunto de <em>k</em> medoides, los grupos se construyen asignando cada observación al medoide más cercano.</p>
<p>Posteriormente, cada medoide <em>m</em> y cada punto de datos no medoide seleccionado se intercambian y se calcula la función objetivo.</p>
<p><strong>La función objetivo corresponde a la suma de las disimilitudes de todos los objetos a su medoide más cercano.</strong></p>
<p>El paso de intercambio intenta mejorar la calidad de la agrupación mediante el intercambio entre objetos seleccionados (medoides) y objetos no seleccionados. Si la función objetivo puede reducirse intercambiando un objeto seleccionado con un objeto no seleccionado, entonces el se realiza el intercambio. Esto se continúa hasta que la función objetivo ya no puede ser disminuida. El objetivo es encontrar <em>k</em> objetos representativos que minimicen la suma de disimilitudes de las observaciones con su objeto representativo más cercano.</p>
<p>Como se mencionó anteriormente, el algoritmo <em>PAM</em> funciona con una matriz de disimilitud y para calcular esta matriz, el algoritmo puede utilizar dos métricas:</p>
<ul>
<li><p>La distancia <em>euclideana</em>, que es la raíz de la suma de cuadrados de las diferencias;</p></li>
<li><p>Y la distancia de <em>Manhattan</em>, que es la suma de distancias absolutas.</p></li>
</ul>
<p><img src="img/ml-ns/distancias.jpg" width="500pt" height="350pt" style="display: block; margin: auto;" /></p>
<p><strong>Nota:</strong> En la práctica, se debería obtener resultados similares la mayor parte del tiempo, utilizando cualquiera de estas distancias mencionadas. Si lo datos contienen valores atípicos, distancia de <em>Manhattan</em> debería dar resultados más sólidos, mientras que la distancia <em>euclidiana</em> se vería influenciada por valores inusuales.</p>
</div>
</div>
<div id="dbscan" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> DBSCAN</h2>
<p><em>DBSCAN (agrupación espacial basada en densidad y aplicación con ruido)</em>,
es un algoritmo de agrupamiento basado en densidad, que puede
utilizarse para identificar agrupaciones de cualquier forma en un conjunto de datos que contenga ruido y valores atípicos.</p>
<p>La idea básica detrás del enfoque de agrupamiento basado en densidad se deriva de un método de agrupamiento intuitivo. Por ejemplo, mirando la siguiente figura, uno puede identificar fácilmente cuatro grupos junto con varios puntos de ruido, debido a las diferencias
en la densidad de puntos.</p>
<p><img src="img/ml-ns/3-11-3-dbscan.png" width="650pt" height="300pt" style="display: block; margin: auto;" /></p>
<p>Los clústeres son regiones densas en el espacio de datos, separadas por regiones de menor densidad de puntos. El algoritmo <em>DBSCAN</em> se basa en esta noción intuitiva de “clústeres” y “ruido.” La idea clave es que para cada punto de un grupo, la vecindad de un determinado radio debe contener al menos un número mínimo de puntos.</p>
<p>Los métodos de particionamiento vistos anteriormente son adecuados para encontrar grupos de forma esférica o grupos convexos. En otras palabras, ellos funcionan bien solo para grupos compactos y bien separados.
Además, también son severamente afectados por la presencia de ruido y valores atípicos en los datos.</p>
<p>Desafortunadamente, los datos de la vida real pueden contener:</p>
<ul>
<li><p>Grupos de forma arbitraria como los como se muestra en la siguiente figura (grupos ovalados, lineales y en forma de “S”).</p></li>
<li><p>Muchos valores atípicos y ruido.</p></li>
</ul>
<p><img src="img/ml-ns/3-11-3-dbscan-ovalo.png" width="600pt" height="400pt" style="display: block; margin: auto;" /></p>
<p>El gráfico anterior contiene 5 grupos y valores atípicos, que incluyen:</p>
<ul>
<li>2 clústeres ovalados</li>
<li>2 clústeres lineales</li>
<li>1 clúster compacto</li>
</ul>
<p>Dados los datos “multishapes” del paquete <em>factoextra</em>, el algoritmo de <em>k-medias</em> tiene dificultades para identificar estos grupos con
formas arbitrarias.</p>
<p>Para ilustrar esta situación, el siguiente código calcula <em>k-medias</em>
en el conjunto de datos mencionado.</p>
<p><img src="Taller-de--Introducci%C3%B3n-a-Ciencia-de-Datos-y-Machine-Learning-_files/figure-html/unnamed-chunk-114-1.png" width="672" /></p>
<p>Sabemos que hay <em>5</em> grupos de en los datos, pero se puede ver que el método de <em>k-medias</em> identifica incorrectamente estos 5 grupos.</p>
<div id="algoritmo" class="section level3" number="7.4.1">
<h3><span class="header-section-number">7.4.1</span> Algoritmo</h3>
<p>El objetivo es identificar regiones densas, que se pueden medir por la cantidad de objetos cerca de un punto dado.</p>
<p>Se requieren dos parámetros importantes para <em>DBSCAN</em>:</p>
<ul>
<li><p><strong>epsilon (“eps”):</strong> Define el radio de vecindad alrededor
un punto <em>x</em>.</p></li>
<li><p><strong>puntos mínimos (“MinPts”):</strong> Es el número mínimo de vecinos dentro del radio <em>“eps”</em>.</p></li>
</ul>
<p>Cualquier punto <em>x</em> en el conjunto de datos, con un recuento de vecinos mayor o igual que <em>MinPts</em>, es marcado como un <strong>punto central</strong>.</p>
<p>Decimos que <em>x</em> es un <strong>punto fronterizo</strong>, si el número de sus vecinos es menos que <em>MinPts</em>, pero pertenece a la vecindad de algún punto central <em>z</em>.</p>
<p>Finalmente, si un punto no es ni un núcleo ni un punto fronterizo, entonces se denomina <strong>punto de ruido o parte aislada</strong>.</p>
<p>La siguiente figura muestra los diferentes tipos de puntos (puntos centrales, fronterizos y atípicos) usando <em>MinPts = 6</em>.</p>
<p>Aquí <em>x</em> es un punto central porque los vecinos <span class="math inline">\(s_{\epsilon}(x) = 6\)</span>, <em>y</em> es un punto fronterizo ya que <span class="math inline">\(s_{\epsilon}(y) &lt; \text{ MinPts}\)</span>,
pero pertenece a la vecindad del punto central <em>x</em>. Finalmente, <em>z</em> es un punto de ruido.</p>
<p><img src="img/ml-ns/3-11-3-dbscan-puntos.png" width="600pt" height="300pt" style="display: block; margin: auto;" /></p>
<p>Un clúster basado en densidad se define como un grupo de puntos conectados por densidad. El algoritmo de agrupamiento basado en densidad (<em>DBSCAN</em>) funciona de la siguiente manera:</p>
<blockquote>
<ol style="list-style-type: decimal">
<li>Para cada punto <span class="math inline">\(x_i\)</span>, calcular la distancia entre <span class="math inline">\(x_i\)</span> y los otros puntos. Hallar todos los puntos vecinos dentro de la distancia <em>eps</em> del punto de partida (<span class="math inline">\(x_i\)</span>). Cada punto, con un vecino cuenta mayor
o igual a <em>MinPts</em>, se marca como punto central o visitado.</li>
</ol>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>Para cada punto central, si aún no está asignado a un clúster, crear un nuevo clúster. Encuentrar recursivamente todos sus puntos densamente conectados y asignarlos a el mismo grupo que el punto
central.</li>
</ol>
</blockquote>
<blockquote>
<ol start="3" style="list-style-type: decimal">
<li>Iterar a través de los puntos no visitados restantes en el conjunto de datos.</li>
</ol>
</blockquote>
<p>Los puntos que no pertenecen a ningún clúster se tratan como valores atípicos o ruido.</p>
</div>
<div id="implementación-en-r-2" class="section level3" number="7.4.2">
<h3><span class="header-section-number">7.4.2</span> Implementación en R</h3>
<p>Utilizaremos el paquete <em>fpc</em> para calcular <em>DBSCAN</em>. También es posible utilizar el paquete <em>dbscan</em>, que proporciona una reimplementación más rápida del algoritmo en comparación con el paquete <em>fpc</em>.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="agrupa.html#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;fpc&quot;</span>)</span>
<span id="cb22-2"><a href="agrupa.html#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="agrupa.html#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb22-4"><a href="agrupa.html#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="agrupa.html#cb22-5" aria-hidden="true" tabindex="-1"></a>db <span class="ot">&lt;-</span> fpc<span class="sc">::</span><span class="fu">dbscan</span>(df, <span class="at">eps =</span> <span class="fl">0.15</span>, <span class="at">MinPts =</span> <span class="dv">5</span>)</span></code></pre></div>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="agrupa.html#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_cluster</span>(</span>
<span id="cb23-2"><a href="agrupa.html#cb23-2" aria-hidden="true" tabindex="-1"></a>  db,</span>
<span id="cb23-3"><a href="agrupa.html#cb23-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> df,</span>
<span id="cb23-4"><a href="agrupa.html#cb23-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">stand =</span> <span class="cn">FALSE</span>,</span>
<span id="cb23-5"><a href="agrupa.html#cb23-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">ellipse =</span> <span class="cn">FALSE</span>, </span>
<span id="cb23-6"><a href="agrupa.html#cb23-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">show.clust.cent =</span> <span class="cn">FALSE</span>,</span>
<span id="cb23-7"><a href="agrupa.html#cb23-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>,</span>
<span id="cb23-8"><a href="agrupa.html#cb23-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">palette =</span> <span class="st">&quot;jco&quot;</span>,</span>
<span id="cb23-9"><a href="agrupa.html#cb23-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">ggtheme =</span> <span class="fu">theme_minimal</span>()</span>
<span id="cb23-10"><a href="agrupa.html#cb23-10" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="Taller-de--Introducci%C3%B3n-a-Ciencia-de-Datos-y-Machine-Learning-_files/figure-html/unnamed-chunk-117-1.png" width="672" /></p>
<p><strong>Nota:</strong> La función <code>fviz_cluster()</code> usa diferentes símbolos de puntos para los puntos centrales (es decir, puntos semilla) y puntos fronterizos. Los puntos negros corresponden a valores atípicos.</p>
<p>Puede verse que <em>DBSCAN</em> funciona mejor para estos conjuntos de datos y puede identificar el conjunto correcto de clústeres en comparación con los algoritmos de <em>k-medias</em>.</p>
<p>Los resultados del algoritmo se pueden ver de la siguiente manera</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="agrupa.html#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(db)</span></code></pre></div>
<pre><code>## dbscan Pts=1100 MinPts=5 eps=0.15
##         0   1   2   3  4  5
## border 31  24   1   5  7  1
## seed    0 386 404  99 92 50
## total  31 410 405 104 99 51</code></pre>
<p>En la tabla anterior, los nombres de las columnas son el número de grupo. El grupo <span class="math inline">\(0\)</span> corresponde a valores atípicos (puntos negros en el gráfico DBSCAN).</p>
</div>
<div id="ventajas-de-dbscan" class="section level3" number="7.4.3">
<h3><span class="header-section-number">7.4.3</span> Ventajas de DBSCAN</h3>
<ul>
<li><p>A diferencia de <em>K-medias</em>, <em>DBSCAN</em> no requiere que el usuario especifique el número de clústeres que se generarán.</p></li>
<li><p><em>DBSCAN</em> puede encontrar cualquier forma de clústeres. No es necesario que el grupo sea circular.</p></li>
<li><p><em>DBSCAN</em> puede identificar valores atípicos.</p></li>
</ul>

</div>
</div>
</div>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script>

$('.pipehover_incremental tr').hover(function() {
  $(this).removeClass()
  $(this).prevAll().removeClass()
  $(this).nextAll().removeClass()
  $(this).addClass('hover');
  $(this).prevAll().addClass('hover');
  $(this).closest('div').next().find('img').attr("src", $(this).attr("link"));
});


$('.pipehover_select_one_row tr').hover(function() {
  $(this).removeClass()
  $(this).prevAll().removeClass()
  $(this).nextAll().removeClass()
  $(this).addClass('hover');
  $(this).closest('div').next().find('img').attr("src", $(this).attr("link"));
});

</script>
            </section>

          </div>
        </div>
      </div>
<a href="muestreo.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="otros.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Taller de  Introducción a Ciencia de Datos y Machine Learning .pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
